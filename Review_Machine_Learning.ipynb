{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Ch 1. What is deep learning](#Ch1)\n",
    "2. [Ch 2. Building block of NN](#Ch2)\n",
    "    1. [Example code](#example1)\n",
    "    2. [Some comments](#comments)\n",
    "3. [Ch 3. Getting started with NN](#Ch3)\n",
    "    1. [Example of 2-class classification: Movie review](#example_classification1)\n",
    "        1. [Get_data](#example2_get_data)\n",
    "        2. [Preparing data](#example2_pre_data)\n",
    "        3. [Building NN](#example2_build_NN)\n",
    "        4. [Validation: Separate the datas](#example2_validation1)\n",
    "        5. [Validation: Plot the losses](#example2_validation2)\n",
    "        6. [Apply to test data](#example2_apply_test)\n",
    "        7. [Summary of the example](#example2_summary)\n",
    "    2. [Example of more-class classificaiton: NewsWire](#example_classification2)    \n",
    "        1. [Get_data](#example2_classification2_get_data)\n",
    "        2. [Preparing data](#example2_classification2_pre_data)\n",
    "        3. [Building NN](#example2_classification2_build_NN)\n",
    "        4. [Validation: Separate the datas](#example2_classification2_validation1)\n",
    "        5. [Validation: Plot the losses](#example2_classification2_validation2)\n",
    "        6. [Apply to test data](#example2_classification2_apply_test)\n",
    "        7. [Generate prediction with new data](#prediction)\n",
    "        8. [Different way to handle labels and loss](#diff_way_handle_label_loss)\n",
    "        9. [Summary of the example](#example2_classification2_summary)\n",
    "    3. [Example of regression: House prices](#example_regression)\n",
    "        1. [Get_data](#example3_get_data)\n",
    "        2. [Preparing data](#example3_pre_data)\n",
    "        3. [Building NN](#example3_build_NN)\n",
    "        4. [Validation: K-fold validation](#example3_validation)\n",
    "        5. [Validation: Plot the losses](#example3_validation2)\n",
    "        6. [Apply to test data](#example3_apply_test)\n",
    "        7. [Summary of the example](#example3_summary)\n",
    "    4. [Summary of the chapter](#summary_ch3)\n",
    "4. [Ch 4. Fundamentals of ML](#Ch4)\n",
    "    1. [Four kinds of ML](#4ML)\n",
    "    2. [Evaluation of ML models](#Evaluation_ML)\n",
    "        1. [Training, validation and test sets](#TrainValidateTest)\n",
    "        2. [Three validation methods (key subsection)](#3validation)\n",
    "        3. [Things to keep in mind](#ThingsToKeep)\n",
    "    3. [Feature engineering, data processing](#prepare_data)\n",
    "    4. [Overfitting (Key section)](#Overfit)\n",
    "        1. [Reducing the network size](#reduce_size)\n",
    "        2. [Weight regularization](#Weight_regularization)\n",
    "        3. [Add dropout](#dropout)\n",
    "    5. [Workflow of ML (key summary)](#workflow)\n",
    "    6. [Some vocabularies for classification and regression](#vocabulary)\n",
    "5. [Ch 5. Deep learning for computer vision](#Ch5)\n",
    "    1. [Introduction to CNN](#intro_cnn)\n",
    "    2. [Training CNN from scratch on a small dataset](#Training_cnn)\n",
    "        1. [Get Data](#Ch5_get_data)\n",
    "        2. [Build CNN](#Ch5_build_cnn)        \n",
    "        3. [Data preprocessing](#Ch5_data_process)\n",
    "        4. [Data augmentation](#Ch5_data_augmentation)\n",
    "    3. [Pretrained public CNN (TBC)](#public_cnn)\n",
    "        1. [Feature extraction](#Ch5_feature_extraction)\n",
    "        2. [Fine-tuning](#Ch5_fine_tuning)\n",
    "    4. [Visualization of what CNN learn](#visualization_cnn)\n",
    "        1. [Visualization of intermediate activations](#see_activation)\n",
    "        2. [Visualizing CNN filters](#see_filters)\n",
    "        3. [Visualizing heatmaps of class activations(elephant example)](#heatmap)\n",
    "    5. [Summary](#ch5_summary)\n",
    "    6. [Tentative: Use CNN for multiclass classifcation](#Tentative:multiclass)\n",
    "6. [Ch 6. Deep learning for text and sequences (TBC)](#Ch6)\n",
    "7. [Ch 7. Advanced deep-learning best practice (TBC)](#Ch7)\n",
    "8. [Ch 8. Generate deep learning (TBC)](#Ch8)\n",
    "9. [Ch 9. Conclusion](#Ch9)\n",
    "    1. [Key concepts](#KeyConcepts)\n",
    "    2. [The limitation of deep learning](#Limitation)\n",
    "    3. [The future of deep learning](#Future)\n",
    "    4. [How to stay up to date](#StayUpToDate)\n",
    "10. [Ch 10. Some practices with Python](#Ch10)\n",
    "    1. [Practices from Ch2: Tensor operations](#practiceCh2)\n",
    "    2. [Practices from Ch3: More tensor operations](#practiceCh3)\n",
    "    3. [Practices from Ch5: Python generators](#practiceCh5)   \n",
    "    4. [Practicse from project: Read data from text file](#practice_project)\n",
    "    5. [Practicse from project: Plot something](#practice_project_2)\n",
    "    6. [Practicse from project: String manipulation](#string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 1. What is deep learning<a name=\"Ch1\"></a>\n",
    "1. In contrast to the usual workflow which is rules + data gives answers, machine learning (ML) works like data + answers gives rules. \n",
    "\n",
    "2. The difference between ML and statistics is that ML deals with more complex problems than stat. Thus ML has less math\n",
    "\n",
    "3. What ML needs? Three things. Input, target (label) and loss function (measure of the performance of the algorithm for adjusting the learning). \n",
    "\n",
    "4. Learning is to find the better representation of the data in the hypothetic space which is the predefiened space of possibilityes. \n",
    "\n",
    "5. So what is \"deep learning (DL)\"? The deep is in the sense that the network has deeper layer, with successive layers of changing the rep of the data. This is very different from the \"shallow learning\" comapred to the early days of ML. \n",
    "\n",
    "6. There are a few parameters of the neural network (NN). N_layer, N_nodes and the weight. The last one is the parameters or knowledge of NN, which is the rep of the data, or can be thought as the coefficient of many polpynomials, each for each layer and the number of monomials are the number of nodes, I guess. \n",
    "\n",
    "7. To measrue how far we are from the target, or the expected output, it is measured by the loss function, or the objective function \n",
    "\n",
    "8. Initially the weights in the NN is assigned randomly, then through descenting, we are approaching to the better values, which minimize the loss functions. This is the tranining loops.\n",
    "\n",
    "9. There are various version of DL. The earliest version is the probabilistic modeling, such as Naive Bayes algorithm, as the name suggests, using the stat techniques. \n",
    "\n",
    "10. The related one is the logistic regressing, which is NOT regression, but rather classification algorithm\n",
    "\n",
    "11. Kernal methods, which is mostly math sounded. The best known one is the SVM, support vector machine, which aims to find hte best decision boundaries for classificaiton. It works in two steps, 1st is to change the coordinate, ort the representation of the data (in fact this is the feature engineering that is needed for later NN too), followed by MAXimizing the margin such that a clear bounary can be founded. It is easy to see that why this is math sounded, as it is just a linear algebra problem. The PROBLEM of svm is that it can NOT scaled to more complicated problem, and the feature enginnering can only be done manually. \n",
    "\n",
    "\n",
    "12. Decision tree, is a flow chart to classify stuffs. Random forest is a bunch of decison tress. Gradient boosting machines is a bunch of random forest. The point is to repeately strengthen the weak point. \n",
    "\n",
    "13. So why NN is good? 1st the feature engineering is automated. 2nd is that the network learns the feature \"together\" although there are multiple layers, rather than layer by layer. In a sens, it is 1+1+1>3. The NN is joined. \n",
    "\n",
    "14. 2 essential feature of NN. 1, Layer and layer make the  rep of data more and more complicated (cause we have deeper network) 2, In the above process, the parameters of the WHOLE network is updated simultaneously, rather than layer by layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 2. The building block of NN<a name=\"Ch2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we come with the first example: MNIST which is to classify the digits. Below is the summary. After, we shall insert in the python code in the next cell.\n",
    "\n",
    "Summary of 1st example.\n",
    "\n",
    "1. Load the data from MINST, including training.data, training.label, similarly for the test data. They are in Numpy. \n",
    "\n",
    "2. From Keras to load models and network. Network has multiple layers.\n",
    "\n",
    "3. Reshape the datas for Kera.\n",
    "\n",
    "4. Chose the three property of the network, the loss function for measuring the fit, the optimizer for determining how to update the network, the metric for accuracy. \n",
    "\n",
    "5. Catagoricaly encode label\n",
    "\n",
    "6. Feed it in, to train the network\n",
    "\n",
    "7. Use the network for the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code <a name=\"example1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "[5 0 4 ... 5 6 8]\n",
      "uint8\n",
      "[7 2 1 ... 4 5 6]\n",
      "uint8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "(train_images , train_labels) , (test_images , test_labels) = mnist.load_data() \n",
    "\n",
    "train_images.shape\n",
    "print( len(train_labels) ) # 60000\n",
    "test_images.shape\n",
    "print( len(test_labels) )  # 10000\n",
    "\n",
    "print( train_labels )\n",
    "print( train_labels.dtype )\n",
    "print( test_labels )\n",
    "print( test_labels.dtype )\n",
    "\n",
    "digit = train_images[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# 512 and 10 are the size of the\n",
    "# output. In particular,\n",
    "# 10 in the 2nd line\n",
    "# is the probability of\n",
    "# that digits being 0-9\n",
    "# The size of input\n",
    "# of second line is not\n",
    "# specified, and by \n",
    "# default being the \n",
    "# size of the output of\n",
    "# the previous layer, \n",
    "# which is 512 here. \n",
    "# BUT, why we set the\n",
    "# size of the output to\n",
    "# be 512 in the first line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])            \n",
    "\n",
    "# The compilation loop above needs THREE things.\n",
    "# 1. Loss function for measure purposes\n",
    "# 2. Optimizer: Tele the NN how to update itself\n",
    "# 3. The metric: For monitoring the during training and testing\n",
    "#  (NOt sure the difference between metric and the loss fun)\n",
    "# See Sec 2.4.3 for optimizer with momentum in  mini SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215687\n",
      " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
      " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313726\n",
      " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13725491 0.94509804\n",
      " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      " 0.5882353  0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.5803922\n",
      " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058825\n",
      " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
      " 0.3137255  0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333336 0.99215686\n",
      " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape(( len(train_labels) , 28 * 28 ) )\n",
    "print( train_images[0] ) \n",
    "train_images = train_images.astype('float32') / 255\n",
    "# Initially train_images has values between 0,255. We \n",
    "# transform it such that it is between 0,1\n",
    "print( train_images[0] ) \n",
    "test_images = test_images.reshape( ( len(test_labels) , 28 * 28 ))\n",
    "test_images = test_images.astype( 'float32') / 255\n",
    "\n",
    "# The above is to transform the data so that it is in the \n",
    "# right shape to be fed into the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "print( train_labels ) \n",
    "train_labels = to_categorical( train_labels)\n",
    "test_labelsa = to_categorical( test_labels)\n",
    "print( train_labels[0] ) \n",
    "print( train_labels[1] ) \n",
    "print( train_labels[2] ) \n",
    "# The above is to categorically encode the labels. See Ch 3.\n",
    "# Essentially it is an array of size 10, with number 0 and 1\n",
    "# only. The number 5 corresponding to an array with 5th element\n",
    "# being 1, and the rest is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.2565 - accuracy: 0.9259\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1028 - accuracy: 0.9694\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.0681 - accuracy: 0.9793\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.0493 - accuracy: 0.9854\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0373 - accuracy: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x10a27cf60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOW WE ARE READY TO TRAIN THE NETWORK\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "# Batch is the size of the samples that were analyze each time, and it will run \n",
    "# 60000/128 times to cover the whole sample (need not cover the whole thing\n",
    "# as the batch is chosen randomly?). This is called one epoch. Then it will \n",
    "# repeat for 5 epochs. Question: Since each batch is \n",
    "# drawn randomly, it \n",
    "# won’t cover the whole\n",
    "# sample? There must\n",
    "# be some samples \n",
    "# are not covered within\n",
    "# each epoch??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "batch1 = train_images[:128]\n",
    "batch2 = train_images[128 : 256]\n",
    "# print( batch1 )\n",
    "# print( batch2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments <a name = \"comments\"></a>\n",
    "    \n",
    "We have skip quite a few discussion on tensors here (see below for the practice of python with them), and we come to the notion of data batchs. \n",
    "\n",
    "1. Sample axis, which is the label of the samples in the data, which is often teh first index. \n",
    "\n",
    "2. Most of time, for a given sample we break it into different batches, which are labeld as batch axis/dim. \n",
    "\n",
    "3. There are FOUR kinds of data stuctures mostly often seen, which are just 2,3,4,5D\n",
    "\n",
    "4. Broadcasting, this allows us to add matrix with vector, in such a way that the smaller tensor will be broadcasted to match the shape of the larger tensor. This looks like it requires the two tensors have the same right-most indices. It has two steps. 1st Axes will be added to the smaller tensor such that the nubmer of axes of two tensors are the same; 2nd, the smaller tensor will be repeated along each axes such that the shape is the same. (Question: What if NO axis is the same)\n",
    "\n",
    "5. The geometric interpretaiton of DL is really basis transforamtion in high-dimensional space of the data. \n",
    "\n",
    "6. Gradient-based optimization. The tensor we talked about previously are the weight or trainable parameters of the layers, or the knowledge of the NN. They were assigned randomly initially, but through the training loop, they will be changed such that the loss function is minimized. \n",
    "\n",
    "7. What is training loop? There are four steps. 1st, Draw a batch, 2nd, Run the NN for the batch to get a result, 3rd, Compute the loss, 4th Update ALL the weights to minimize the loss. here, Compute the gradient of the loss with regard to the network’s parameters (a backward pass). This is  a mini SGD with backward pass. it is stochastic because batches are drawb randomly.  \n",
    "\n",
    "8. Momentum can be used to avoid over fitting. \n",
    "\n",
    "9. Backward propagation. We never need to implement it by hand, if we use TensorFlow, but we NEED to understand it.\n",
    "\n",
    "10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch3. Getting started with NN<a name=\"Ch3\"></a>\n",
    "1. Two most common scenario of NN: classification (binary and multiclass, multi-label) and regression (scalar and vector).\n",
    "\n",
    "2. Five elements of NN: Layer, input data and target, loss function,  optimizer and measure\n",
    "\n",
    "3. Layer: This is essentially the data structure of the NN, which is either 2,3,4,5D. Most common ones for Keras: Sequence is 3D, densely connected/fully connected are 2D. The different layers have to be compatible, in the sense that the size of the output of the previous layer has to be the same as the size of the input of the next layer.\n",
    "\n",
    "4. Model, which is a bunch of layers. There are three kinds of topology of the NN. 2-branch (tree), multihead (graph) and inception blocks (not sure what). These topologies are the HYPOTHESIS SPACE in which we constraint our space of possibilities that the NN can look for. In a sense, it constraint the relations of the weights of differnet layers. \n",
    "\n",
    "5. Loss function. For a single output of NN, it is associated with a single loss function. Thus if there are multiple outputs, then there will more than one loss function. BUT, during the process of gradient descent, there can be ONLY ONE loss function, which is the average of all the loss functions of the output. How to chose the loss, it is STANDARD. 2-class classification: binary crossentropy; many-class classifcaiton: Categorical crossentropy; Regression: mean-square error (as usual).\n",
    "\n",
    "6. Measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use Keras? <a name = \"Keras\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your training data: input tensor and target tensors. \n",
    "\n",
    "# 2. DEfine your NN that maps your input to your targets\n",
    "#\n",
    "# These can be done in TWO ways in Keras, \n",
    "# Sequential (only for linear stacks of layers, commonly used) \n",
    "# or functional API (which could be graph-like network)\n",
    "# Funtional API in Keras manipulates data tensor as functions. \n",
    "# For more info on API, see Ch 7. \n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add( layers.Dense( 32 , activation = 'relu' , input_shape = (784,) ) ) \n",
    "model.add( layers.Dense( 10 , activation = 'softmax' ) ) \n",
    "\n",
    "# The above is done via Sequentialabs\n",
    "# The same thing could be done via functional API\n",
    "\n",
    "input_tensor = layers.Input( shape = (784 , ) ) \n",
    "x = layers.Dense( 32 , activation='relu')(input_tensor) \n",
    "output_tensor = layers.Dense( 10 , activation='softmax')(x)\n",
    "model = models.Model( inputs = input_tensor , outputs = output_tensor )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Once your model is defined, we then come to the learning\n",
    "# part, which IS THE SAME whether you use Sequential or functional API\n",
    "#\n",
    "# The learning process is configured in the compilation step, where\n",
    "# we specify the THREE things: optimizer , loss function and metric.\n",
    "\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile( optimizer= optimizers.RMSprop( lr = 0.001 ) , \n",
    "             loss = 'mse' , \n",
    "             metrics = ['accuracy' ] ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Finally is the FIT process, which consists of passing \n",
    "# Numpy array of input datas and the target datas to the model \n",
    "# via fit() method. \n",
    "#\n",
    "# Note we need to specify the batch size and the number of epochs\n",
    "\n",
    "model.fit( input_tensor , target_tenor , batch_size=128 , epochs = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of 2-class classification: Movie review <a name = \"example_classification1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data <a name = 'example2_get_data' ></a> \n",
    "We load the data and see how it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data , train_labels) , (test_data , test_labels) = imdb.load_data( num_words = 10000 ) \n",
    "\n",
    "# We first load the data. num_words means we keep only the first 10000 words that are most \n",
    "# frequenct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "1\n",
      "218\n",
      "189\n",
      "[1 0 0 ... 0 1 0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print( train_data.shape ) \n",
    "print( train_labels.shape )\n",
    "# print( train_data[0] )\n",
    "print( train_labels[0] )\n",
    "print( len( train_data[0] ) )\n",
    "print( len( train_data[1] ) )\n",
    "\n",
    "print( train_labels )\n",
    "print( train_labels[1] )\n",
    "\n",
    "# 0 is positive review, and 1 is negative. We are doing binary classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# If we want to decode review, we do the following\n",
    "\n",
    "word_index = imdb.get_word_index() # This get the mapping from numbers to the corresponding word in the dictionary\n",
    "reverse_word_index = dict( [ (value , key) for (key , value) in word_index.items() ] ) \n",
    "\n",
    "decode_review = ' '.join( [reverse_word_index.get( i - 3 , '?' ) for i in train_data[0]])\n",
    "\n",
    "# The first three indices are for sth else, not sure why the ? mark\n",
    "\n",
    "print( decode_review )\n",
    "\n",
    "\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data <a name = 'example2_pre_data' ></a>\n",
    "It is important to preparing the data in the right form before using ML. There are TWO ways.\n",
    "\n",
    "1. Pad the data so that they have the same length, which is could be of the shape (samples , word_indices). And then use the \"Embedding\" layer as the first layer for it. See later. \n",
    "\n",
    "2. Here we use the folloiwng. So called one-hot encoding. It is of size ( length(sequences) , dimension ). For each sample, if certain word appears, then the corresponding entry would be 1, otherwise would be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 10000)\n",
      "(25000, 10000)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences( sequences , dimension = 10000 ): \n",
    "    results = np.zeros( ( len(sequences) , dimension ) )\n",
    "    for i , sequences in enumerate( sequences ): \n",
    "        results[ i , sequences ] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences( train_data ) \n",
    "x_test = vectorize_sequences( test_data )\n",
    "\n",
    "# We also need to vectorize the labels\n",
    "# Here np.asarray convert the input into array, ndarray.astype \n",
    "# Copy of the array, cast to a specified type.\n",
    "y_train = np.asarray( train_labels).astype('float32')\n",
    "y_test = np.asarray( test_labels).astype('float32')\n",
    "\n",
    "print( x_train.shape ) \n",
    "print( x_test.shape ) \n",
    "print( y_train.shape ) \n",
    "print( y_test.shape ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building NN<a name = 'example2_build_NN'></a>\n",
    "Now we have vectorized the data and the labels are scalars. The NN suitable for this problem is the \"Dense\" layers with \"relu\" activaations, or so called rectified linear unit function.\n",
    "\n",
    "There are TWO key architecture decisions for the \"Dense\" layers.\n",
    "\n",
    "1. How may layers\n",
    "2. How many nodes for each layer\n",
    "\n",
    "For the second one, it is called the number of hidden units, which is a dimension of the representation space of the layer. Recall that for each layer, it operates the following tensor operatoration\n",
    "\n",
    "output = relu( dot( W , input ) + b ) \n",
    "\n",
    "where b is the bias vector and W is the weight (to be trained). Now W is a matrix of the size ( input_dimension , number of hidden unit). \n",
    "\n",
    "Note the difference between \"relu\" and \"sigmoid\". The sigmoid function is \n",
    "\n",
    "sigmoid(x) = 1 / ( 1 + exp(-x) ) \n",
    "\n",
    "with output being [0,1] regardless of the input. \n",
    "\n",
    "while relu is \n",
    "\n",
    "relu(x) = max( x , 0 ) \n",
    "\n",
    "\n",
    "Why we need activation functions? Ans, we need some non-linearity so that we can get benefit from the fact that we have mulitple layers in the network. Without activation, the output will be simply the linear combination of inputs, no matter how many layers we have. We don't want that, rather we want the NN have richer, nontriival, non-linear structures, which is done via activation functions. There are more activation function, like relu, sigmoid, prelu, elu etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use three layers.\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add( layers.Dense( 16 , activation = 'relu' , input_shape = (10000 , ) ) )\n",
    "model.add( layers.Dense( 16 , activation = 'relu' ) ) \n",
    "model.add( layers.Dense( 1 , activation = 'sigmoid' ) )\n",
    "\n",
    "# Chose optimizer, loss, metric\n",
    "# The following are standard choices\n",
    "model.compile( optimizer = 'rmsprop' , \n",
    "             loss = 'binary_crossentropy' , \n",
    "             metrics = ['accuracy' ] ) \n",
    "\n",
    "\n",
    "# We could put customized optimizer, loss and metric as follows\n",
    "# from keras import optimizers\n",
    "# from keras import losses\n",
    "# from keras import metrics\n",
    "\n",
    "# model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='binary_crossentropy',\n",
    "# metrics=['accuracy'])\n",
    "\n",
    "# model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy,\n",
    "# metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation: Separate the datas <a name = \"example2_validation1\" ></a>\n",
    "This is like separate your training set into two parts, and train on one part, and test on the other part. Repeat the process until you are satisfied. \n",
    "Keras fit() function allow you to simply for validation data at the end fro validation. Here the number of epoches covers the whole training set, including validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 117us/step - loss: 0.5077 - acc: 0.7953 - val_loss: 0.3836 - val_acc: 0.8740\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 91us/step - loss: 0.3047 - acc: 0.9039 - val_loss: 0.3163 - val_acc: 0.8774\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 89us/step - loss: 0.2261 - acc: 0.9259 - val_loss: 0.2770 - val_acc: 0.8925\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 80us/step - loss: 0.1798 - acc: 0.9423 - val_loss: 0.2787 - val_acc: 0.8888\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 82us/step - loss: 0.1475 - acc: 0.9535 - val_loss: 0.2800 - val_acc: 0.8885\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.1222 - acc: 0.9619 - val_loss: 0.2872 - val_acc: 0.8879\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 79us/step - loss: 0.1001 - acc: 0.9706 - val_loss: 0.3026 - val_acc: 0.8857\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0846 - acc: 0.9763 - val_loss: 0.3330 - val_acc: 0.8807\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0673 - acc: 0.9825 - val_loss: 0.3579 - val_acc: 0.8785\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0549 - acc: 0.9863 - val_loss: 0.3693 - val_acc: 0.8791\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0460 - acc: 0.9894 - val_loss: 0.3941 - val_acc: 0.8770\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 80us/step - loss: 0.0358 - acc: 0.9918 - val_loss: 0.4507 - val_acc: 0.8676\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.0293 - acc: 0.9945 - val_loss: 0.4566 - val_acc: 0.8714\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 101us/step - loss: 0.0224 - acc: 0.9961 - val_loss: 0.4976 - val_acc: 0.8681\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 2s 124us/step - loss: 0.0175 - acc: 0.9979 - val_loss: 0.5244 - val_acc: 0.8713\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 2s 127us/step - loss: 0.0143 - acc: 0.9979 - val_loss: 0.5563 - val_acc: 0.8711\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 2s 118us/step - loss: 0.0096 - acc: 0.9993 - val_loss: 0.5934 - val_acc: 0.8702\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 79us/step - loss: 0.0093 - acc: 0.9986 - val_loss: 0.6274 - val_acc: 0.8677\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 96us/step - loss: 0.0046 - acc: 0.9998 - val_loss: 0.6595 - val_acc: 0.8672\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 83us/step - loss: 0.0060 - acc: 0.9991 - val_loss: 0.6954 - val_acc: 0.8671\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[ :10000 ] # Remember we have vectoerize the thing\n",
    "partial_x_train = x_train[ 10000: ] \n",
    "y_val = y_train[ :10000 ] # Remember we have vectoerize the thing\n",
    "partial_y_train = y_train[ 10000: ] \n",
    "\n",
    "\n",
    "model.compile( optimizer='rmsprop', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'] ) # Note that this is not \"accuracy\" i guess...\n",
    "\n",
    "history = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512, \n",
    "                    validation_data=(x_val , y_val) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The fit()  return a History obj with a member \n",
    "# history that has data about everything in the training\n",
    "# process. The dictionary has FOUR keys, one per metric that \n",
    "# was monitored\n",
    "history_dict = history.history \n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation: Plot the losses <a name = \"example2_validation2\" ></a>\n",
    "Here we plot the training and validation loss, and acc side by side. The lessson we learn is the following. Indeed, the training loss decreases as the number of epochs increase, and the acc increases. This is expected (that is why trainign is for, which is minimize the loss and maximize the acc). However, this is not the case for the validation set, where after some point, the loss increases, and the acc decreases. That happens after the number of epochs exceed 3. This is called the OVER-FITTING, which means that as the number of epochs increases, the NN will perform better and better on the training data, because you forces it to remember more details of the training. However, it loses the ability to generate for the un-seen data, which is the validation set here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we plot the training and validation loss side by side.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range( 1 , len(loss_values) + 1 ) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot( epochs , loss_values , 'bo' , label = 'Training loss' )\n",
    "plt.plot( epochs , val_loss_values , 'b' , label = 'Validation loss' ) \n",
    "plt.title( 'Training vs validation loss' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'Loss' ) \n",
    "plt.legend() \n",
    "\n",
    "# Here we plot the training and validation accurary side by side.\n",
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "\n",
    "epochs = range( 1 , len(loss_values) + 1 ) \n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot( epochs , acc_values , 'bo' , label = 'Training acc' )\n",
    "plt.plot( epochs , val_acc_values , 'b' , label = 'Validation acc' ) \n",
    "plt.title( 'Training vs validation acc' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'acc' ) \n",
    "plt.legend() \n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply to test data <a name = \"example2_apply_test\" ></a>\n",
    "We have seen the over-fitting. Thus let's fix our number of epoches to be 4, and train a new NN from stratch, and see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 2s 67us/step - loss: 0.4355 - accuracy: 0.8204\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 47us/step - loss: 0.2490 - accuracy: 0.9135\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.1951 - accuracy: 0.9299\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 48us/step - loss: 0.1645 - accuracy: 0.9421\n",
      "25000/25000 [==============================] - 2s 65us/step\n",
      "[0.3177081020259857, 0.8763200044631958]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the data. \n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data , train_labels) , (test_data , test_labels) = imdb.load_data( num_words = 10000 ) \n",
    "\n",
    "\n",
    "# Step 2: Data vectorization\n",
    "\n",
    "import numpy as np\n",
    "def vectorize_sequences( sequences , dimension = 10000): \n",
    "    results = np.zeros( ( len(sequences) , dimension ) )\n",
    "    for i , sequences in enumerate( sequences ): \n",
    "        results[ i , sequences ] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences( train_data ) \n",
    "x_test = vectorize_sequences( test_data )\n",
    "\n",
    "y_train = np.asarray( train_labels).astype('float32')\n",
    "y_test = np.asarray( test_labels).astype('float32')\n",
    "\n",
    "# Step 3: Train the NN\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add( layers.Dense( 16 , activation = 'relu' , input_shape = (10000 , ) ) )\n",
    "model.add( layers.Dense( 16 , activation = 'relu' ) ) \n",
    "model.add( layers.Dense( 1 , activation = 'sigmoid' ) )\n",
    "\n",
    "model.compile( optimizer = 'rmsprop' , \n",
    "             loss = 'binary_crossentropy' , \n",
    "             metrics = ['accuracy' ] ) \n",
    "\n",
    "model.fit( x_train , y_train , epochs = 4 , batch_size = 512 ) \n",
    "\n",
    "# Step 4: Apply to test data\n",
    "\n",
    "results = model.evaluate( x_test , y_test ) \n",
    "print( results ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the exmple <a name = \"example2_summary\"></a>\n",
    "What you can do\n",
    "1. Try one or three hidden layers\n",
    "2. Try more hidden units or nodes\n",
    "3. Try replace the loss function from binary_crossentropy to mse\n",
    "4. Try replace relu to tanh\n",
    "\n",
    "What you have learnt\n",
    "1. The workflow of the NN, there are FOUR steps\n",
    "2. \"Dense\" layer with relu is useful for many classiication prlbem\n",
    "3. For classficaiton problem, the last unit usually come with \"sigmoid\" bewteen 0 and 1, for probability\n",
    "4. The \"rmsprop\" optimizer is good enough, and that is the last thing you should worry about\n",
    "5. There is over-fititng problem, do you validation homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of more-class classificaiton: NewsWire <a name = \"example_classification2\"></a>\n",
    "Here we try to understand the problem of the type **single-valued** and **multiclass** classification, because our data would be single type, if it is **multile categories**, it would be **multilabel**, and **multiclasss** classification. \n",
    "\n",
    "Reuters dataset, has 46 different topics, each topic has at least 10 examples. Our jobs is to classify the topics, or it is 46-fold classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data <a name = 'example2_classification2_get_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "8982\n",
      "2246\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "( train_data , train_labels) , ( test_data , test_labels) = reuters.load_data( num_words = 10000 ) \n",
    "# Again, we care only the 10000 most frequent occuring words\n",
    "\n",
    "print( len( train_data ) ) \n",
    "print( len( train_targets ) ) \n",
    "print( len( test_data ) ) \n",
    "print( len( test_targets ) ) \n",
    "\n",
    "# for i , label in enumerate( train_labels ):\n",
    "#     print( i ) \n",
    "#     print( label ) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data <a name = 'example2_classification2_pre_data'></a>\n",
    "Vectorize the data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 46)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences( sequences , dimension = 10000 ):\n",
    "    results = np.zeros( ( len( sequences) , dimension ) )\n",
    "    for i , sequences in enumerate( sequences ):\n",
    "        results[ i , sequences ] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences( train_data ) \n",
    "x_test = vectorize_sequences( test_data )\n",
    "\n",
    "def vectorize_labels( labels , dimension = 46 ):\n",
    "    results = np.zeros( ( len( labels ) , dimension ) ) \n",
    "    for i , label in enumerate( labels ):\n",
    "        results[ i , label ] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = vectorize_labels( train_labels ) \n",
    "one_hot_test_labels = vectorize_labels( test_labels ) \n",
    "\n",
    "print( one_hot_train_labels.shape ) \n",
    "\n",
    "# # Just use the Keras built in things\n",
    "# one_hot_train_labels = to_categorical( train_labels ) \n",
    "# one_hot_test_labels = to_categorical( test_labels ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building NN <a name = 'example2_classification2_build_NN'></a>\n",
    "One difference with N-class and 2-class classifciaiton is that, since we have a stack of Dense layers, each layer can only accss to the output of the pervious layers, we don't want the **size** of each layer to be smaller than N, the number of class. This is intuitive: If we start from certain layer with output number less than N, it is asking the rest of the network to give N-class with number of data points less than N, which is impossible. Thus we need a larger NN for larger class.\n",
    "\n",
    "A few things to note for the following network\n",
    "1. The last layer has 46 output (units), each for one classificaiton\n",
    "2. **softmax** gives the probabilty distribution over the 46 classes: For a single input sample, the CNN gives the probability of it being in each class. The sum of the prob is 1.\n",
    "3. The loss function is **categorical_crossentropy**, which measures the distance betweetn the true prob distribution and the output prob distribution\n",
    "4. There is another way to pre-process the labels, as discussed in [Different way to handle labels and loss](#diff_way_handle_label_loss), where we encode the labels as integers, rather than using the one-hot enconding. That case, we shall use the **sparse_categorical_crossentropy** as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add( layers.Dense( 64 , activation = 'relu' , input_shape = (10000,) ) ) \n",
    "model.add( layers.Dense( 64 , activation = 'relu' ) ) \n",
    "model.add( layers.Dense( 46 , activation = 'softmax' ) ) \n",
    "\n",
    "model.compile( optimizer= 'rmsprop' , \n",
    "             loss = 'categorical_crossentropy' , \n",
    "             metrics = ['accuracy' ] ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation: Separate the datas <a name = 'example2_classification2_validation1'></a>\n",
    "Take out 1000 as the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 10000)\n",
      "(8982, 46)\n",
      "(7982, 10000)\n",
      "(7982, 46)\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 243us/step - loss: 2.6632 - accuracy: 0.5475 - val_loss: 1.7645 - val_accuracy: 0.6600\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 131us/step - loss: 1.4244 - accuracy: 0.7111 - val_loss: 1.3154 - val_accuracy: 0.7260\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 133us/step - loss: 1.0516 - accuracy: 0.7751 - val_loss: 1.1457 - val_accuracy: 0.7570\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 136us/step - loss: 0.8242 - accuracy: 0.8266 - val_loss: 1.0315 - val_accuracy: 0.7810\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 137us/step - loss: 0.6576 - accuracy: 0.8641 - val_loss: 0.9577 - val_accuracy: 0.8030\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 118us/step - loss: 0.5268 - accuracy: 0.8929 - val_loss: 0.9282 - val_accuracy: 0.8030\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 110us/step - loss: 0.4311 - accuracy: 0.9109 - val_loss: 0.8966 - val_accuracy: 0.8140\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 108us/step - loss: 0.3494 - accuracy: 0.9262 - val_loss: 0.8960 - val_accuracy: 0.8220\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 115us/step - loss: 0.2944 - accuracy: 0.9375 - val_loss: 0.9169 - val_accuracy: 0.8100\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 120us/step - loss: 0.2460 - accuracy: 0.9440 - val_loss: 0.8982 - val_accuracy: 0.8170\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 123us/step - loss: 0.2119 - accuracy: 0.9479 - val_loss: 0.8839 - val_accuracy: 0.8250\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 108us/step - loss: 0.1857 - accuracy: 0.9515 - val_loss: 0.9600 - val_accuracy: 0.8120\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 106us/step - loss: 0.1700 - accuracy: 0.9536 - val_loss: 0.9444 - val_accuracy: 0.8130\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 121us/step - loss: 0.1541 - accuracy: 0.9548 - val_loss: 0.9299 - val_accuracy: 0.8220\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 200us/step - loss: 0.1439 - accuracy: 0.9567 - val_loss: 0.9678 - val_accuracy: 0.8170\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 312us/step - loss: 0.1324 - accuracy: 0.9564 - val_loss: 1.0258 - val_accuracy: 0.8050\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 199us/step - loss: 0.1254 - accuracy: 0.9585 - val_loss: 1.0056 - val_accuracy: 0.8160\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 186us/step - loss: 0.1223 - accuracy: 0.9593 - val_loss: 1.0395 - val_accuracy: 0.8160\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 108us/step - loss: 0.1203 - accuracy: 0.9575 - val_loss: 1.0275 - val_accuracy: 0.8150\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 112us/step - loss: 0.1145 - accuracy: 0.9584 - val_loss: 1.0219 - val_accuracy: 0.8120\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[ : 1000 ]\n",
    "partial_x_train = x_train[ 1000 : ]\n",
    "y_val = one_hot_train_labels[ : 1000 ]\n",
    "partial_y_train = one_hot_train_labels[ 1000 : ]\n",
    "\n",
    "print( x_train.shape ) \n",
    "print( one_hot_train_labels.shape ) \n",
    "\n",
    "print( partial_x_train.shape ) \n",
    "print( partial_y_train.shape ) \n",
    "\n",
    "history = model.fit( partial_x_train , \n",
    "                   partial_y_train ,\n",
    "                    epochs = 20 , \n",
    "                    batch_size = 512 , \n",
    "                    validation_data = ( x_val , y_val ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation: Plot the losses <a name = 'example2_classification2_validation2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [65.3311567541055, 39.776090942392365, 34.32261133918742, 27.680670603362863, 23.473644146365363, 22.52619806592619, 22.79192004368253, 22.770185779291435, 17.891637504778544, 17.22231869969295, 19.80953382602939, 17.605009325823776, 16.321068039287567, 20.478961443159694, 15.61127507652789, 16.718608436294563, 14.699746130739483, 15.916404533125082, 16.031627229373385, 14.611600887082844, 18.139140705647186, 14.015900961077302, 14.239361140213095, 12.561681124091258, 13.30613582983318, 12.55906216896803, 12.260872022373288, 12.29156907676563, 11.712689146895704, 12.450729773651883, 12.56630845620305, 13.483685229759718, 13.15491041243837, 11.78229565013464, 12.88459904933127, 13.023799285264962, 12.108842663661724, 15.469679353627438, 12.555042235641787, 11.742788167993105, 11.880856605593747, 12.777319117148965, 12.349031028275535, 11.934441249798015, 13.97433355483653, 11.428836756669408, 11.156954098963132, 11.412460465727767, 11.697844318199515, 11.250578743562397, 12.011186958216905, 12.337325209267622, 12.178285356486853, 11.327628672268187, 11.572528048414428, 11.095232769357194, 11.547587482355498, 10.89704412067687, 11.776617650933643, 11.509231031617169, 11.466086535947541, 11.513909517327498, 10.878487468141564, 11.353505508656214, 11.826184791835523, 11.274384294338594, 11.077379363542308, 11.407087584223545, 11.823160643403483, 11.348023253455551, 11.535188405061922, 10.740262542298584, 11.037835361880347, 14.468348459376976, 12.320054728812122, 11.065318511526957, 10.196312874574318, 12.710773596032938, 11.184319578576185, 14.380173534172181, 10.056968553236862, 11.607888777131713, 10.254099730050823, 10.31047039789535, 11.046551178993269, 9.971497246822674, 11.647552685435206, 10.820723609676952, 12.975559028220285, 10.762306096291603, 11.109022101723085, 11.377209244102602, 11.485624256451604, 10.960727207325768, 10.668497398350443, 10.295484725284473, 10.907976301165071, 11.585771309493667, 11.870662813715747, 10.849608952646365, 10.990505034799636, 12.256060263684876, 10.94029323541168, 12.084088799294712, 10.74751931502274, 12.117025012888089, 10.008791942240459, 9.79032624602715, 10.873732757974775, 10.368831819473481, 11.182109516187765, 10.525392988269358, 11.768114437497829, 9.791652520158383, 10.366831844489905, 10.82123230183255, 12.160218325327294, 10.89712327180919, 11.043144851552743, 10.081087021443862, 10.535608439033131, 10.981081374146338, 11.402706202919656, 9.641788224146776, 10.21196618419438, 9.848204830273557, 10.717139214437482, 10.058930607633618, 11.48641505815717, 11.387720414948543, 11.184878863368903, 10.15860658798927, 12.634969771101996, 10.743748031331844, 10.069616052005982, 10.55061410463618, 12.081727217527286, 10.640027858140547, 12.421999816619099, 10.501246002437039, 10.781571683541602, 10.865363658600922, 11.282211145569223, 10.651188596404626, 10.474688320334236, 11.273722748484204, 11.259925015301976, 10.630266794053108, 12.447139341542082, 9.531828820530503, 9.956735663198968, 9.748018149767875, 10.409433149802672, 10.152983615106885, 9.59336286511346, 10.036261642000195, 10.745350843100661, 10.289103100297622, 10.692397897947801, 10.49608223188189, 10.085275915301507, 11.01686289372144, 11.551699779498954, 11.743552048921142, 10.236048154720212, 10.003178234232127, 12.719112821227622, 10.945822092821034, 10.675044961083811, 10.549677406397354, 10.49193488142573, 10.728818675470862, 10.940691456562059, 10.84922628008961, 11.01185695745722, 10.051327843984387, 10.640393516860238, 11.411730008719646, 11.496731827229187, 11.453003342329902, 10.852511891076322, 12.516667835979197, 11.492266653801803, 10.234439696689565, 12.459450470223691, 12.359721258965017, 11.081772637189284, 10.962543924886036, 12.61372473296631, 9.850524183592707, 10.286291907384697, 11.774103224088465, 9.844982801992469, 10.519427263546772, 10.927765613629846, 9.613802498727484, 12.55826472032799, 11.780731899813647, 11.058068283976777, 11.240432644936044, 10.490413442413129, 12.564543059319925, 10.48954338531268, 12.654398422164952, 10.994893745696725, 12.080461966491645, 13.261745644357553, 11.6413525543775, 9.836769459838035, 12.075115144137515, 12.862921366995579, 10.113928052677432, 11.893327762829376, 11.689800425947846, 10.449994934995953, 11.015308582012445, 11.687384869371677, 11.755015448881498, 12.302777554598373, 11.231973049502894, 10.992727318675042, 10.543350873671766, 11.46447047514855, 11.023617577342556, 10.955677665488812, 11.006285483068279, 12.847765292896398, 11.719314388968257, 12.531713320081678, 10.86539141863981, 11.256658129028095, 11.873176343726353, 13.280287280346792, 11.88398772697308, 11.72025412732348, 11.150501354622866, 11.54384481018446, 11.509621295098233, 10.863472428163606, 11.504212209246893, 10.389840810269552, 12.343409710462288, 12.921313927478922, 12.037928154618285, 12.186794953734273, 11.90736489839815, 11.791193818500634, 11.977544576941073, 10.965935815415914, 11.357686833466984, 12.575537758883973, 11.417199296182906, 11.05332478821349, 10.768702672204316, 11.508758675254015, 10.500545356247464, 12.971465700649482, 12.578036080928298, 10.409328385003805, 10.838445667762974, 11.385151716939573, 11.10936545092733, 11.89665979744897, 11.768126593036076, 11.764489002391525, 10.433418090586633, 11.835506468261379, 11.49151284216516, 11.561622943853646, 11.741023956153699, 11.148711052986316, 11.063064687314794, 11.604472803910053, 12.4958389158233, 10.89721076955355, 11.89152140730282, 11.522090089796855, 13.090380321117493, 12.13504893457895, 12.422137335919668, 11.571214227239391, 11.52560947469083, 14.158431707134488, 11.3299609481168, 11.540383247499548, 12.96275170767417, 12.63215307663725, 12.358268080685532, 11.616475433708395, 12.04842548565893, 12.363837242284951, 12.332381280253776, 11.617373671097102, 10.971368267196278, 12.040184809263293, 12.687486459948307, 11.865505270567713, 11.823908487541743, 11.440298954249096, 10.490947587704886, 11.624458745580702, 12.84551649897606, 11.108117978530014, 11.527772955196486, 10.187797875922907, 13.992230955517131, 11.89330376746677, 11.415479546260679, 11.95081842869079, 11.373123203775538, 11.2934203155046, 11.226333294944702, 11.532691394445743, 11.34567767996537, 12.130202175972414, 11.65659369673928, 12.17052636920108, 11.940386037913953, 11.992950105878275, 12.07894192289764, 11.87908706573818, 10.85732156628804, 12.329237632592909, 11.110863232362721, 12.303808273196609, 11.864865515020325, 11.961818305810679, 11.656400748146769, 11.911147564165478, 12.25361731643489, 12.21247069386415, 11.936371548913044, 11.863568916999837, 11.212625553641022, 10.997925142144396, 12.182984163065436, 11.338976000314625, 12.234816147454946, 10.946965022215347, 11.607283619014629, 11.697233101660224, 10.742593120599723, 12.455691906012362, 11.797499330852661, 11.526683771359794, 11.290294799704732, 11.406707287364926, 13.736556675939793, 11.406114047070467, 11.075187637402694, 11.522262124549874, 11.588487254164107, 11.861103766938985, 11.133505986704915, 12.432518915619037, 11.478453169103094, 11.755333923777949, 11.319070897584217, 11.173291440610543, 12.370201978841704, 12.322139398404051, 11.175845650256715, 11.87313064726186, 11.965410798121852, 11.94271442607837, 11.058701252446067, 11.028619943220535, 12.129428748638741, 11.889728588846578, 12.809146471039766, 11.831866145993832, 13.695942451423875, 11.304745525228883, 11.879099989357309, 12.915280366869698, 11.914896120616099, 12.843217641887485, 12.014743528223947, 11.869111323004011, 12.129566811310706, 12.754096997677548, 13.717300574560326, 12.31862914376537, 12.61541999500858, 12.366739948217914, 11.537812413036734, 11.91091265110299, 12.18103127077563, 16.655474447662826, 10.947803195290245, 14.559492101048566, 12.015418878315007, 14.20721447985564, 11.131116361024162, 11.82692696709889, 11.405576453305606, 13.798701377522136, 12.043030425781698, 12.689781064275966, 10.866371028906912, 11.415241893949126, 11.692653138614963, 13.255928202901073, 12.244842967079647, 13.817661345980879, 13.48659210519215, 11.521258065025183, 11.844822196621854, 13.345741135234215, 11.718478283430663, 11.17588955701541, 11.15008881830698, 10.839376662225039, 12.58156837507308, 11.250312844365242, 11.559941194383, 13.338718300427098, 12.785005292938768, 12.09435125358609, 11.10301182430876, 11.440058018422068, 11.167148101421907, 13.121674063475025, 12.21453776528686, 10.95295434707196, 11.04353059378584, 12.075505577276765, 11.774276006070085, 11.333151501348983, 11.547879993160011, 11.206109856336612, 10.8058997791861, 11.80116850309452, 11.798564801732594, 11.318831831231796, 11.363603020977866, 10.853078589074448, 10.46462519410917, 11.277320146881243, 10.760413231509093, 11.308609320941187, 10.334381166583913, 12.075841505337497, 10.572865080535596, 11.673463521110317, 11.811445593423588, 12.001574647031946, 11.452848711559643, 11.47923523285841, 11.786545140710244, 11.332718823307959, 12.34043554514462, 11.196656414943904, 11.885120759256061, 10.689283006298956, 12.35718212618235, 12.631220618287236, 11.54326956426681, 11.75820554164602, 10.912363280577926, 11.704249008111471, 11.015055351597605, 11.10872843005178, 11.604611335177463, 12.434844627188596, 11.301924379577049, 10.933735837085239, 11.624334305490247, 12.39819823963986, 10.942490057076908, 11.849445703127053, 11.634798237154868, 10.935304897008026, 11.296661566174128, 10.803479604529123, 11.8579395358283, 11.267758376335053, 11.66178952625925, 13.072226912480557, 11.75955469174873, 11.634338584612447, 11.80103961395222, 12.398508997724823, 12.741647663057261, 11.297989030414731, 11.742471542936983, 11.888876423519797, 12.504967256887774, 12.71599434467844, 11.429731481056091, 11.843527792551841, 11.444512407106952, 12.402724263242717, 11.837286215708914, 11.84874085486938, 11.143946190477076, 11.124408122670916, 12.954272402284465, 11.18837321261789, 10.93133025730838], 'val_mae': [5.523077487945557, 4.1374831199646, 3.555220127105713, 3.2080938816070557, 2.9883244037628174, 2.987407684326172, 3.2139437198638916, 2.9211301803588867, 2.736867666244507, 2.608494520187378, 2.864055633544922, 2.722028970718384, 2.580324172973633, 2.9588067531585693, 2.570664882659912, 2.641514539718628, 2.542715549468994, 2.6418521404266357, 2.6021974086761475, 2.496811866760254, 3.110607147216797, 2.4963808059692383, 2.5381052494049072, 2.427574634552002, 2.5158743858337402, 2.459254741668701, 2.4194233417510986, 2.4227747917175293, 2.378005266189575, 2.4702744483947754, 2.516080379486084, 2.6461331844329834, 2.578428030014038, 2.4374170303344727, 2.5760295391082764, 2.596708297729492, 2.4748339653015137, 2.8639793395996094, 2.4969303607940674, 2.3966662883758545, 2.411832571029663, 2.4831221103668213, 2.4761972427368164, 2.391998291015625, 2.7840497493743896, 2.3455028533935547, 2.2873659133911133, 2.302049160003662, 2.3331003189086914, 2.3261852264404297, 2.4080255031585693, 2.419857978820801, 2.4366817474365234, 2.3224833011627197, 2.350356101989746, 2.2844669818878174, 2.4053962230682373, 2.2705771923065186, 2.4252281188964844, 2.3367292881011963, 2.3177075386047363, 2.3504443168640137, 2.3612654209136963, 2.32837176322937, 2.4001176357269287, 2.3668479919433594, 2.293905735015869, 2.338059425354004, 2.3833608627319336, 2.330684185028076, 2.3707141876220703, 2.2564027309417725, 2.235870599746704, 2.655576229095459, 2.503192663192749, 2.280715227127075, 2.2059121131896973, 2.675981044769287, 2.326974630355835, 2.810013771057129, 2.173780679702759, 2.396002769470215, 2.1697185039520264, 2.238389015197754, 2.2858574390411377, 2.192901611328125, 2.447803497314453, 2.265162944793701, 2.506925106048584, 2.2318010330200195, 2.3453569412231445, 2.4621164798736572, 2.415386915206909, 2.2443747520446777, 2.30350399017334, 2.2408604621887207, 2.196425199508667, 2.3575451374053955, 2.386664390563965, 2.305676221847534, 2.316141366958618, 2.436589002609253, 2.37937068939209, 2.5294294357299805, 2.2737228870391846, 2.519465923309326, 2.247131586074829, 2.090589761734009, 2.2835536003112793, 2.264127016067505, 2.4492807388305664, 2.226227045059204, 2.5177242755889893, 2.1579184532165527, 2.240522623062134, 2.3336989879608154, 2.477189064025879, 2.3083810806274414, 2.3554255962371826, 2.3048410415649414, 2.33007550239563, 2.371023654937744, 2.380781650543213, 2.1459245681762695, 2.2624030113220215, 2.193742275238037, 2.3189165592193604, 2.272606611251831, 2.3616387844085693, 2.3604464530944824, 2.433168888092041, 2.1701364517211914, 2.624925136566162, 2.34464430809021, 2.1845812797546387, 2.2481110095977783, 2.4365243911743164, 2.402569055557251, 2.4504599571228027, 2.2142038345336914, 2.2690420150756836, 2.335712194442749, 2.3614916801452637, 2.3313283920288086, 2.3412086963653564, 2.3837532997131348, 2.4149320125579834, 2.3454179763793945, 2.582510232925415, 2.1494438648223877, 2.2059848308563232, 2.171373128890991, 2.239072322845459, 2.2488887310028076, 2.153799295425415, 2.256035327911377, 2.3341546058654785, 2.2760775089263916, 2.310497283935547, 2.3071978092193604, 2.2715227603912354, 2.3816916942596436, 2.5093629360198975, 2.5036303997039795, 2.279751777648926, 2.2950072288513184, 2.576014518737793, 2.346137285232544, 2.3565285205841064, 2.3351476192474365, 2.253253698348999, 2.3283960819244385, 2.3550729751586914, 2.3292665481567383, 2.4324517250061035, 2.1669228076934814, 2.2673566341400146, 2.4047255516052246, 2.3867974281311035, 2.445456027984619, 2.375671148300171, 2.544745445251465, 2.4015488624572754, 2.222095489501953, 2.587569236755371, 2.5706753730773926, 2.345179319381714, 2.297121047973633, 2.4304850101470947, 2.2087411880493164, 2.3251142501831055, 2.5032200813293457, 2.231449604034424, 2.2498934268951416, 2.3164234161376953, 2.1689419746398926, 2.6011176109313965, 2.4203426837921143, 2.3623406887054443, 2.429260015487671, 2.307421922683716, 2.6080470085144043, 2.3294081687927246, 2.485574960708618, 2.3874857425689697, 2.4734206199645996, 2.6813108921051025, 2.452509880065918, 2.26904559135437, 2.4135499000549316, 2.588629961013794, 2.2290985584259033, 2.4883718490600586, 2.3840701580047607, 2.227126359939575, 2.325735330581665, 2.502748489379883, 2.406759262084961, 2.5321316719055176, 2.335273027420044, 2.271791458129883, 2.2720212936401367, 2.398653745651245, 2.3636696338653564, 2.2979156970977783, 2.363558530807495, 2.6184563636779785, 2.4847543239593506, 2.5103840827941895, 2.2943241596221924, 2.3490374088287354, 2.4313085079193115, 2.6174123287200928, 2.4906065464019775, 2.4211931228637695, 2.3323028087615967, 2.4856441020965576, 2.400946855545044, 2.3441171646118164, 2.347707748413086, 2.20176362991333, 2.5374529361724854, 2.599090576171875, 2.546797752380371, 2.548337936401367, 2.606703042984009, 2.431061267852783, 2.5004336833953857, 2.366760492324829, 2.3557796478271484, 2.696385145187378, 2.344386577606201, 2.308049201965332, 2.374965190887451, 2.506213903427124, 2.314282178878784, 2.654829978942871, 2.560715436935425, 2.2545738220214844, 2.383561611175537, 2.476085662841797, 2.3186659812927246, 2.4063644409179688, 2.4506115913391113, 2.555797815322876, 2.3134570121765137, 2.4775445461273193, 2.4126384258270264, 2.4106178283691406, 2.4175844192504883, 2.3115079402923584, 2.3249452114105225, 2.438260316848755, 2.498734474182129, 2.313870906829834, 2.3841781616210938, 2.3715999126434326, 2.588519334793091, 2.51088285446167, 2.542556047439575, 2.3636815547943115, 2.3639461994171143, 2.7419493198394775, 2.357292890548706, 2.4056520462036133, 2.5335769653320312, 2.473590612411499, 2.498396396636963, 2.3808188438415527, 2.4763436317443848, 2.4500741958618164, 2.5742268562316895, 2.4088754653930664, 2.271042823791504, 2.4386515617370605, 2.43957781791687, 2.451793909072876, 2.5139100551605225, 2.4364724159240723, 2.2749087810516357, 2.405299663543701, 2.5826938152313232, 2.4080963134765625, 2.3990674018859863, 2.2654614448547363, 2.7069010734558105, 2.434535503387451, 2.3547537326812744, 2.4281768798828125, 2.4505465030670166, 2.3388705253601074, 2.2751712799072266, 2.4122138023376465, 2.33699893951416, 2.529068946838379, 2.4669008255004883, 2.4338412284851074, 2.4715428352355957, 2.5013885498046875, 2.5848543643951416, 2.4306559562683105, 2.343393564224243, 2.440964460372925, 2.4622650146484375, 2.478954315185547, 2.545316457748413, 2.341581106185913, 2.4648711681365967, 2.5121450424194336, 2.584683895111084, 2.4943997859954834, 2.4931371212005615, 2.406203031539917, 2.3065743446350098, 2.356827735900879, 2.5116348266601562, 2.3882696628570557, 2.4610064029693604, 2.347660541534424, 2.441549062728882, 2.3444361686706543, 2.37929368019104, 2.5412421226501465, 2.438138723373413, 2.3973886966705322, 2.3943915367126465, 2.3750133514404297, 2.6134939193725586, 2.394477367401123, 2.429180383682251, 2.4607486724853516, 2.515946865081787, 2.510448455810547, 2.459289312362671, 2.5460219383239746, 2.416591167449951, 2.4618568420410156, 2.2739593982696533, 2.4231796264648438, 2.5324478149414062, 2.5771050453186035, 2.3931148052215576, 2.445410966873169, 2.513209342956543, 2.528188705444336, 2.387864589691162, 2.3425872325897217, 2.5411205291748047, 2.4453513622283936, 2.50427508354187, 2.5086803436279297, 2.6735877990722656, 2.3932690620422363, 2.4815280437469482, 2.564267873764038, 2.449699878692627, 2.5535972118377686, 2.4840240478515625, 2.4727745056152344, 2.4457879066467285, 2.5328783988952637, 2.5622658729553223, 2.543921947479248, 2.527557611465454, 2.5134007930755615, 2.353787899017334, 2.3977715969085693, 2.5106348991394043, 3.062303066253662, 2.3015856742858887, 2.78549861907959, 2.481151580810547, 2.716665744781494, 2.351750373840332, 2.441180467605591, 2.3681273460388184, 2.728083848953247, 2.4639713764190674, 2.612853765487671, 2.282335042953491, 2.42419695854187, 2.432589530944824, 2.504739999771118, 2.529200553894043, 2.6917362213134766, 2.602813959121704, 2.4638164043426514, 2.4058399200439453, 2.6623034477233887, 2.482865333557129, 2.459773540496826, 2.3457536697387695, 2.2868573665618896, 2.56193470954895, 2.3868329524993896, 2.4170076847076416, 2.6671199798583984, 2.5476479530334473, 2.5649349689483643, 2.3688604831695557, 2.3861403465270996, 2.299562931060791, 2.666123390197754, 2.4925696849823, 2.32822585105896, 2.3348300457000732, 2.507760524749756, 2.368683099746704, 2.3244400024414062, 2.464265823364258, 2.3303632736206055, 2.324303388595581, 2.466625213623047, 2.3932859897613525, 2.3733160495758057, 2.406420946121216, 2.416971445083618, 2.269782543182373, 2.3460915088653564, 2.34865403175354, 2.3427679538726807, 2.231901168823242, 2.5099027156829834, 2.2855381965637207, 2.4174318313598633, 2.431811571121216, 2.4601657390594482, 2.3657963275909424, 2.3942649364471436, 2.4748520851135254, 2.358689785003662, 2.5171523094177246, 2.390988826751709, 2.400848627090454, 2.3127801418304443, 2.4911270141601562, 2.5491886138916016, 2.376286029815674, 2.4215736389160156, 2.3360095024108887, 2.389388084411621, 2.317291259765625, 2.380664825439453, 2.489184617996216, 2.5359127521514893, 2.429445505142212, 2.3439958095550537, 2.3877079486846924, 2.549248695373535, 2.2952120304107666, 2.4625306129455566, 2.455000400543213, 2.3197736740112305, 2.386593818664551, 2.2960426807403564, 2.4587197303771973, 2.3197076320648193, 2.4325666427612305, 2.589977502822876, 2.4203691482543945, 2.4263741970062256, 2.4165244102478027, 2.4822075366973877, 2.579841136932373, 2.3088393211364746, 2.367992639541626, 2.422844886779785, 2.505941152572632, 2.544504404067993, 2.3513128757476807, 2.40826678276062, 2.4734485149383545, 2.4675605297088623, 2.395188808441162, 2.422553539276123, 2.4507877826690674, 2.314685583114624, 2.5832207202911377, 2.3942296504974365, 2.35732102394104], 'loss': [188.03333951234436, 30.064779857826824, 20.955798191548816, 16.77282237367077, 14.933833902438217, 13.84867175904344, 12.388920079150507, 12.251390593745356, 11.861021461292154, 11.384864727076783, 10.87125933396781, 10.628634181456814, 10.138097334806355, 10.039847899875342, 9.937068596534914, 9.843866697981879, 9.339934528212865, 8.746324532454766, 9.591411335149045, 8.76545878775714, 8.456843689738854, 8.737585588848978, 8.374706982200722, 8.198521702316885, 8.358670161621061, 8.364889232167938, 8.219009144725266, 7.7802399837869345, 8.017581293400308, 7.312685551849187, 7.553974370584275, 7.1400129030116295, 7.255521814995308, 7.389221387001052, 7.177244417524238, 7.121125978831488, 6.9402817933780545, 6.9044447207206545, 6.765976077396724, 6.73816023618709, 6.6725407271293715, 6.911719116911224, 6.692445821225462, 6.671863186999622, 6.204962552752375, 6.5164012435483105, 5.943586646675991, 6.580116071238351, 6.076523372236552, 5.9061997936021084, 6.27805778401683, 6.057076343680183, 5.535680158645378, 5.780621977642842, 5.294485127695309, 6.190808303103623, 5.643820000833062, 5.7036519722548915, 5.7666976935234135, 5.454811525292712, 5.511351914674108, 5.662832850929233, 5.65089823832218, 5.399975415934539, 5.592436530291332, 5.515194412904093, 4.907598752222082, 5.153685953422375, 4.801836225165364, 5.006775772455014, 4.917430737916514, 5.208869199584291, 4.993940673019277, 4.856197454229512, 5.164664722560677, 4.701414974271228, 5.2457551446435655, 4.772259776613648, 4.348310911109407, 4.736207202108801, 4.84948745196657, 4.472116272882264, 4.8842588285738335, 4.234109849602379, 4.302423320418709, 4.556662613880143, 4.489529948996596, 4.415258641735407, 4.079955938004349, 4.402689895224642, 4.441570738586426, 4.6009625709940805, 4.3508420726052055, 4.005961939804266, 4.218796681806372, 4.675924279616594, 4.3300577177136885, 4.196086826666, 4.144786032546841, 3.9088041394858286, 3.9682688438799185, 3.9255714900441054, 3.961484088459653, 4.023381084952715, 4.130193386572705, 3.878979504542321, 4.163383605965714, 4.097416779282044, 3.6633538377287618, 3.8534687844309006, 3.9970185739635276, 3.7658514921456803, 3.894493340668008, 3.657578354316146, 3.852261782448676, 3.77813371512313, 3.5787927463498628, 3.672064742165567, 3.74328949873947, 3.9393730900393877, 3.6719934407021966, 3.6132924715614116, 3.2808540684200125, 3.503525455734441, 3.3857261967529273, 3.67929872890184, 3.648026764567884, 3.5557211095364627, 3.384660979641069, 3.3581930484970712, 3.568752842555717, 3.494497295105488, 3.41708458056787, 3.5601140193706695, 3.2716975681804294, 3.6935381968951635, 3.056674324042616, 3.390742842322652, 3.264025516809666, 3.2112064731888252, 3.0727648412688326, 3.3084536735133154, 3.032015888137004, 3.2958507898337883, 3.232251277024231, 2.986057617135074, 3.1528859573792745, 3.153465094707217, 3.0627514335466532, 3.1587208415510415, 3.1009169816420927, 3.227325618666937, 3.1579588382432986, 3.014408785077033, 3.057157237588579, 2.8511344542482275, 2.94456240873016, 2.738532367511028, 2.940541340956716, 3.036667627336161, 2.84933820507826, 2.6412367493705022, 2.9490295472547925, 2.854043831478824, 2.8256707631802476, 2.733756539450612, 2.961846878332856, 2.6751169206253937, 2.6532443102058294, 2.796266967552968, 2.845611269168009, 2.745646912249081, 2.714918989512767, 2.8327800772847684, 2.8648010753542663, 2.352979525868403, 2.841878870325681, 2.75133902078116, 2.4515225943203034, 2.571182293049187, 2.5570190275047393, 2.60604254861962, 2.5475434720460592, 2.6705208246827836, 2.5202941221936204, 2.529845545555128, 2.559705068890379, 2.4096183088287777, 2.4676328737066786, 2.448355999231575, 2.5957637313281774, 2.4063844872598383, 2.4185853944277804, 2.3937533397450776, 2.447015794418284, 2.372766870915248, 2.4189937473327596, 2.4913243964871468, 2.358449007739728, 2.1322958255257274, 2.3692751889239867, 2.2259187168014662, 2.2426628154598083, 2.2354390112043205, 2.4748248742011603, 2.3820958941899337, 2.091909623233306, 2.23108965840365, 2.4214782687054033, 2.364007318874242, 2.1936703962012065, 2.2751703017934006, 2.2904229265389975, 2.2056927879074353, 2.2193927206880177, 2.184850777360693, 2.2300277320203743, 2.086159758112264, 2.152517690017514, 2.166838274357234, 2.1612217039401123, 2.0979712866126876, 1.9330506534401133, 1.9843234061571349, 2.182639087544291, 2.168742644152198, 1.9522709272174872, 1.8594112719270781, 2.075629514206955, 2.1360730347616794, 2.118656995356318, 1.923947036119319, 2.0646920482747246, 1.9731547503032691, 2.0224963170080192, 1.956102062822736, 1.9255263561512272, 2.0829376424235817, 1.9869986142360179, 2.1266327981089135, 1.981046264112046, 1.7707012108281805, 2.004014987907428, 1.9064671449569408, 1.789306289069134, 2.0031772186530423, 1.951888942712709, 2.0684493997209623, 2.012941248198811, 1.854386096259146, 1.7385167620987187, 1.9910186758852162, 1.7650525806895991, 1.867293700107874, 1.9427923520687735, 1.7775893104405287, 1.9741892614672745, 1.834574810461256, 1.8795376856364558, 1.900696997296787, 1.7803496302566248, 1.6291495485237355, 1.8830818369912594, 1.8578709959515092, 1.8358273599828578, 1.7247666671440312, 1.7041418836724855, 1.79895193542895, 1.7754228888896209, 1.771681067605266, 1.6986649406503738, 1.7459969461629299, 1.690323815384143, 1.6410536286357726, 1.7705128029260881, 1.6942294152217878, 1.7614029671136997, 1.6376874873424192, 1.6085553918343702, 1.6971100279842004, 1.6618929345870022, 1.7952695444684001, 1.5080217173388117, 1.469484510894605, 1.5166092279071115, 1.783380917917183, 1.6280824972031396, 1.6985174083717536, 1.4643009147055115, 1.50600015656098, 1.695860560582113, 1.5123238197475766, 1.6550806742502018, 1.5292410027010048, 1.6364844358359063, 1.4681194160214188, 1.6213519474620284, 1.6118415058388762, 1.4133100276451513, 1.7027471506482692, 1.5659931215680507, 1.4831898183829944, 1.5650486035414444, 1.5333729491659103, 1.5241574626906105, 1.6017630461881187, 1.7199028473278604, 1.574199796073228, 1.615918087946039, 1.4833082074267085, 1.5667851590068294, 1.4063738221798978, 1.4780711181517256, 1.541599996999094, 1.4190952038655031, 1.6193394862936294, 1.5796659499941441, 1.5728251712315864, 1.6478520084041304, 1.4446214007468081, 1.4786329382698, 1.412710195200567, 1.3844887262100107, 1.4844814316763792, 1.4250589022331253, 1.3119076602758977, 1.4430103295086432, 1.4282898210036161, 1.4379731816009016, 1.4543492516434349, 1.5360775623715022, 1.2879218915125994, 1.5032005574302283, 1.4997272474007068, 1.3281820935006918, 1.4412784264800438, 1.3487594176249782, 1.5543778175813372, 1.3698411345306554, 1.4503408068815382, 1.3441772937416312, 1.301327936709397, 1.4689162891476426, 1.3089366230511112, 1.384263983469402, 1.5709261174151203, 1.197717544460511, 1.294859762075167, 1.3645529406725947, 1.1982492652614414, 1.3038468550397913, 1.2563157839909949, 1.3071673632024516, 1.436084754570576, 1.243646611424221, 1.470177827363592, 1.3594249325035923, 1.3532445113526745, 1.405188942268351, 1.1216651345060713, 1.284077824978679, 1.2807911390197417, 1.2505293464773755, 1.4585885260260485, 1.2542956365937588, 1.1811791032049237, 1.4079116109276482, 1.151500363499028, 1.0962549653513243, 1.3247353050796622, 1.383888172664979, 1.251536839995403, 1.274817382720093, 1.318015348503719, 1.207386455313411, 1.2876517534391991, 1.2442387602876215, 1.3178126351476238, 1.2095579805713552, 1.2568178285000133, 1.364929893006162, 1.2383947268752806, 1.2943304732103607, 1.1222237397559642, 1.1598699458077841, 1.2549622409330876, 1.2771874363533433, 1.2151275729727053, 1.1546678624947986, 1.2379065756836474, 1.2299876690179163, 1.21195813910303, 1.2403408701852237, 1.1361282531306822, 1.081551246576589, 1.228005480031456, 1.1541407041513234, 1.0859385779322441, 1.2100157425189213, 1.197252647869721, 1.0635448958036018, 1.149641519529078, 1.0810728008161423, 1.2877285598392354, 1.2033955012174382, 1.1794287097712879, 1.2890214749280462, 1.0155502129073464, 1.1167289480296976, 1.1652382974895499, 0.893355557391434, 1.2203424734258144, 1.393591468806581, 1.0821874774401385, 1.0507171999197693, 1.095056792309148, 1.2030443919383447, 1.1228481947619844, 1.1542435377242168, 1.0573688883524293, 1.1331237226287298, 1.1929461674892934, 1.1750848164517111, 1.1094514189151194, 1.1590125910326348, 1.2557208014926313, 1.1956505266227588, 1.0403075845400958, 1.2194982696083592, 1.1329693765247992, 1.16576602072309, 1.2507022753897916, 1.0399534499575196, 0.9754722345215749, 1.0682157622964656, 1.0555916334814666, 1.125846934131404, 0.9881271315152304, 1.1057710987603755, 1.0975720772715252, 0.9888269366516763, 1.0435716444214027, 1.0180646717289172, 1.0544246043165264, 1.1291272034663316, 1.1035429599709243, 1.0495409143570735, 1.0216790326452214, 0.9678858565158218, 1.2037787973384682, 1.0779978127532321, 1.2225304857909176, 0.9390206288638846, 0.9372292466121945, 1.0491795159529407, 1.0228242817770044, 0.9653905126402552, 1.1040048909616815, 0.9991048184911592, 1.0402262381031315, 1.0262054680401214, 1.075512761972382, 0.9957015101274922, 0.9954622838893004, 1.0230988266864576, 0.9999614589247934, 0.9641281579886817, 0.991703830487516, 0.9756492237080574, 0.8951514348915501, 1.0506253372387488, 0.9095182693126364, 0.9469269287596629, 1.0390051987015365, 1.0565683356056763, 1.0220749093240025, 1.0396961979365815, 0.8949323544520337, 1.0324902278057906, 0.9411695368239589, 0.875455411188632, 0.8894440293316491, 0.872672542767081, 1.0756039593656854, 0.902496140745792, 0.9697196168636095, 1.0769050104530047, 0.9919460747333498, 0.919437268549427, 0.976575572320684, 1.0346087391782013, 0.9518908043851132, 0.8855025123020253, 0.9430250011371712, 0.8627052971507402, 1.0109759280216042, 0.9088170193031642, 0.8415765647231443, 0.9463250722956368, 0.855484740911286], 'mae': [10.482096, 3.6114776, 2.8708832, 2.5916884, 2.4901974, 2.3954685, 2.2678182, 2.1842701, 2.2311149, 2.1465707, 2.1354012, 2.0649688, 2.0866392, 2.0381052, 2.0088122, 1.9928917, 1.9749875, 1.9686183, 1.9332622, 1.9401271, 1.866943, 1.8635213, 1.8968366, 1.8687851, 1.9052924, 1.856486, 1.8901635, 1.8247482, 1.831379, 1.8158753, 1.8503424, 1.7872455, 1.7599026, 1.8308815, 1.7419145, 1.7352698, 1.7175361, 1.6777364, 1.6897978, 1.7412083, 1.7810444, 1.676279, 1.6511055, 1.6713247, 1.6321424, 1.661678, 1.6269443, 1.7066078, 1.6216681, 1.5868319, 1.5906131, 1.5907907, 1.5852118, 1.560473, 1.5173509, 1.5539408, 1.5407107, 1.554221, 1.466442, 1.5442448, 1.5599812, 1.5142719, 1.5541242, 1.4941803, 1.483987, 1.5418036, 1.5024867, 1.5240748, 1.4650421, 1.4407516, 1.5236013, 1.44078, 1.4104234, 1.4641145, 1.5234839, 1.382189, 1.4550805, 1.4101851, 1.3885417, 1.365404, 1.3994638, 1.4213473, 1.4152331, 1.3517108, 1.2917843, 1.365826, 1.3713081, 1.357422, 1.3009424, 1.3889592, 1.3527607, 1.3196917, 1.3409245, 1.2806695, 1.3261639, 1.3266332, 1.365226, 1.3241893, 1.2807844, 1.3203187, 1.2741213, 1.3451463, 1.3269085, 1.2961507, 1.3004655, 1.3084195, 1.2937962, 1.2769984, 1.2491632, 1.2914573, 1.246104, 1.2091552, 1.2520111, 1.2214187, 1.2661319, 1.2246609, 1.2634604, 1.2860571, 1.1809905, 1.2490638, 1.230062, 1.2343036, 1.1875054, 1.2781512, 1.19517, 1.2473437, 1.2314813, 1.2146441, 1.1768861, 1.1902386, 1.1924978, 1.1751206, 1.1690513, 1.2089164, 1.1806616, 1.2227143, 1.147866, 1.1815406, 1.1361841, 1.1381599, 1.1551913, 1.1439433, 1.153801, 1.1732438, 1.172611, 1.1085799, 1.1442795, 1.1540902, 1.1563008, 1.157643, 1.1099519, 1.1710252, 1.1472089, 1.168373, 1.1419457, 1.0753236, 1.1554062, 1.1298808, 1.112866, 1.0983319, 1.0874014, 1.095856, 1.1217494, 1.0890387, 1.0967801, 1.1050973, 1.1133102, 1.1085547, 1.1118286, 1.0408738, 1.0591404, 1.0741059, 1.0931264, 1.0772793, 1.04897, 0.99457896, 1.0847737, 1.089983, 1.0051361, 1.057249, 1.0643559, 1.0417764, 1.0522181, 1.1011921, 1.0497586, 1.0215402, 1.0501093, 1.0206487, 1.045992, 1.0161185, 1.0460565, 1.0262389, 1.0303304, 0.9922427, 1.0654937, 1.0311725, 1.0297695, 1.0287026, 1.0628339, 0.9566405, 1.0272716, 0.974864, 0.9933091, 0.9801704, 1.0025318, 1.0669578, 0.97631824, 0.98447883, 1.0226585, 1.0601394, 0.9732974, 1.0015936, 1.002618, 0.98361295, 1.016186, 0.97066194, 0.9713178, 0.98959714, 0.9740524, 0.98612183, 0.9724407, 0.98856467, 0.93875223, 1.0028433, 1.0103636, 1.021373, 0.94393116, 0.9829925, 0.9951026, 1.0153829, 0.96889657, 0.9292851, 0.97135055, 0.9953045, 0.9926218, 0.96617365, 0.97039634, 0.9662118, 0.9517622, 0.9660281, 1.0101291, 0.9275819, 1.0023869, 0.94923234, 0.9652578, 0.9413573, 0.9973347, 0.98025787, 0.914862, 0.96411043, 0.9318554, 0.9753368, 0.9425804, 0.9732989, 0.9675198, 0.9442445, 0.9511916, 0.98853755, 0.9817077, 0.9347768, 0.96163195, 0.91568273, 0.96610916, 0.9099146, 0.94347966, 0.9001249, 0.96387756, 0.9565243, 0.92974544, 0.9621499, 0.90958154, 0.8959957, 0.9439769, 0.8540991, 0.8876403, 0.91488236, 0.9137954, 0.87225837, 0.9058336, 0.8867532, 0.9124754, 0.96541965, 0.8706223, 0.88247013, 0.9279864, 0.9379519, 0.9055288, 0.88696754, 0.8623025, 0.91712874, 0.90328175, 0.89753485, 0.9235155, 0.87788427, 0.9340895, 0.8953519, 0.91486067, 0.89913094, 0.9180232, 0.91435206, 0.91121125, 0.8748154, 0.89695865, 0.93051326, 0.881844, 0.89455706, 0.92033935, 0.8727554, 0.88233125, 0.84775585, 0.9039527, 0.8166607, 0.86551285, 0.8926078, 0.90627426, 0.91319764, 0.87867886, 0.8825418, 0.90004355, 0.8747528, 0.8801249, 0.84722066, 0.82172024, 0.85836196, 0.80954415, 0.800795, 0.8419467, 0.8420734, 0.86277616, 0.88089657, 0.8886698, 0.8533973, 0.88029593, 0.8938006, 0.8154171, 0.85497916, 0.8725614, 0.88371414, 0.8537776, 0.84278685, 0.8147765, 0.82032347, 0.8563922, 0.83212644, 0.84033203, 0.8865235, 0.799892, 0.8444317, 0.8718266, 0.7955815, 0.8502285, 0.8134441, 0.8316831, 0.8474228, 0.8352442, 0.8661115, 0.84175396, 0.8373292, 0.8834805, 0.78944874, 0.80510354, 0.84464717, 0.8081738, 0.8509568, 0.8086682, 0.8209047, 0.8466426, 0.78600115, 0.7759953, 0.7875033, 0.8207716, 0.8195173, 0.81070703, 0.83479375, 0.78960973, 0.8466411, 0.8010305, 0.8071373, 0.8165642, 0.7973323, 0.8167554, 0.79134566, 0.8467136, 0.7623888, 0.7668035, 0.83622044, 0.8286523, 0.82684845, 0.8019483, 0.7765291, 0.81079084, 0.8255889, 0.8165576, 0.7931514, 0.77243143, 0.81538767, 0.7616216, 0.7820858, 0.80844283, 0.7972698, 0.75449073, 0.7807795, 0.7401558, 0.8045355, 0.76279885, 0.7985192, 0.80781573, 0.7328736, 0.801391, 0.77203614, 0.71612173, 0.7463951, 0.81319255, 0.75455904, 0.73055667, 0.7650609, 0.7665845, 0.7684506, 0.77513266, 0.7785531, 0.7688829, 0.75275296, 0.78673494, 0.78511935, 0.7780806, 0.7886314, 0.77386683, 0.7593762, 0.76084167, 0.78045857, 0.8272445, 0.76364535, 0.76072943, 0.75758064, 0.7637898, 0.77939993, 0.7705034, 0.7561452, 0.74352294, 0.7537622, 0.74384314, 0.75632584, 0.7562607, 0.7608113, 0.75253755, 0.7774121, 0.76098096, 0.71653056, 0.74034274, 0.7929444, 0.76332104, 0.7802674, 0.70992064, 0.70627886, 0.7560445, 0.76626056, 0.75032455, 0.7736787, 0.7411175, 0.76692164, 0.7460987, 0.75177246, 0.708163, 0.70820606, 0.6976229, 0.7155028, 0.72022146, 0.733814, 0.7349209, 0.6694964, 0.7332616, 0.69342124, 0.6893779, 0.73255354, 0.7488694, 0.75076663, 0.74310035, 0.6978421, 0.6830059, 0.70936996, 0.6955235, 0.71154934, 0.6545219, 0.7474175, 0.69270766, 0.7331063, 0.7407678, 0.7177447, 0.6954927, 0.7500192, 0.73790944, 0.714143, 0.68724436, 0.71528405, 0.69827473, 0.70133764, 0.70638144, 0.67935973, 0.7340099, 0.70512515]}\n"
     ]
    }
   ],
   "source": [
    "print( history_dict ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-662-5817204bb8d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss' ] \n",
    "val_loss= history.history['val_loss' ] \n",
    "epochs = range( 1 , len(loss) + 1 ) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot( epochs , loss_values , 'bo' , label = 'Training loss' )\n",
    "plt.plot( epochs , val_loss_values , 'b' , label = 'Validation loss' ) \n",
    "plt.title( 'Training vs validation loss' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'Loss' ) \n",
    "plt.legend() \n",
    "\n",
    "# Here we plot the training and validation accurary side by side.\n",
    "\n",
    "\n",
    "acc = history.history['acc' ] \n",
    "val_acc = history.history['val_acc' ]\n",
    "\n",
    "\n",
    "epochs = range( 1 , len(loss_values) + 1 ) \n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot( epochs , acc_values , 'bo' , label = 'Training acc' )\n",
    "plt.plot( epochs , val_acc_values , 'b' , label = 'Validation acc' ) \n",
    "plt.title( 'Training vs validation acc' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'acc' ) \n",
    "plt.legend() \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply to test data <a name = 'example2_classification2_apply_test'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 0s 142us/step\n",
      "[0.9962994980153927, 0.7809438705444336]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add( layers.Dense( 64 , activation = 'relu' , input_shape = (10000 , ) ) ) \n",
    "model.add( layers.Dense( 64 , activation = 'relu' ) ) \n",
    "model.add( layers.Dense( 46 , activation = 'softmax' ) ) \n",
    "\n",
    "model.compile( optimizer = 'rmsprop' , \n",
    "             loss = 'categorical_crossentropy' , \n",
    "             metrics = ['accuracy' ] ) \n",
    "\n",
    "model.fit( partial_x_train , \n",
    "         partial_y_train , \n",
    "         epochs = 9 , batch_size = 512 , \n",
    "         validation_data = (x_val , y_val ) , verbose = 0 ) \n",
    "\n",
    "results = model.evaluate( x_test , one_hot_test_labels ) \n",
    "\n",
    "print( results ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate prediction with new data  <a name = 'prediction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2246, 46)\n",
      "[0.99999994 1.0000001  0.99999994 ... 0.9999999  1.0000001  1.        ]\n",
      "(2246,)\n",
      "[ 3 10  1 ...  3  3  1]\n",
      "(2246,)\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict( x_test ) \n",
    "\n",
    "\n",
    "print( prediction.shape ) \n",
    "\n",
    "print( np.sum( prediction , axis = 1 )  ) \n",
    "print( np.sum( prediction , axis = 1 ).shape  ) \n",
    "\n",
    "print( np.argmax( prediction , axis = 1 ) ) \n",
    "print( np.argmax( prediction , axis = 1 ).shape ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different way to handle labels and loss<a name = 'diff_way_handle_label_loss'></a>        \n",
    "\n",
    "We mention in [Building NN](#example2_classification2_build_NN), that we can pre-process the labels as integers, rather than using hte on-hot encoding. Then we shall use the **sparse_categorical_crossentropy** as our loss function. Let's see how it performs. We shall do it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982,)\n",
      "(8982,)\n",
      "(2246,)\n",
      "(2246,)\n",
      "Model: \"sequential_88\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_249 (Dense)            (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 647,214\n",
      "Trainable params: 647,214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 2s 227us/step - loss: 2.6258 - accuracy: 0.5407 - val_loss: 1.7495 - val_accuracy: 0.6490\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 1.4314 - accuracy: 0.7167 - val_loss: 1.2934 - val_accuracy: 0.7170\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 1.0400 - accuracy: 0.7879 - val_loss: 1.1232 - val_accuracy: 0.7560\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 1s 118us/step - loss: 0.8122 - accuracy: 0.8315 - val_loss: 1.0254 - val_accuracy: 0.7920\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 1s 133us/step - loss: 0.6462 - accuracy: 0.8691 - val_loss: 0.9505 - val_accuracy: 0.8070\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 1s 157us/step - loss: 0.5181 - accuracy: 0.8954 - val_loss: 0.9347 - val_accuracy: 0.7950\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 1s 146us/step - loss: 0.4181 - accuracy: 0.9172 - val_loss: 0.8943 - val_accuracy: 0.8160\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 1s 139us/step - loss: 0.3428 - accuracy: 0.9295 - val_loss: 0.8975 - val_accuracy: 0.8110\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s 113us/step - loss: 0.2858 - accuracy: 0.9384 - val_loss: 0.8984 - val_accuracy: 0.8110\n",
      "2246/2246 [==============================] - 0s 161us/step\n",
      "[0.9789952244593008, 0.7845057845115662]\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "( train_data , train_labels ) , ( test_data , test_labels ) = reuters.load_data( num_words = 10000 )\n",
    "\n",
    "print( train_data.shape ) \n",
    "print( train_labels.shape ) \n",
    "print( test_data.shape ) \n",
    "print( test_labels.shape ) \n",
    "\n",
    "def vectorize( sequence , dimension ): \n",
    "    result = np.zeros( ( len(sequence) , dimension ) ) \n",
    "    for i , data in enumerate( sequence ):\n",
    "        result[ i , data ] = 1.\n",
    "    return result \n",
    "\n",
    "train_data_vec = vectorize( train_data , 10000 )  \n",
    "test_data_vec = vectorize( test_data , 10000 ) \n",
    "\n",
    "# # This is the one-hot way to vectorize the labeling, with acc ~ 0.7876\n",
    "# train_labels_vec = vectorize( train_labels , 46 ) \n",
    "# test_labels_vec = vectorize( test_labels , 46 ) \n",
    "\n",
    "# This is the integer way to pre-process the labels, with acc ~ 0.784, not much different\n",
    "train_labels_vec = np.array( train_labels ) \n",
    "test_labels_vec = np.array( test_labels ) \n",
    "\n",
    "from keras import Sequential \n",
    "model = models.Sequential() \n",
    "model.add( layers.Dense( 64 , activation = 'relu' , input_shape = ( 10000 , ) ) ) \n",
    "model.add( layers.Dense( 64 , activation = 'relu' ) ) \n",
    "model.add( layers.Dense( 46 , activation = 'softmax' ) ) \n",
    "\n",
    "model.summary() \n",
    "\n",
    "# # This is the way we compile the model if we vectorize the labls using one-hot way\n",
    "# model.compile( optimizer = 'rmsprop' , loss = 'categorical_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "# This is the way we compile the model if we vectorize the labls using the integer way \n",
    "model.compile( optimizer = 'rmsprop' , loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'] )\n",
    "\n",
    "\n",
    "train_data_vec_1 = train_data_vec[ : 1000 ] # For validation \n",
    "train_data_vec_2 = train_data_vec[ 1000 : ] # For actual training\n",
    "train_labels_vec_1 = train_labels_vec[ : 1000 ] # For validation \n",
    "train_labels_vec_2 = train_labels_vec[ 1000 : ] # For actual training\n",
    "\n",
    "model.fit( train_data_vec_2 , train_labels_vec_2 , \n",
    "          epochs = 9 , batch_size = 512 , \n",
    "          validation_data = ( train_data_vec_1 , train_labels_vec_1 ) ) \n",
    "\n",
    "result = model.evaluate( test_data_vec , test_labels_vec ) \n",
    "print( result ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the example <a name = 'example2_classification2_summary'></a>\n",
    "1. For N-class classification, the Dense layer should be of size N\n",
    "2. For single-label, multi-class classificaiton, end your CNN with **softmax** activation s.t. the output is a prob over all the N class\n",
    "3. Use **categorical_crossentropy** for the loss function\n",
    "4. Two ways to handle the labels (there are N of them)\n",
    "    1. One-hot encoding with categorical_crossentropy as loss function\n",
    "    2. Encode the labels as integer, and use **sparse_categorical_crossentropy** as loss function\n",
    "5. Always make sure that the size of the intermediate layers are BIGGER than N, the number of class that you try to classify    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of regression: House prices <a name = \"example_regression\"></a>\n",
    "Different from previous two examples, **regression** consistes of predicting a **continuous** values instead of discrete ones. Here the example is the Boston housing price as a function of such as crime rate, local property etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data <a name = 'example3_get_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n",
      "(404,)\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: loads the data \n",
    "\n",
    "from keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data,test_targets) = boston_housing.load_data() \n",
    "print( train_data.shape )\n",
    "print( test_data.shape )\n",
    "print( train_targets.shape )\n",
    "print( test_targets.shape )\n",
    "# print( train_targets )\n",
    "\n",
    "# There are 402 training samples, each with 13 features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data <a name = 'example3_pre_data' ></a>\n",
    "\n",
    "Since our data of different nature, we should normalize them to between 0 and 1. It would be best if they could be center at 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.74511057e+00 1.14801980e+01 1.11044307e+01 6.18811881e-02\n",
      " 5.57355941e-01 6.26708168e+00 6.90106436e+01 3.74027079e+00\n",
      " 9.44059406e+00 4.05898515e+02 1.84759901e+01 3.54783168e+02\n",
      " 1.27408168e+01]\n",
      "[9.22929073e+00 2.37382770e+01 6.80287253e+00 2.40939633e-01\n",
      " 1.17147847e-01 7.08908627e-01 2.79060634e+01 2.02770050e+00\n",
      " 8.68758849e+00 1.66168506e+02 2.19765689e+00 9.39946015e+01\n",
      " 7.24556085e+00]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare the data\n",
    "\n",
    "# It seems like the mean() is doing it column by column \n",
    "# It actually depends on the \"axis\" arguemnt. See Ch 10\n",
    "mean = train_data.mean( axis=0 ) # It seems like we don't need \"axis\"\n",
    "print( mean ) \n",
    "train_data -= mean \n",
    "std = train_data.std( axis=0 ) # Not sure why \"axis\" is here...\n",
    "print( std ) \n",
    "train_data /= std \n",
    "test_data -= mean \n",
    "test_data /= std\n",
    "\n",
    "# Note here we normalizet the test data using the mean \n",
    "# and std from the training data. You should NEVER use \n",
    "# any info from the test data, even for data normalizatin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building NN <a name = 'example3_build_NN' ></a>\n",
    "Now since we have only 402 training samples, there are very FEW samples, in this sense. Normally there will be very severe over-fitting issue. Thus we can pick a very small NN (less parameters) to overcome this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build the NN or model\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model(): # We run a fun as we will need it a few times\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64 , activation = 'relu',input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64,activation='relu'))\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    # Two things to note above. One is that there is only one unit and two is that\n",
    "#     there is no activation. Thus this is a linear layer, which allows us to predict\n",
    "#     a scalar which can take any value (as there is no activation, if there were say \n",
    "#     \"sigmoid\", then the value is restricted to be between 0 and 1.                                    \n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='mse',metrics=['mae'])\n",
    "    # mse is mean square error, mae is mean absolute error\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation: K-fold validation <a name = 'example3_validation'></a>\n",
    "Since there are too few datas, we use K-fold cross validations. It goes like this. First we RANDOMLY split the avaliable data into K parts, (oftenly K = 4,5); Second we pick one as validation, and the rest K-1 parts as training. Third, we train the NN with a validaiton score (losa or accurary); Fourth, we repeat the above proecess, and average over the validaiton score as our final validaiton scores. \n",
    "\n",
    "The take home here is that indeed the validation score for each partition are very differetn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n",
      "[2.2317609786987305, 2.405125141143799, 2.8976359367370605, 2.5846455097198486]\n",
      "2.5297918915748596\n",
      "[9.925135857988112, 11.646908316281763, 16.894660109340556, 13.920438051223755, 13.091492619844946, 14.573289210253423, 12.890148861573474, 12.968337729425713, 9.310137420597643, 9.129610651790506, 16.430710325146666, 13.700343532137351]\n",
      "12.87343439046699\n",
      "--- 104.11889505386353 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time() \n",
    "\n",
    "import numpy as np\n",
    "k = 4  # Numer of partitions\n",
    "num_val_samples = len( train_data) // k  ## // means do the division and take only the integer part\n",
    "num_epochs = 100 \n",
    "all_scores_mae = []\n",
    "all_scores_loss = []\n",
    "\n",
    "for i in range(k):\n",
    "    print( 'processing fold #' , i)\n",
    "    val_data = train_data[ i * num_val_samples : (i+1) * num_val_samples ]\n",
    "    val_targets = train_targets[ i * num_val_samples : (i+1) * num_val_samples ]\n",
    "    \n",
    "    partial_train_data = np.concatenate( [ train_data[ : i * num_val_samples] , \n",
    "                                         train_data[ (i+1) * num_val_samples:]] , \n",
    "                                       axis=0 )\n",
    "    # The axis along which the arrays will be joined. If axis is None, arrays \n",
    "    # are flattened before use. Default is 0.\n",
    "    partial_train_targets = np.concatenate( [ train_targets[ : i * num_val_samples] , \n",
    "                                         train_targets[ (i+1) * num_val_samples:]] , \n",
    "                                       axis=0 )\n",
    "    model = build_model()\n",
    "    model.fit( partial_train_data , partial_train_targets , epochs = num_epochs , \n",
    "              batch_size = 1 , verbose=0) \n",
    "    # verbose means train the model in the silent mode so that \"epoch\" will not be printed\n",
    "    val_mse , val_mae = model.evaluate( val_data , val_targets , verbose = 0 )\n",
    "    all_scores_mae.append(val_mae)\n",
    "    all_scores_mse.append(val_mse)    \n",
    "#     Note the over-fitting problem\n",
    "print(all_scores_mae) \n",
    "print( np.mean( all_scores_mae ) )\n",
    "print(all_scores_mse) \n",
    "print( np.mean( all_scores_mse ) )\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation: Plot the losses <a name = 'example3_validation2'></a>\n",
    "Here we repeat the above with diffrent number of epochs and see how the score changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "dict_keys(['val_loss', 'val_mae', 'loss', 'mae'])\n",
      "processing fold # 1\n",
      "dict_keys(['val_loss', 'val_mae', 'loss', 'mae'])\n",
      "processing fold # 2\n",
      "dict_keys(['val_loss', 'val_mae', 'loss', 'mae'])\n",
      "processing fold # 3\n",
      "dict_keys(['val_loss', 'val_mae', 'loss', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 622.9338400363922 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500 \n",
    "all_mae_histories = []\n",
    "all_loss_histories = []\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "# loss_values = history_dict['loss']\n",
    "# val_loss_values = history_dict['val_loss']\n",
    "\n",
    "\n",
    "k = 4 # Number of partition\n",
    "for i in range(k):\n",
    "    print( 'processing fold #' , i ) \n",
    "    val_data = train_data[ i * num_val_samples : (i+1) * num_val_samples ]\n",
    "    val_targets = train_targets[ i * num_val_samples : (i+1) * num_val_samples ]\n",
    "    \n",
    "    partial_train_data = np.concatenate( [ train_data[ : i * num_val_samples ] , \n",
    "                                         train_data[ (i+1) * num_val_samples: ] ] , \n",
    "                                       axis=0 )\n",
    "    partial_train_targets = np.concatenate( [ train_targets[ : i * num_val_samples ] , \n",
    "                                         train_targets[ (i+1) * num_val_samples: ] ] , \n",
    "                                       axis=0 )\n",
    "    model = build_model() \n",
    "    history = model.fit( partial_train_data , partial_train_targets , \n",
    "                       validation_data = (val_data , val_targets) , \n",
    "                        epochs = num_epochs , batch_size = 1 , verbose = 0 )\n",
    "    \n",
    "    history_dict = history.history \n",
    "    print( history_dict.keys() )  \n",
    "    mae_history = history_dict[ 'val_mae' ] \n",
    "    loss_history = history_dict[ 'val_loss' ] \n",
    "    \n",
    "#     print( history.history.keys() ) \n",
    "#     mae_history = history.history[ ' val_mean_absolute_error' ] \n",
    "    \n",
    "    all_mae_histories.append( mae_history )\n",
    "    all_loss_histories.append( loss_history )\n",
    "\n",
    "\n",
    "average_mae_history = [ np.mean( [ x[i] for x in all_mae_histories ] ) for i in range(num_epochs ) ] \n",
    "average_loss_history = [ np.mean( [ x[i] for x in all_loss_histories ] ) for i in range(num_epochs ) ] \n",
    "\n",
    "# print( history_dict.keys() )  \n",
    "# print( all_mae_histories.shape ) \n",
    "# print( all_loss_histories.shape ) \n",
    "# print( average_mae_history.shape ) \n",
    "# print( average_loss_history.shape ) \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range( 1 , len(average_mae_history) + 1 ) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot( epochs , average_mae_history )\n",
    "plt.title( 'Validation MAE by epoch' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'Validation MAE' ) \n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot( epochs , average_loss_history )\n",
    "plt.title( 'Validation loss by epoch' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'Validation loss' ) \n",
    "\n",
    "plt.show() \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To make a better plot\n",
    "# The take home is that it stats over-fitting after epoch=80\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot( epochs[10:] , average_mae_history[10:] )\n",
    "plt.title( 'Validation MAE by epoch' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'Validation MAE' ) \n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot( epochs[10:] , average_loss_history[10:] )\n",
    "plt.title( 'Validation loss by epoch' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'Validation loss' ) \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply to test data <a name = 'example3_apply_test'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 372us/step\n",
      "15.925643322514553 2.562500238418579\n"
     ]
    }
   ],
   "source": [
    "# According to the above, the validation MAE stops improving \n",
    "# after 80 epochs, so let's try that with the test data. \n",
    "model = build_model() \n",
    "model.fit( train_data , train_targets , epochs = 80 , \n",
    "         batch_size = 16 , verbose = 0 ) \n",
    "test_mse_score , test_mae_score = model.evaluate( test_data , test_targets ) \n",
    "\n",
    "print( test_mse_score , test_mae_score ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the example <a name = 'example3_summary' ></a>\n",
    "1. Regression use **MSE** as loss function, rather than binaryentroy for the classifcaiton problem\n",
    "2. We use **MAE** as our evaluation metric rather than \"accuracy\"\n",
    "3. Remember to normalize the data if they have different natures.\n",
    "4. Use K-fold validation for tuning the NN\n",
    "5. Use smaller network (shallower, less node, less epochs) to avoid over-fitting if less data is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is validation? \n",
    "It is during the process of you adjusting say the depth or the number of nodes, or number of epochs in your network, you want to know the performaces of your NN. You want to have a \"test set\" within your training set for \"validation\". The measure of this validation is called the validation score (a kind of measure). Most of the time, we use K-fold validation (it is necessary if your data is too less), which split the training data into training set and a validation set. \n",
    "\n",
    "1. split the data into K pieces; \n",
    "\n",
    "2. use the first piece as validation and the rest K-1 pieces as training; \n",
    "\n",
    "3. obtain a validation score; \n",
    "\n",
    "4. repeat step 2-3 for using other piece as validation; \n",
    "\n",
    "5. average the validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the chapter <a name = \"summary_ch3\"></a>\n",
    "1. Rember to process the data because you feed it into the NN, such as normalization for regrssion problem\n",
    "2. Use different loss function and different measure for different problem. But this is standard.\n",
    "3. Check the [summary of the regression example](#example3_summary) for the regresson problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 4. Fundamentals of ML<a name=\"Ch4\"></a>\n",
    "Most of these chapters is to re-enforce and summarize what we laernt in the previous chatpers, and mostly **descriptive**, except there is a detailed section on how to tune the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four kinds of ML <a name = '4ML' ></a>\n",
    "1. Supervised learning (classifcaiton, regression, language and image stuff)\n",
    "2. un-supervised learning (dimension reduction, clustering) \n",
    "3. self-supervised learning (not worried abt it) \n",
    "4. Reinforcement learning (Deepmind did it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of ML models <a name = 'Evaluation_ML'></a>\n",
    "Here we simply list out some practical strategies in perfecting the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, validation and test sets <a name = 'TrainValidateTest'></a>\n",
    "\n",
    "1. We need to validate the model, before we apply it to the test set. That said, we need to split the available data into three sets, training, validation and test. 1st you train on the training set, and tune the model with the validation set (with K-fold validation say) to adjust the depth, nodes, and epochs. Once these are ready, you apply the model to the test data for ONE FINAL TIME.\n",
    "\n",
    "2. Info leak. That is the phenomena happening when you train your NN on the validation set, that the info of validation set goes into the NN, which is of course the case. The thing is that you don't want your NN to do too well on either traiing or validation set, which will result in over-fitting. \n",
    "\n",
    "\n",
    "3. The issue of ML is the competition of optimization (for adjusting the model such that the loss function is minimized for given training data) and the generalizaiton (for good performance on unknown data). At the beginning of the training, the lower of the loss for the training data, the lower the loss for the test data. This is under-fit. After some iteration, the model start overfit which means it learns features that are specific to the training data, but not in others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three validation methods (key subsection) <a name = '3validation' ></a>\n",
    "Three ways to validate your NN, we shall see examples below\n",
    "1. Simple hold-out validaiton (K=2 fold validation, the problem for too less data is that if you shuffle the data the measure is changed a lot)\n",
    "2. K-fold validation\n",
    "3. Iterated K-fold validation with shuffling (It is applying K-fold validation several times, shuffling the data everytime before splitting them to trianing and validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold out validation\n",
    "num_validation_sample = 10000 \n",
    "np.random.shuffle(data) # Shuffle the data\n",
    "\n",
    "validation_data = data[ : num_validation_sample ] # Define the validaton set\n",
    "training_data = data[ num_validation_sample : ] # Define the validaton set\n",
    "\n",
    "model = get_model() \n",
    "\n",
    "model.training( train_data ) \n",
    "\n",
    "validation_score = model.evaluate( validation_data ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold validation\n",
    "k = 4\n",
    "num_validation_sample = len( data ) // k # Here only take the integer part\n",
    "np.random.shuffle( data ) \n",
    "\n",
    "validation_score = [] \n",
    "\n",
    "for fold in range[k]: \n",
    "    validation_data = data[ num_validation_sample * fold : \n",
    "                          num_validation_sample * (fold+1) ] \n",
    "    \n",
    "    training_data = data[ : num_validation_sample * fold ] + data[ num_validation_sample * (fold+1) ] \n",
    "    \n",
    "    model = get_model() \n",
    "    model.train( training_data ) \n",
    "    validation_score = model.evaluate( validation_data ) \n",
    "    validation_scores.append( validation_score ) \n",
    "\n",
    "validation_score = np.average( validation_scores ) \n",
    "    \n",
    "model = get_model()\n",
    "model.training( data ) \n",
    "test_score = model.evaluate( test_data ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterated K-fold validation\n",
    "# You simply repeat the above procedure P times, and each time you re-shuffle the data\n",
    "\n",
    "P = 5 # Shuffle P times\n",
    "k = 4 # K-fold validation\n",
    "num_validation_sample = len( data ) // k # Here only take the integer part\n",
    "\n",
    "\n",
    "validation_score = [] \n",
    "\n",
    "for pp in range(P): \n",
    "\n",
    "    np.random.shuffle( data ) \n",
    "\n",
    "    for fold in range[k]: \n",
    "        validation_data = data[ num_validation_sample * fold : \n",
    "                              num_validation_sample * (fold+1) ] \n",
    "\n",
    "        training_data = data[ : num_validation_sample * fold ] + data[ num_validation_sample * (fold+1) ] \n",
    "\n",
    "        model = get_model() \n",
    "        model.train( training_data ) \n",
    "        validation_score = model.evaluate( validation_data ) \n",
    "        validation_scores.append( validation_score ) \n",
    "\n",
    "validation_score = np.average( validation_scores ) \n",
    "    \n",
    "model = get_model()\n",
    "model.training( data ) \n",
    "test_score = model.evaluate( test_data ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to keep in mind <a name = 'ThingsToKeep'></a>\n",
    "\n",
    "1. Always randomly shuffle your data\n",
    "2. However, if you are dealing things with certain directions, say time, DO NOT shuffle in that direction\n",
    "3. DO NOT include repeated or redundant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering, data processing  <a name = 'prepare_data' ></a>\n",
    "\n",
    "Before we build the NN model, we need to ask \"how do you prepare the input data and targets before feeding them into NN\"? Here we review some basics that are working for all data domains (there are specific tech for specfic datas, not cover here)\n",
    "\n",
    "1. Data vetorization/tensorization\n",
    "2. Data normaliation (for all different types of datas, take small value between 0,1 and have mean 0, and std 1)\n",
    "\n",
    "3. Put the missing data as 0 (provided that 0 is NOT meaningful in data and output. Not revalvent for now)\n",
    "\n",
    "4. UNDERSTAND your problem and translate it into a simple way that NN can solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting (Key section) <a name = 'Overfit' ></a>\n",
    "\n",
    "In general there are FOUR strategies \n",
    "\n",
    "1. Get more data (so that it cannot memorize too much small patterns but forced to learn most imporatnat patterns which may be generalized)\n",
    "2. Reduce the size of NN such as layer, node, epoch (so that it can NOT memorize too much same as above) \n",
    "3. Add weight regularization. It is adding COST to the loss function of the NN associated with having large weights. Thare are two kinds, L1 and L2 regulations corresponding to abs value and square of abs vlaue of weights. **Why weight regulartion  is useful?**  Because it constraint the range within which the weight (the knowledge of the NN) can take, such that the entropy of the NN is less, and the model is simpler, more regular. \n",
    "4. Add dropout. It consists of introducing **noise** in the output values of a intermediate layer which breaks up the coincident patterns that are not important, which otherwise the NN will start to remember it, if no noise is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing the network size  <a name = 'reduce_size' ></a>\n",
    "1. This is the simplest way to overcome overfitting, which is to reduce the number of paramters in the NN. A model with more parameter will be able to remember more UN-necessary details in the training set, and over-fit. \n",
    "\n",
    "2. With less parametre, in order to minimize loss, it has to \"remmeber\" more imporatnt things, which can be generalized to test set. \n",
    "\n",
    "3. With more parameters, the NN can fit the training set much quicker, but that is useless.\n",
    "\n",
    "4. You have to tune the model many times, to find the right number of parameters. \n",
    "\n",
    "5. We won't demonstrate the above here. See p 105-107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight regularization  <a name = 'Weight_regularization' ></a>\n",
    "\n",
    "1. Occam's razor: A simpler, hence smaller model, will more likely be generalized to test set, and less likely be overfitted.\n",
    "\n",
    "2. A **simple model** here means a model with the distribution of parameters has less entropy. As a result, we would like to constraint the range of the parameters so that they all can only take values within a certain small range. This is called **weight regularization** which makes the distribution of weight (the knowledge of the NN) to be more **regular**. \n",
    "\n",
    "3. This is done by adding to the NN a **loss** (don't confuse with other lossess) so that the NN will have to pay for having large weight. There are TWO kinds of regularizations\n",
    "    1. **L1 regularization** abs value of the weight coe\n",
    "    2. **L2 regularization** abs square of the weight coe\n",
    "    \n",
    "4. In Keras, you can do that with jsut one argument. There are three options, demonstrated below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization (DO NOT confuse that with number 12, it is lower case L)\n",
    "# Here it means every coe in the weight matrix of the layer will \n",
    "# add 0.001 * weight_coe_value to the total loss of the NN\n",
    "model.add( layers.Dense( 16 , kernel_regularizer = regularizers.12( 0.001 ) , \n",
    "                       activation='relu' , input_shape = (10000 , ) ) ) \n",
    "\n",
    "# L1 is simply replace l2 to l1\n",
    "model.add( layers.Dense( 16 , kernel_regularizer = regularizers.11( 0.001 ) , \n",
    "                       activation='relu' , input_shape = (10000 , ) ) ) \n",
    "\n",
    "## Or you can do L1 and L2 together\n",
    "model.add( layers.Dense( 16 , kernel_regularizer = regularizers.11_l2( l1 = 0.001 , l2 = 0.001 ) , \n",
    "                       activation='relu' , input_shape = (10000 , ) ) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add dropout  <a name = 'dropout' ></a>\n",
    "1. Dropout is applied to an intermedinate layer, which randomly **dropping out** a number of output features, during training, by setting them to zero.\n",
    "2. The **dropout rate** is the number of features that are zeroed out, which is usually bewteen 0.2 and 0.5\n",
    "3. The idea is that through introducing **noise**, it breaks up the coincident patterns in the feature map, which otherwise will be remmeber by NN if there is no noise.\n",
    "4. The above is done in the training process. For the test set, **no** unit will be drop out. There are two ways to handle the test data\n",
    "    1. We scale **DOWN** the test output by the dropout rate (times the dropout rate), as more units are active in the test time.\n",
    "    2. We didn't touch the test output, rather we scale **UP** the training output by the dropout rate( divided by the dropout rate). This is equivalent to the above.\n",
    "    \n",
    "5. In keras, this is done by intrducing the **dropout layer**, as demonstrated below. **But I am not sure how to modify the layer out as discussed above yet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential() \n",
    "model.add( layers.Dense( 16 , activation= 'relu' , input_shape = ( 10000 , ) ) ) \n",
    "\n",
    "# Add a dropout layer\n",
    "model.add( layers.Dropout( 0.5 ) ) \n",
    "model.add( layers.Dense( 16 , activation = 'relu' ) ) \n",
    "\n",
    "# Add another dropout layer\n",
    "model.add( layers.Dropout( 0.5 ) ) \n",
    "\n",
    "model.add( layers.Dense( 1 , activation = 'sigmoid' ) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow of ML (key summary) <a name = 'workflow' ></a>\n",
    "It is simply FOUR steps: \n",
    "\n",
    "1. Problem Defintion \n",
    "    1. What will the input data look like\n",
    "    2. What you want to prdcit\n",
    "    3. What type of problem? Classification, regression etc.\n",
    "    4. Cannot be non-stationary problem \n",
    "    5. You make the following hypothesis\n",
    "        1. You asssume that your outputs can be predicted with your input\n",
    "        2. You assume that you have feed in big enough datas\n",
    "\n",
    "2. Metric of success\n",
    "    1. How you want to say your NN is good?\n",
    "    2. For different problem, there are differnet metric, like \"accuracy\" for classification, \"mse\" for regression etc.\n",
    "\n",
    "3. Validation methods (fight with overfit)\n",
    "    1. You have to tune your NN, with validation step. \n",
    "    2. Hold-out (K=2 fold validaiton)\n",
    "    3. K-fold validation\n",
    "    4. Iterated K-fold validation\n",
    "\n",
    "4. Data processing\n",
    "    1. Before feed your data into NN, massage them  abit\n",
    "    2. Make them as tensor, with zero mean, std = 1\n",
    "    3. For small data, you may want to do some feature engineering, such as [Data augmentation](#Ch5_data_augmentation) in Ch 5.\n",
    "\n",
    "5. Make sure your model is better than baseline\n",
    "    1. For classificaiton problem, your model has to be better than just blindly guessing. **For regression problem, not sure what is the baseline yet**\n",
    "    2. If you cannot, check if the follwing two assumpatiosn you made\n",
    "        1. You asssume that your outputs can be predicted with your input\n",
    "        2. You assume that you have feed in big enough datas\n",
    "    3. You have to make the follwing key choices at the key positions at your NN\n",
    "        1. Last layer activation, 'relu' or 'sigmoid' or sth else\n",
    "        2. Loss function, \"binary_crossentropy\" or sth\n",
    "        3. Optimization config, \"rmsprop\" etc. \n",
    "    4. Use the followign **last-layer activation** and **loss function**\n",
    "        1. Prob. type,  last-layer act., loss func.\n",
    "        2. Class-binary, \"sigmoid\", \"binary_crossentropy\"\n",
    "        3. Class-multiclass-single-label, \"softmax\", \"categorical_crossentropy\"\n",
    "        4. Class-multiclass-multi-label, \"sigmoid\", \"binary_crossentropy\"        \n",
    "        5. Regression-arbitrary-value , \"None\" , \"mse\"\n",
    "        6. Regression-between_0_1 , \"Sigmoid\" , \"mse\" or \"binary_crossentropy\"\n",
    "\n",
    "6. Scaling your model to have overfits\n",
    "    1. You don't want to overfit, but you have to know when it starts overfit.\n",
    "    2. Thus when your model is UNDERfit, you should add layers, make your models bigger, add more epochs\n",
    "    3. Monitor the training loss, validaiton loss. \n",
    "7. Regularize your model (time-consuming)\n",
    "    1. Add drop out\n",
    "    2. Tune other crap in your model\n",
    "    3. Add L1/L2 regularization\n",
    "    4. Try different number of layers, nodes, watever\n",
    "    5. watever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Some vocabularies for classification and regression <a name = 'vocabulary' ></a>\n",
    "1. Sample/input, ONE data point goes into model\n",
    "2. Prediction/output, what comes out from model\n",
    "3. Target, the true value (part of the data goes into model)\n",
    "4. Prediction error/loss value, measure of distance of target and output\n",
    "5. Classes, obvious meaning\n",
    "6. Ground-truth/annotation. All targets collected into classes. Done by human\n",
    "6. Label, a specific instance of a class annoation in the classfication problem\n",
    "7. Binary classfication, prediction is either 0 or 1\n",
    "8. Multiclass classfication, prediction is 0,1,2,3,...\n",
    "9. multilabel classficaiton, Each input could have been assigned multiple labels, rather than one. Different input could even have variant number of labels\n",
    "10. Scalar regression. Regression problem with output being a scalar which can take a continous value\n",
    "11. Vector regression. Regression prlbem with vector output, that can take a continuos value\n",
    "12. Mini-batch/batch. A small set of samples that are processed at a time by the model, which is often times chosen randomly. It is used to compute a single gradient descent update that will be used to update the weight in the model.\n",
    "\n",
    "13. Hyperparameters, number of layers and number of nodes per layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 5. Deep learning for computer vision<a name=\"Ch5\"></a>\n",
    "This chapter is essentially about CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to CNN <a name = 'intro_cnn' ></a>\n",
    "Here we show what a basic CNN looks like. It is a stack of **Conv2D** (Could be 1D, 2D and 3D, higher D is not available in the framework, got to write your own!) and **MaxPooling2D** layers.\n",
    "\n",
    "In particular, it is 3 layers of Conv2D and 2 layers of MaxPooling2D stacked in an alternative way. \n",
    "\n",
    "The input and output of each layer is of the shape (height , width , channels), where the 1st two dimensions tend to shrink as we go deeper into the CNN.\n",
    "\n",
    "We display the architecture of the CNN, but not quite understand the meaning yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The architecture of the CNN before\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 9, 9, 64)          51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 88,512\n",
      "Trainable params: 88,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "The architecture of the CNN after\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 9, 9, 64)          51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 105,610\n",
      "Trainable params: 105,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add( layers.Conv2D( 32 , (3,3) , activation = 'relu' , input_shape = (28,28,1) ) ) \n",
    "model.add( layers.MaxPooling2D( (2,2) ) ) \n",
    "model.add( layers.Conv2D( 64 , (5,5) , activation = 'relu' ) ) \n",
    "model.add( layers.MaxPooling2D( (2,2) ) ) \n",
    "model.add( layers.Conv2D( 64 , (3,3) , activation = 'relu' ) ) \n",
    "\n",
    "print( 'The architecture of the CNN before')\n",
    "model.summary()\n",
    "\n",
    "model.add( layers.Flatten() ) \n",
    "model.add( layers.Dense( 64 , activation = 'relu' ) ) \n",
    "model.add( layers.Dense( 10 , activation = 'softmax' ) ) \n",
    "\n",
    "print( 'The architecture of the CNN after')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of MNIST again <a name = 'MNIST_again' ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 32s 538us/step - loss: 0.1806 - accuracy: 0.9426\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 32s 537us/step - loss: 0.0471 - accuracy: 0.9851\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 32s 533us/step - loss: 0.0327 - accuracy: 0.9898\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 32s 525us/step - loss: 0.0235 - accuracy: 0.9928\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 32s 534us/step - loss: 0.0203 - accuracy: 0.9937\n",
      "10000/10000 [==============================] - 2s 158us/step\n",
      "0.02663247952898091 0.9919999837875366\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the data\n",
    "(train_images , train_labels) , (test_images , test_labels) = mnist.load_data()\n",
    "\n",
    "# Vectoerize the data\n",
    "train_images = train_images.reshape( ( 60000 , 28 , 28 , 1 ) ) \n",
    "train_images = train_images.astype( 'float32' ) / 255\n",
    "test_images = test_images.reshape( ( 10000 , 28 , 28 , 1 ) ) \n",
    "test_images = test_images.astype( 'float32' ) / 255\n",
    "\n",
    "train_labels = to_categorical( train_labels ) \n",
    "test_labels = to_categorical( test_labels ) \n",
    "\n",
    "# Train the model\n",
    "\n",
    "model.compile( optimizer = 'rmsprop' , \n",
    "             loss = 'categorical_crossentropy' , \n",
    "             metrics = ['accuracy' ] ) \n",
    "model.fit( train_images , train_labels , epochs = 5 , batch_size = 64 ) \n",
    "\n",
    "# Apply to the test data\n",
    "test_loss , test_acc = model.evaluate( test_images , test_labels ) \n",
    "print( test_loss , test_acc ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The CNN operation  <a name = 'CNN_operation' ></a>\n",
    "\n",
    "1. The difference between **Dense** layer and **CNN** layer is that the Dense layer learns the all the data at once (global), while the CNN layer learns part of the data once at a time (local). \n",
    "\n",
    "2. Two characteristics of CNN. \n",
    "    1. The patterns CNN learns are translation invariant. If CNN learn sth somewhere, it can recognize it anywere. Also they need fewer training samples to learn representations so that the generalization power is greater.\n",
    "    2. CNN can learn spatial hierarchies of patterns. The first layer learn sth concrete, and as we go deeper into the CNN, the things it learn gets more and more abstract. \n",
    "    \n",
    "3. CNN operate over 3D tensors, with (height, width, channel). The CNN operation extracts patches (part of the data), and apply some transformation to produce the **output feature map**. The output is of size (height , width , filter) where the dimensions need to be the same as the input. **Filter encode certain feature of the input, which could be very abstract.** Thus the output is just a **response map of input** of size (height , width) where each point has a filter.\n",
    "\n",
    "4. CNN are defined by TWO key parameters\n",
    "    1. Size of patches extracted from the inputs. Often (3x3) or (5x5)\n",
    "    2. Number of channels of the output feature map. \n",
    "    3. It is often written as Conv2D( output_channel , (patch_height , patch_width) )\n",
    "\n",
    "5. CNN will slide these patches to all positions, followed by a tensor transformation done by **convolution kernel**, into a 1D vector of shape (output_channel,). Finally all these vectors will be ensembled to the output of size (output_height , output_width , output_channel). The output width and height could be different from the input, due to bordering effect. \n",
    "\n",
    "6. Border effect. This is easy to understand. Given a square of size (M,N), and if you want to scan through it with a patch of size (m,n), there the output map will be of size (M-m+1 , N-n+1).\n",
    "\n",
    "7. Padding. If you want the output size to be the same as the input size, then you can add some extra number of rows and columns. **But this seems really weird!! How do you create the \"ghost\" inputs?**\n",
    "\n",
    "8. In Conv2D, padding can be realized via the \"padding\" argument, which take TWO values. By default it is \"valid\", which means no padding; Or it can take \"same\" which will pad in such a way that the output and input have the same size.\n",
    "\n",
    "9. Strides. Strides can also affect hte output size. In the above, we assume that each time when we slide the patch, we slide it by ONE unit. But this need not be the case, and it is called the **strided convolutions**. This is rarely used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The max-pooling operation <a name = 'max_pooling_operation'></a>\n",
    "\n",
    "Max-pooling is to **agressivly downsample the feature maps (shrink its size), much like strided convolutions**. \n",
    "\n",
    "1. Max pooling is like convolution, except that the latter use linear transformation (convolution kernal) while the latter use hardcoded **max** tensor operation.\n",
    "\n",
    "2. Max pooling is usually done with 2x2 pathc and stride 2, which often half the size of the feature map. \n",
    "\n",
    "3. The reason we downsample the feature map is the following.\n",
    "    1. If we don't do that, there will be lots of parameter s to fit, which will result in severe over-fitting.\n",
    "    2. If we don't do that, the CNN may not be able to learn the general feature of the input. This is similar to the previous one: If you have lots of parameter, the network will still remember lots of details that are specific to the training set. Thus by downsample the feature, CNN can learn the **spatial hierachy of features**\n",
    "    \n",
    "    \n",
    "4. There are other ways to downsample the feature map, such as the naive stride convolution, average polling (by averaging the value of each channel over the patch, rather than taking just the max). But max-pooling is oftentimes better. The general philosophy is that you first want to have a dense feature map, then look at the max which is the most informative ones. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN from scratch on a small dataset <a name = 'Training_cnn'></a>\n",
    "Here we will to use **data augmentation** to handle the case where we have only very few data, without too much overfitting. Moreover, deep learning Network are very **repurposable** means that a network trained for one thing can be adapted to solve other problems. For that reason in [Pretrained public CNN], we will learn **feature extration with a pre-trained network** and **how to fine-tune a pretrained network**.  With the data augmentation tech above, we shall be tabel to handle small data better\n",
    "\n",
    "1. Data augmentataion is designed to solve the problem that we have too less data such that the overfitting could be very severe. Data augmentation takes the approach of generating more training data from the existing training sample, by **augmenting the sample via randomly transform the image to yield sort-of-like images.** The point is that your network won't see the exactly the same picture twice, but changed a bit.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data <a name = 'Ch5_get_data'></a>\n",
    "Here we load the data of cat and dog. DO NOT spend too much time on this right, I just copy paste. Note we have to download the pictures form teh website first, then run the code. You will have to go to the directory '/Users/MaoLin/Documents/LookingForJobs/Machine_Learning/Cat_dog_thing/cats_and_dogs_small' so that it can show up. To go to a directory, use Shift + Command + G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "original_dataset_dir = '/Users/MaoLin/Documents/LookingForJobs/Machine_Learning/Cat_dog_thing/kaggle_original_data'\n",
    "\n",
    "base_dir = '/Users/MaoLin/Documents/LookingForJobs/Machine_Learning/Cat_dog_thing/cats_and_dogs_small' \n",
    "os.mkdir(base_dir)\n",
    "train_dir = os.path.join(base_dir, 'train') \n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation') \n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test') \n",
    "os.mkdir(test_dir)\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats') \n",
    "os.mkdir(train_cats_dir)\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs') \n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats') \n",
    "os.mkdir(validation_cats_dir)\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs') \n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "test_cats_dir = os.path.join(test_dir, 'cats') \n",
    "os.mkdir(test_cats_dir)\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs') \n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)] \n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname) \n",
    "    dst = os.path.join(train_cats_dir, fname) \n",
    "    shutil.copyfile(src, dst)   \n",
    "    \n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)] \n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname) \n",
    "    dst = os.path.join(validation_cats_dir, fname) \n",
    "    shutil.copyfile(src, dst)\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)] \n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname) \n",
    "    dst = os.path.join(test_cats_dir, fname) \n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "    \n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)] \n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname) \n",
    "    dst = os.path.join(train_dogs_dir, fname) \n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)] \n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname) \n",
    "    dst = os.path.join(validation_dogs_dir, fname) \n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)] \n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname) \n",
    "    dst = os.path.join(test_dogs_dir, fname) \n",
    "    shutil.copyfile(src, dst)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 1000\n",
      "total training dog images: 1000\n",
      "total validation cat images: 500\n",
      "total validation dog images: 500\n",
      "total test cat images: 500\n",
      "total test dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir))) \n",
    "print('total training dog images:', len(os.listdir(train_dogs_dir))) \n",
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n",
    "print('total test cat images:', len(os.listdir(test_cats_dir)))\n",
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build CNN <a name = 'Ch5_build_cnn'></a>\n",
    "The CNN here is really the same as that used for the [MNIST problem](#intro_cnn), which has alternated Conv2D (with relu) and MaxPooling2D layers. The different thing is that because the problem at hand is quit big, we will ahve one more Conv2D + MaxPooling2D. The former is so that we have more parameters to fit the problem, the latter is to quickly downsample it so that we dont overfit. \n",
    "\n",
    "As we will see in this CNN, the number of channels (depth) will increase quickly from 32 to 128, while the dimensional size (height,width) will shrink. This is common.\n",
    "\n",
    "For the compilation step, it is reall the same as that of [2-class classification problem](#example2_validation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "\n",
    "# This is literally copy paste from previous examples\n",
    "# Here the input size (150,150) is chosen arbitraly. \n",
    "# NOT sure why there is a 3 in the input size yet.\n",
    "model = models.Sequential()\n",
    "model.add( layers.Conv2D( 32 , (3,3) , activation = 'relu' , input_shape = (150,150,3) ) ) \n",
    "model.add( layers.MaxPooling2D( (2,2) ) ) \n",
    "model.add( layers.Conv2D( 64 , (3,3) , activation = 'relu' ) ) \n",
    "model.add( layers.MaxPooling2D( (2,2) ) ) \n",
    "model.add( layers.Conv2D( 128 , (3,3) , activation = 'relu' ) ) \n",
    "model.add( layers.MaxPooling2D( (2,2) ) ) \n",
    "\n",
    "# These are the extra things, for the image prolbem at hand\n",
    "model.add( layers.Conv2D( 128 , (3,3) , activation = 'relu' ) ) \n",
    "model.add( layers.MaxPooling2D( (2,2) ) ) \n",
    "\n",
    "model.add( layers.Flatten() ) \n",
    "model.add( layers.Dense( 512 , activation = 'relu' ) ) \n",
    "model.add( layers.Dense( 1 , activation = 'sigmoid' ) ) \n",
    "\n",
    "model.summary()\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "# Here we complie the model\n",
    "\n",
    "model.compile( loss = 'binary_crossentropy' , \n",
    "             optimizer = optimizers.RMSprop(lr=1e-4) , \n",
    "             metrics = ['acc' ] )   \n",
    "\n",
    "model.save('cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing <a name = 'Ch5_data_process'></a>\n",
    "We know taht we should do sth to teh data we have now, as they are jpg in the drive. The steps are the following\n",
    "1. Read the files, in jpg\n",
    "2. Convert the jpg to RGB\n",
    "3. Conver the RGB to floating number\n",
    "4. Rescale the floating number, which is 0-255 to 0,1\n",
    "\n",
    "5. This can be done with **ImageDataGenerator** in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator( rescale = 1./255 ) \n",
    "test_datagen = ImageDataGenerator( rescale = 1./255 ) \n",
    "\n",
    "train_generator = train_datagen.flow_from_directory( train_dir , # Target dir\n",
    "                                                   target_size=(150,150) , # Resize all images\n",
    "                                                   batch_size=20 , \n",
    "                                                   class_mode = 'binary' ) # \n",
    "\n",
    "# Do the same for validation\n",
    "validation_generator = test_datagen.flow_from_directory( train_dir , # Target dir\n",
    "                                                   target_size=(150,150) , # Resize all images\n",
    "                                                   batch_size=20 , \n",
    "                                                   class_mode = 'binary' ) # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation <a name = 'Ch5_data_augmentation'></a>\n",
    "\n",
    "Data augmentation simply take the exisitng images, and randomly distort them abit, so that human will not tell the difference,  then feed it back to the CNN. Note that CNN will not see the exactly the figure twice (Note that we should never have redundancy in our data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained public CNN (TBC) <a name = 'public_cnn'></a>\n",
    "A pretrained network is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, then the spatial hierarchy of fea- tures learned by the pretrained network can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computer- vision problems, even though these new problems may involve completely different classes than those of the original task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of what CNN learn <a name = 'visualization_cnn' ></a>\n",
    "The representations learned by convnets are highly amenable to visualization, in large part because they’re **representations of visual concepts**. There are THREE approaches\n",
    "1. Visualizing intermediate convnet outputs **(intermediate activations)**\n",
    "2. Visualizing **convnets filters**\n",
    "3. Visualizing **heatmaps of class activation** in an image\n",
    "\n",
    "You probably want to go back to [Training CNN from scratch on a small dataset](#Training_cnn), and re-run the thing and save the model, and then come back here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of intermediate activations <a name = 'see_activation'></a>\n",
    "1. Visualizing intermediate activations consists of displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input (the output of a layer is often called its **activation**, the output of the activation function). \n",
    "2. Each channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2D image. \n",
    "\n",
    "\n",
    "We outline the steps below\n",
    "\n",
    "1. Grab a trained network, and an image that is NOT in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model( 'cats_and_dogs_small_2.h5' )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Keras class Model** takes batches of images as input, and outputs the activations of all convolution and pooling layers. Note that this is our **1ST** to see a model with more than one input and more than oue output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "# The first line extracts the output of the first 8 layers\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]] \n",
    "# The second line is a model takes in sth, and output sth. \n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualize the diferent activations in the different layers. Different activations in the SAME layer could encode different stuffs, like edges, colros etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "first_layer_activation = activations[0] \n",
    "print(first_layer_activation.shape)\n",
    "\n",
    "plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\n",
    "plt.matshow(first_layer_activation[0, :, :, 7], cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Notes to take\n",
    "    1. The first layer acts as a collection of various edge detectors. At that stage, the activations retain almost all of the information present in the initial picture.\n",
    "    2. As you go higher, the activations become increasingly abstract and less visually interpretable.\n",
    "    3. Higher presentations carry increasingly less information about the visual contents of the image, and increasingly more information related to the class of the image.\n",
    "    4. The sparsity of the activations increases with the depth of the layer: in the first layer, all filters are activated by the input image; but in the following layers, more and more filters are blank. This means the pattern encoded by the filter isn’t found in the input image. **understand the last part**\n",
    "    5. an important universal characteristic of the representations learned by deep neural networks: the features extracted by a layer become increas- ingly abstract with the depth of the layer. \n",
    "    6. A deep neu- ral network effectively acts as an information distillation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulazing CNN filters <a name = 'see_filters'></a>\n",
    "\n",
    "1. Another easy way to inspect the **filters learned by convnets** is to display the visual pattern that each filter is meant to respond to. \n",
    "2. This can be done with **gradient ascent** in input space: applying **gradient descent** to the value of the input image of a convnet so as to maximize the response of a specific filter, starting from a blank input image. The resulting input image will be one that the chosen filter is maximally responsive to.\n",
    "3. The process is simple: you’ll build a loss function that maximizes the value of a given filter in a given convolution layer, and then you’ll use stochastic gradient descent to adjust the values of the input image so as to maximize this activation value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "each layer in a convnet learns a collection of filters such that their inputs can be expressed as a combination of the filters. Again the first few layers encode sth simple, while the deeper layer is sth more abstrast. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing heatmaps of class activation (elephant example)<a name = 'heatmap' ></a>\n",
    "1. This tech is useful for understanding which parts of a given image led a convnet to its final classification decision.\n",
    "2. This general category of techniques is called **class activation map (CAM) visualization**, and it consists of producing heatmaps of class activation over input images. \n",
    "3. A **class activation heatmap** is a 2D grid of scores associated with a specific output class, computed for every location in any input image, indicating how important each location is with respect to the class under consideration.\n",
    "4. For instance, given an image fed into a dogs- versus-cats convnet, CAM visualization allows you to generate a heatmap for the class “cat,” indicating how cat-like different parts of the image are, and also a heatmap for the class “dog,” indicating how dog-like parts of the image are.\n",
    "5. It is in this paper https://arxiv.org/abs/ 1610.02391.\n",
    "6. it consists of taking the output feature map of a convolution layer, given an input image, and weighing every channel in that feature map by the gradient of the class with respect to the channel.\n",
    "7. **Intuitively**, one way to understand this trick is that you’re weighting a spatial map of “how intensely the input image activates different chan- nels” by “how important each channel is with regard to the class,” resulting in a spatial map of “how intensely the input image activates the class.”\n",
    "8. This visualization technique answers two important questions:\n",
    "    1. Why did the network think this image contained certain object?\n",
    "    2. Where is the object located in the picture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary <a name = 'ch5_summary' ></a>\n",
    "\n",
    "1. CNN is good for image-classification\n",
    "2. CNN learns the hierachy of the patterns \n",
    "3. CNN is not like a black box, and we can open the black box \n",
    "4. You know how to do data augmentation to overcome overfitting \n",
    "5. You know how to use pre-trained CNN to do feature extration and fine-tuning. \n",
    "6. In order to view what the CNN learn, you can genreate visualization of the filters learn by your CNN as well as heatmaps of class activity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentative: Use CNN for multiclass classifcation<a name = 'Tentative:multiclass'></a>\n",
    "Let's do the multiclass classification in [Example of more-class classificaiton](#example_classification2) with CNN. Note that previous we got 78% acc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data and vectorize them <a name = 'Tentative_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "8982\n",
      "2246\n",
      "2246\n",
      "(8982, 46)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "( train_data , train_labels) , ( test_data , test_labels) = reuters.load_data( num_words = 10000 ) \n",
    "# Again, we care only the 10000 most frequent occuring words\n",
    "\n",
    "print( len( train_data ) ) \n",
    "print( len( train_labels ) ) \n",
    "print( len( test_data ) ) \n",
    "print( len( test_labels ) ) \n",
    "\n",
    "# for i , label in enumerate( train_labels ):\n",
    "#     print( i ) \n",
    "#     print( label ) \n",
    "    \n",
    "import numpy as np\n",
    "def vectorize_sequences( sequences , dimension = 10000 ):\n",
    "    results = np.zeros( ( len( sequences) , dimension ) )\n",
    "    for i , sequences in enumerate( sequences ):\n",
    "        results[ i , sequences ] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences( train_data ) \n",
    "x_test = vectorize_sequences( test_data )\n",
    "\n",
    "def vectorize_labels( labels , dimension = 46 ):\n",
    "    results = np.zeros( ( len( labels ) , dimension ) ) \n",
    "    for i , label in enumerate( labels ):\n",
    "        results[ i , label ] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = vectorize_labels( train_labels ) \n",
    "one_hot_test_labels = vectorize_labels( test_labels ) \n",
    "\n",
    "print( one_hot_train_labels.shape ) \n",
    "\n",
    "# # Just use the Keras built in things\n",
    "# one_hot_train_labels = to_categorical( train_labels ) \n",
    "# one_hot_test_labels = to_categorical( test_labels )     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building NN <a name = 'Tentative_build_nn'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The architecture of the CNN before\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 98, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 49, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 45, 45, 128)       204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 22, 128)       0         \n",
      "=================================================================\n",
      "Total params: 206,720\n",
      "Trainable params: 206,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "The architecture of the CNN after\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 98, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 49, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 45, 45, 128)       204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                3964992   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 4,174,702\n",
      "Trainable params: 4,174,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add( layers.Conv2D( 64 , (3,3) , activation = 'relu' , input_shape = (100,100,3) ) ) \n",
    "model.add( layers.MaxPooling2D( (2,2) ) ) \n",
    "model.add( layers.Conv2D( 128 , (5,5) , activation = 'relu' ) ) \n",
    "model.add( layers.MaxPooling2D( (2,2) ) ) \n",
    "\n",
    "# model.add( layers.Conv2D( 64 , (3,3) , activation = 'relu' ) ) \n",
    "\n",
    "print( 'The architecture of the CNN before')\n",
    "model.summary()\n",
    "\n",
    "model.add( layers.Flatten() ) \n",
    "model.add( layers.Dense( 64 , activation = 'relu' ) ) \n",
    "model.add( layers.Dense( 46 , activation = 'softmax' ) ) \n",
    "\n",
    "print( 'The architecture of the CNN after')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building NN <a name = 'example2_classification2_build_NN'></a>\n",
    "One difference with N-class and 2-class classifciaiton is that, since we have a stack of Dense layers, each layer can only accss to the output of the pervious layers, we don't want the **size** of each layer to be smaller than N, the number of class. This is intuitive: If we start from certain layer with output number less than N, it is asking the rest of the network to give N-class with number of data points less than N, which is impossible. Thus we need a larger NN for larger class.\n",
    "\n",
    "A few things to note for the following network\n",
    "1. The last layer has 46 output (units), each for one classificaiton\n",
    "2. **softmax** gives the probabilty distribution over the 46 classes: For a single input sample, the CNN gives the probability of it being in each class. The sum of the prob is 1.\n",
    "3. The loss function is **categorical_crossentropy**, which measures the distance betweetn the true prob distribution and the output prob distribution\n",
    "4. There is another way to pre-process the labels, as discussed in [Different way to handle labels and loss](#diff_way_handle_label_loss), where we encode the labels as integers, rather than using the one-hot enconding. That case, we shall use the **sparse_categorical_crossentropy** as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add( layers.Dense( 64 , activation = 'relu' , input_shape = (10000,) ) ) \n",
    "model.add( layers.Dense( 64 , activation = 'relu' ) ) \n",
    "model.add( layers.Dense( 46 , activation = 'softmax' ) ) \n",
    "\n",
    "model.compile( optimizer= 'rmsprop' , \n",
    "             loss = 'categorical_crossentropy' , \n",
    "             metrics = ['accuracy' ] ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation: Separate the datas <a name = 'example2_classification2_validation1'></a>\n",
    "Take out 1000 as the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 10000)\n",
      "(8982, 46)\n",
      "(7982, 10000)\n",
      "(7982, 46)\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 243us/step - loss: 2.6632 - accuracy: 0.5475 - val_loss: 1.7645 - val_accuracy: 0.6600\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 131us/step - loss: 1.4244 - accuracy: 0.7111 - val_loss: 1.3154 - val_accuracy: 0.7260\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 133us/step - loss: 1.0516 - accuracy: 0.7751 - val_loss: 1.1457 - val_accuracy: 0.7570\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 136us/step - loss: 0.8242 - accuracy: 0.8266 - val_loss: 1.0315 - val_accuracy: 0.7810\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 137us/step - loss: 0.6576 - accuracy: 0.8641 - val_loss: 0.9577 - val_accuracy: 0.8030\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 118us/step - loss: 0.5268 - accuracy: 0.8929 - val_loss: 0.9282 - val_accuracy: 0.8030\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 110us/step - loss: 0.4311 - accuracy: 0.9109 - val_loss: 0.8966 - val_accuracy: 0.8140\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 108us/step - loss: 0.3494 - accuracy: 0.9262 - val_loss: 0.8960 - val_accuracy: 0.8220\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 115us/step - loss: 0.2944 - accuracy: 0.9375 - val_loss: 0.9169 - val_accuracy: 0.8100\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 120us/step - loss: 0.2460 - accuracy: 0.9440 - val_loss: 0.8982 - val_accuracy: 0.8170\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 123us/step - loss: 0.2119 - accuracy: 0.9479 - val_loss: 0.8839 - val_accuracy: 0.8250\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 108us/step - loss: 0.1857 - accuracy: 0.9515 - val_loss: 0.9600 - val_accuracy: 0.8120\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 106us/step - loss: 0.1700 - accuracy: 0.9536 - val_loss: 0.9444 - val_accuracy: 0.8130\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 121us/step - loss: 0.1541 - accuracy: 0.9548 - val_loss: 0.9299 - val_accuracy: 0.8220\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 200us/step - loss: 0.1439 - accuracy: 0.9567 - val_loss: 0.9678 - val_accuracy: 0.8170\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 312us/step - loss: 0.1324 - accuracy: 0.9564 - val_loss: 1.0258 - val_accuracy: 0.8050\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 199us/step - loss: 0.1254 - accuracy: 0.9585 - val_loss: 1.0056 - val_accuracy: 0.8160\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 186us/step - loss: 0.1223 - accuracy: 0.9593 - val_loss: 1.0395 - val_accuracy: 0.8160\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 108us/step - loss: 0.1203 - accuracy: 0.9575 - val_loss: 1.0275 - val_accuracy: 0.8150\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 112us/step - loss: 0.1145 - accuracy: 0.9584 - val_loss: 1.0219 - val_accuracy: 0.8120\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[ : 1000 ]\n",
    "partial_x_train = x_train[ 1000 : ]\n",
    "y_val = one_hot_train_labels[ : 1000 ]\n",
    "partial_y_train = one_hot_train_labels[ 1000 : ]\n",
    "\n",
    "print( x_train.shape ) \n",
    "print( one_hot_train_labels.shape ) \n",
    "\n",
    "print( partial_x_train.shape ) \n",
    "print( partial_y_train.shape ) \n",
    "\n",
    "history = model.fit( partial_x_train , \n",
    "                   partial_y_train ,\n",
    "                    epochs = 20 , \n",
    "                    batch_size = 512 , \n",
    "                    validation_data = ( x_val , y_val ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation: Plot the losses <a name = 'example2_classification2_validation2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [65.3311567541055, 39.776090942392365, 34.32261133918742, 27.680670603362863, 23.473644146365363, 22.52619806592619, 22.79192004368253, 22.770185779291435, 17.891637504778544, 17.22231869969295, 19.80953382602939, 17.605009325823776, 16.321068039287567, 20.478961443159694, 15.61127507652789, 16.718608436294563, 14.699746130739483, 15.916404533125082, 16.031627229373385, 14.611600887082844, 18.139140705647186, 14.015900961077302, 14.239361140213095, 12.561681124091258, 13.30613582983318, 12.55906216896803, 12.260872022373288, 12.29156907676563, 11.712689146895704, 12.450729773651883, 12.56630845620305, 13.483685229759718, 13.15491041243837, 11.78229565013464, 12.88459904933127, 13.023799285264962, 12.108842663661724, 15.469679353627438, 12.555042235641787, 11.742788167993105, 11.880856605593747, 12.777319117148965, 12.349031028275535, 11.934441249798015, 13.97433355483653, 11.428836756669408, 11.156954098963132, 11.412460465727767, 11.697844318199515, 11.250578743562397, 12.011186958216905, 12.337325209267622, 12.178285356486853, 11.327628672268187, 11.572528048414428, 11.095232769357194, 11.547587482355498, 10.89704412067687, 11.776617650933643, 11.509231031617169, 11.466086535947541, 11.513909517327498, 10.878487468141564, 11.353505508656214, 11.826184791835523, 11.274384294338594, 11.077379363542308, 11.407087584223545, 11.823160643403483, 11.348023253455551, 11.535188405061922, 10.740262542298584, 11.037835361880347, 14.468348459376976, 12.320054728812122, 11.065318511526957, 10.196312874574318, 12.710773596032938, 11.184319578576185, 14.380173534172181, 10.056968553236862, 11.607888777131713, 10.254099730050823, 10.31047039789535, 11.046551178993269, 9.971497246822674, 11.647552685435206, 10.820723609676952, 12.975559028220285, 10.762306096291603, 11.109022101723085, 11.377209244102602, 11.485624256451604, 10.960727207325768, 10.668497398350443, 10.295484725284473, 10.907976301165071, 11.585771309493667, 11.870662813715747, 10.849608952646365, 10.990505034799636, 12.256060263684876, 10.94029323541168, 12.084088799294712, 10.74751931502274, 12.117025012888089, 10.008791942240459, 9.79032624602715, 10.873732757974775, 10.368831819473481, 11.182109516187765, 10.525392988269358, 11.768114437497829, 9.791652520158383, 10.366831844489905, 10.82123230183255, 12.160218325327294, 10.89712327180919, 11.043144851552743, 10.081087021443862, 10.535608439033131, 10.981081374146338, 11.402706202919656, 9.641788224146776, 10.21196618419438, 9.848204830273557, 10.717139214437482, 10.058930607633618, 11.48641505815717, 11.387720414948543, 11.184878863368903, 10.15860658798927, 12.634969771101996, 10.743748031331844, 10.069616052005982, 10.55061410463618, 12.081727217527286, 10.640027858140547, 12.421999816619099, 10.501246002437039, 10.781571683541602, 10.865363658600922, 11.282211145569223, 10.651188596404626, 10.474688320334236, 11.273722748484204, 11.259925015301976, 10.630266794053108, 12.447139341542082, 9.531828820530503, 9.956735663198968, 9.748018149767875, 10.409433149802672, 10.152983615106885, 9.59336286511346, 10.036261642000195, 10.745350843100661, 10.289103100297622, 10.692397897947801, 10.49608223188189, 10.085275915301507, 11.01686289372144, 11.551699779498954, 11.743552048921142, 10.236048154720212, 10.003178234232127, 12.719112821227622, 10.945822092821034, 10.675044961083811, 10.549677406397354, 10.49193488142573, 10.728818675470862, 10.940691456562059, 10.84922628008961, 11.01185695745722, 10.051327843984387, 10.640393516860238, 11.411730008719646, 11.496731827229187, 11.453003342329902, 10.852511891076322, 12.516667835979197, 11.492266653801803, 10.234439696689565, 12.459450470223691, 12.359721258965017, 11.081772637189284, 10.962543924886036, 12.61372473296631, 9.850524183592707, 10.286291907384697, 11.774103224088465, 9.844982801992469, 10.519427263546772, 10.927765613629846, 9.613802498727484, 12.55826472032799, 11.780731899813647, 11.058068283976777, 11.240432644936044, 10.490413442413129, 12.564543059319925, 10.48954338531268, 12.654398422164952, 10.994893745696725, 12.080461966491645, 13.261745644357553, 11.6413525543775, 9.836769459838035, 12.075115144137515, 12.862921366995579, 10.113928052677432, 11.893327762829376, 11.689800425947846, 10.449994934995953, 11.015308582012445, 11.687384869371677, 11.755015448881498, 12.302777554598373, 11.231973049502894, 10.992727318675042, 10.543350873671766, 11.46447047514855, 11.023617577342556, 10.955677665488812, 11.006285483068279, 12.847765292896398, 11.719314388968257, 12.531713320081678, 10.86539141863981, 11.256658129028095, 11.873176343726353, 13.280287280346792, 11.88398772697308, 11.72025412732348, 11.150501354622866, 11.54384481018446, 11.509621295098233, 10.863472428163606, 11.504212209246893, 10.389840810269552, 12.343409710462288, 12.921313927478922, 12.037928154618285, 12.186794953734273, 11.90736489839815, 11.791193818500634, 11.977544576941073, 10.965935815415914, 11.357686833466984, 12.575537758883973, 11.417199296182906, 11.05332478821349, 10.768702672204316, 11.508758675254015, 10.500545356247464, 12.971465700649482, 12.578036080928298, 10.409328385003805, 10.838445667762974, 11.385151716939573, 11.10936545092733, 11.89665979744897, 11.768126593036076, 11.764489002391525, 10.433418090586633, 11.835506468261379, 11.49151284216516, 11.561622943853646, 11.741023956153699, 11.148711052986316, 11.063064687314794, 11.604472803910053, 12.4958389158233, 10.89721076955355, 11.89152140730282, 11.522090089796855, 13.090380321117493, 12.13504893457895, 12.422137335919668, 11.571214227239391, 11.52560947469083, 14.158431707134488, 11.3299609481168, 11.540383247499548, 12.96275170767417, 12.63215307663725, 12.358268080685532, 11.616475433708395, 12.04842548565893, 12.363837242284951, 12.332381280253776, 11.617373671097102, 10.971368267196278, 12.040184809263293, 12.687486459948307, 11.865505270567713, 11.823908487541743, 11.440298954249096, 10.490947587704886, 11.624458745580702, 12.84551649897606, 11.108117978530014, 11.527772955196486, 10.187797875922907, 13.992230955517131, 11.89330376746677, 11.415479546260679, 11.95081842869079, 11.373123203775538, 11.2934203155046, 11.226333294944702, 11.532691394445743, 11.34567767996537, 12.130202175972414, 11.65659369673928, 12.17052636920108, 11.940386037913953, 11.992950105878275, 12.07894192289764, 11.87908706573818, 10.85732156628804, 12.329237632592909, 11.110863232362721, 12.303808273196609, 11.864865515020325, 11.961818305810679, 11.656400748146769, 11.911147564165478, 12.25361731643489, 12.21247069386415, 11.936371548913044, 11.863568916999837, 11.212625553641022, 10.997925142144396, 12.182984163065436, 11.338976000314625, 12.234816147454946, 10.946965022215347, 11.607283619014629, 11.697233101660224, 10.742593120599723, 12.455691906012362, 11.797499330852661, 11.526683771359794, 11.290294799704732, 11.406707287364926, 13.736556675939793, 11.406114047070467, 11.075187637402694, 11.522262124549874, 11.588487254164107, 11.861103766938985, 11.133505986704915, 12.432518915619037, 11.478453169103094, 11.755333923777949, 11.319070897584217, 11.173291440610543, 12.370201978841704, 12.322139398404051, 11.175845650256715, 11.87313064726186, 11.965410798121852, 11.94271442607837, 11.058701252446067, 11.028619943220535, 12.129428748638741, 11.889728588846578, 12.809146471039766, 11.831866145993832, 13.695942451423875, 11.304745525228883, 11.879099989357309, 12.915280366869698, 11.914896120616099, 12.843217641887485, 12.014743528223947, 11.869111323004011, 12.129566811310706, 12.754096997677548, 13.717300574560326, 12.31862914376537, 12.61541999500858, 12.366739948217914, 11.537812413036734, 11.91091265110299, 12.18103127077563, 16.655474447662826, 10.947803195290245, 14.559492101048566, 12.015418878315007, 14.20721447985564, 11.131116361024162, 11.82692696709889, 11.405576453305606, 13.798701377522136, 12.043030425781698, 12.689781064275966, 10.866371028906912, 11.415241893949126, 11.692653138614963, 13.255928202901073, 12.244842967079647, 13.817661345980879, 13.48659210519215, 11.521258065025183, 11.844822196621854, 13.345741135234215, 11.718478283430663, 11.17588955701541, 11.15008881830698, 10.839376662225039, 12.58156837507308, 11.250312844365242, 11.559941194383, 13.338718300427098, 12.785005292938768, 12.09435125358609, 11.10301182430876, 11.440058018422068, 11.167148101421907, 13.121674063475025, 12.21453776528686, 10.95295434707196, 11.04353059378584, 12.075505577276765, 11.774276006070085, 11.333151501348983, 11.547879993160011, 11.206109856336612, 10.8058997791861, 11.80116850309452, 11.798564801732594, 11.318831831231796, 11.363603020977866, 10.853078589074448, 10.46462519410917, 11.277320146881243, 10.760413231509093, 11.308609320941187, 10.334381166583913, 12.075841505337497, 10.572865080535596, 11.673463521110317, 11.811445593423588, 12.001574647031946, 11.452848711559643, 11.47923523285841, 11.786545140710244, 11.332718823307959, 12.34043554514462, 11.196656414943904, 11.885120759256061, 10.689283006298956, 12.35718212618235, 12.631220618287236, 11.54326956426681, 11.75820554164602, 10.912363280577926, 11.704249008111471, 11.015055351597605, 11.10872843005178, 11.604611335177463, 12.434844627188596, 11.301924379577049, 10.933735837085239, 11.624334305490247, 12.39819823963986, 10.942490057076908, 11.849445703127053, 11.634798237154868, 10.935304897008026, 11.296661566174128, 10.803479604529123, 11.8579395358283, 11.267758376335053, 11.66178952625925, 13.072226912480557, 11.75955469174873, 11.634338584612447, 11.80103961395222, 12.398508997724823, 12.741647663057261, 11.297989030414731, 11.742471542936983, 11.888876423519797, 12.504967256887774, 12.71599434467844, 11.429731481056091, 11.843527792551841, 11.444512407106952, 12.402724263242717, 11.837286215708914, 11.84874085486938, 11.143946190477076, 11.124408122670916, 12.954272402284465, 11.18837321261789, 10.93133025730838], 'val_mae': [5.523077487945557, 4.1374831199646, 3.555220127105713, 3.2080938816070557, 2.9883244037628174, 2.987407684326172, 3.2139437198638916, 2.9211301803588867, 2.736867666244507, 2.608494520187378, 2.864055633544922, 2.722028970718384, 2.580324172973633, 2.9588067531585693, 2.570664882659912, 2.641514539718628, 2.542715549468994, 2.6418521404266357, 2.6021974086761475, 2.496811866760254, 3.110607147216797, 2.4963808059692383, 2.5381052494049072, 2.427574634552002, 2.5158743858337402, 2.459254741668701, 2.4194233417510986, 2.4227747917175293, 2.378005266189575, 2.4702744483947754, 2.516080379486084, 2.6461331844329834, 2.578428030014038, 2.4374170303344727, 2.5760295391082764, 2.596708297729492, 2.4748339653015137, 2.8639793395996094, 2.4969303607940674, 2.3966662883758545, 2.411832571029663, 2.4831221103668213, 2.4761972427368164, 2.391998291015625, 2.7840497493743896, 2.3455028533935547, 2.2873659133911133, 2.302049160003662, 2.3331003189086914, 2.3261852264404297, 2.4080255031585693, 2.419857978820801, 2.4366817474365234, 2.3224833011627197, 2.350356101989746, 2.2844669818878174, 2.4053962230682373, 2.2705771923065186, 2.4252281188964844, 2.3367292881011963, 2.3177075386047363, 2.3504443168640137, 2.3612654209136963, 2.32837176322937, 2.4001176357269287, 2.3668479919433594, 2.293905735015869, 2.338059425354004, 2.3833608627319336, 2.330684185028076, 2.3707141876220703, 2.2564027309417725, 2.235870599746704, 2.655576229095459, 2.503192663192749, 2.280715227127075, 2.2059121131896973, 2.675981044769287, 2.326974630355835, 2.810013771057129, 2.173780679702759, 2.396002769470215, 2.1697185039520264, 2.238389015197754, 2.2858574390411377, 2.192901611328125, 2.447803497314453, 2.265162944793701, 2.506925106048584, 2.2318010330200195, 2.3453569412231445, 2.4621164798736572, 2.415386915206909, 2.2443747520446777, 2.30350399017334, 2.2408604621887207, 2.196425199508667, 2.3575451374053955, 2.386664390563965, 2.305676221847534, 2.316141366958618, 2.436589002609253, 2.37937068939209, 2.5294294357299805, 2.2737228870391846, 2.519465923309326, 2.247131586074829, 2.090589761734009, 2.2835536003112793, 2.264127016067505, 2.4492807388305664, 2.226227045059204, 2.5177242755889893, 2.1579184532165527, 2.240522623062134, 2.3336989879608154, 2.477189064025879, 2.3083810806274414, 2.3554255962371826, 2.3048410415649414, 2.33007550239563, 2.371023654937744, 2.380781650543213, 2.1459245681762695, 2.2624030113220215, 2.193742275238037, 2.3189165592193604, 2.272606611251831, 2.3616387844085693, 2.3604464530944824, 2.433168888092041, 2.1701364517211914, 2.624925136566162, 2.34464430809021, 2.1845812797546387, 2.2481110095977783, 2.4365243911743164, 2.402569055557251, 2.4504599571228027, 2.2142038345336914, 2.2690420150756836, 2.335712194442749, 2.3614916801452637, 2.3313283920288086, 2.3412086963653564, 2.3837532997131348, 2.4149320125579834, 2.3454179763793945, 2.582510232925415, 2.1494438648223877, 2.2059848308563232, 2.171373128890991, 2.239072322845459, 2.2488887310028076, 2.153799295425415, 2.256035327911377, 2.3341546058654785, 2.2760775089263916, 2.310497283935547, 2.3071978092193604, 2.2715227603912354, 2.3816916942596436, 2.5093629360198975, 2.5036303997039795, 2.279751777648926, 2.2950072288513184, 2.576014518737793, 2.346137285232544, 2.3565285205841064, 2.3351476192474365, 2.253253698348999, 2.3283960819244385, 2.3550729751586914, 2.3292665481567383, 2.4324517250061035, 2.1669228076934814, 2.2673566341400146, 2.4047255516052246, 2.3867974281311035, 2.445456027984619, 2.375671148300171, 2.544745445251465, 2.4015488624572754, 2.222095489501953, 2.587569236755371, 2.5706753730773926, 2.345179319381714, 2.297121047973633, 2.4304850101470947, 2.2087411880493164, 2.3251142501831055, 2.5032200813293457, 2.231449604034424, 2.2498934268951416, 2.3164234161376953, 2.1689419746398926, 2.6011176109313965, 2.4203426837921143, 2.3623406887054443, 2.429260015487671, 2.307421922683716, 2.6080470085144043, 2.3294081687927246, 2.485574960708618, 2.3874857425689697, 2.4734206199645996, 2.6813108921051025, 2.452509880065918, 2.26904559135437, 2.4135499000549316, 2.588629961013794, 2.2290985584259033, 2.4883718490600586, 2.3840701580047607, 2.227126359939575, 2.325735330581665, 2.502748489379883, 2.406759262084961, 2.5321316719055176, 2.335273027420044, 2.271791458129883, 2.2720212936401367, 2.398653745651245, 2.3636696338653564, 2.2979156970977783, 2.363558530807495, 2.6184563636779785, 2.4847543239593506, 2.5103840827941895, 2.2943241596221924, 2.3490374088287354, 2.4313085079193115, 2.6174123287200928, 2.4906065464019775, 2.4211931228637695, 2.3323028087615967, 2.4856441020965576, 2.400946855545044, 2.3441171646118164, 2.347707748413086, 2.20176362991333, 2.5374529361724854, 2.599090576171875, 2.546797752380371, 2.548337936401367, 2.606703042984009, 2.431061267852783, 2.5004336833953857, 2.366760492324829, 2.3557796478271484, 2.696385145187378, 2.344386577606201, 2.308049201965332, 2.374965190887451, 2.506213903427124, 2.314282178878784, 2.654829978942871, 2.560715436935425, 2.2545738220214844, 2.383561611175537, 2.476085662841797, 2.3186659812927246, 2.4063644409179688, 2.4506115913391113, 2.555797815322876, 2.3134570121765137, 2.4775445461273193, 2.4126384258270264, 2.4106178283691406, 2.4175844192504883, 2.3115079402923584, 2.3249452114105225, 2.438260316848755, 2.498734474182129, 2.313870906829834, 2.3841781616210938, 2.3715999126434326, 2.588519334793091, 2.51088285446167, 2.542556047439575, 2.3636815547943115, 2.3639461994171143, 2.7419493198394775, 2.357292890548706, 2.4056520462036133, 2.5335769653320312, 2.473590612411499, 2.498396396636963, 2.3808188438415527, 2.4763436317443848, 2.4500741958618164, 2.5742268562316895, 2.4088754653930664, 2.271042823791504, 2.4386515617370605, 2.43957781791687, 2.451793909072876, 2.5139100551605225, 2.4364724159240723, 2.2749087810516357, 2.405299663543701, 2.5826938152313232, 2.4080963134765625, 2.3990674018859863, 2.2654614448547363, 2.7069010734558105, 2.434535503387451, 2.3547537326812744, 2.4281768798828125, 2.4505465030670166, 2.3388705253601074, 2.2751712799072266, 2.4122138023376465, 2.33699893951416, 2.529068946838379, 2.4669008255004883, 2.4338412284851074, 2.4715428352355957, 2.5013885498046875, 2.5848543643951416, 2.4306559562683105, 2.343393564224243, 2.440964460372925, 2.4622650146484375, 2.478954315185547, 2.545316457748413, 2.341581106185913, 2.4648711681365967, 2.5121450424194336, 2.584683895111084, 2.4943997859954834, 2.4931371212005615, 2.406203031539917, 2.3065743446350098, 2.356827735900879, 2.5116348266601562, 2.3882696628570557, 2.4610064029693604, 2.347660541534424, 2.441549062728882, 2.3444361686706543, 2.37929368019104, 2.5412421226501465, 2.438138723373413, 2.3973886966705322, 2.3943915367126465, 2.3750133514404297, 2.6134939193725586, 2.394477367401123, 2.429180383682251, 2.4607486724853516, 2.515946865081787, 2.510448455810547, 2.459289312362671, 2.5460219383239746, 2.416591167449951, 2.4618568420410156, 2.2739593982696533, 2.4231796264648438, 2.5324478149414062, 2.5771050453186035, 2.3931148052215576, 2.445410966873169, 2.513209342956543, 2.528188705444336, 2.387864589691162, 2.3425872325897217, 2.5411205291748047, 2.4453513622283936, 2.50427508354187, 2.5086803436279297, 2.6735877990722656, 2.3932690620422363, 2.4815280437469482, 2.564267873764038, 2.449699878692627, 2.5535972118377686, 2.4840240478515625, 2.4727745056152344, 2.4457879066467285, 2.5328783988952637, 2.5622658729553223, 2.543921947479248, 2.527557611465454, 2.5134007930755615, 2.353787899017334, 2.3977715969085693, 2.5106348991394043, 3.062303066253662, 2.3015856742858887, 2.78549861907959, 2.481151580810547, 2.716665744781494, 2.351750373840332, 2.441180467605591, 2.3681273460388184, 2.728083848953247, 2.4639713764190674, 2.612853765487671, 2.282335042953491, 2.42419695854187, 2.432589530944824, 2.504739999771118, 2.529200553894043, 2.6917362213134766, 2.602813959121704, 2.4638164043426514, 2.4058399200439453, 2.6623034477233887, 2.482865333557129, 2.459773540496826, 2.3457536697387695, 2.2868573665618896, 2.56193470954895, 2.3868329524993896, 2.4170076847076416, 2.6671199798583984, 2.5476479530334473, 2.5649349689483643, 2.3688604831695557, 2.3861403465270996, 2.299562931060791, 2.666123390197754, 2.4925696849823, 2.32822585105896, 2.3348300457000732, 2.507760524749756, 2.368683099746704, 2.3244400024414062, 2.464265823364258, 2.3303632736206055, 2.324303388595581, 2.466625213623047, 2.3932859897613525, 2.3733160495758057, 2.406420946121216, 2.416971445083618, 2.269782543182373, 2.3460915088653564, 2.34865403175354, 2.3427679538726807, 2.231901168823242, 2.5099027156829834, 2.2855381965637207, 2.4174318313598633, 2.431811571121216, 2.4601657390594482, 2.3657963275909424, 2.3942649364471436, 2.4748520851135254, 2.358689785003662, 2.5171523094177246, 2.390988826751709, 2.400848627090454, 2.3127801418304443, 2.4911270141601562, 2.5491886138916016, 2.376286029815674, 2.4215736389160156, 2.3360095024108887, 2.389388084411621, 2.317291259765625, 2.380664825439453, 2.489184617996216, 2.5359127521514893, 2.429445505142212, 2.3439958095550537, 2.3877079486846924, 2.549248695373535, 2.2952120304107666, 2.4625306129455566, 2.455000400543213, 2.3197736740112305, 2.386593818664551, 2.2960426807403564, 2.4587197303771973, 2.3197076320648193, 2.4325666427612305, 2.589977502822876, 2.4203691482543945, 2.4263741970062256, 2.4165244102478027, 2.4822075366973877, 2.579841136932373, 2.3088393211364746, 2.367992639541626, 2.422844886779785, 2.505941152572632, 2.544504404067993, 2.3513128757476807, 2.40826678276062, 2.4734485149383545, 2.4675605297088623, 2.395188808441162, 2.422553539276123, 2.4507877826690674, 2.314685583114624, 2.5832207202911377, 2.3942296504974365, 2.35732102394104], 'loss': [188.03333951234436, 30.064779857826824, 20.955798191548816, 16.77282237367077, 14.933833902438217, 13.84867175904344, 12.388920079150507, 12.251390593745356, 11.861021461292154, 11.384864727076783, 10.87125933396781, 10.628634181456814, 10.138097334806355, 10.039847899875342, 9.937068596534914, 9.843866697981879, 9.339934528212865, 8.746324532454766, 9.591411335149045, 8.76545878775714, 8.456843689738854, 8.737585588848978, 8.374706982200722, 8.198521702316885, 8.358670161621061, 8.364889232167938, 8.219009144725266, 7.7802399837869345, 8.017581293400308, 7.312685551849187, 7.553974370584275, 7.1400129030116295, 7.255521814995308, 7.389221387001052, 7.177244417524238, 7.121125978831488, 6.9402817933780545, 6.9044447207206545, 6.765976077396724, 6.73816023618709, 6.6725407271293715, 6.911719116911224, 6.692445821225462, 6.671863186999622, 6.204962552752375, 6.5164012435483105, 5.943586646675991, 6.580116071238351, 6.076523372236552, 5.9061997936021084, 6.27805778401683, 6.057076343680183, 5.535680158645378, 5.780621977642842, 5.294485127695309, 6.190808303103623, 5.643820000833062, 5.7036519722548915, 5.7666976935234135, 5.454811525292712, 5.511351914674108, 5.662832850929233, 5.65089823832218, 5.399975415934539, 5.592436530291332, 5.515194412904093, 4.907598752222082, 5.153685953422375, 4.801836225165364, 5.006775772455014, 4.917430737916514, 5.208869199584291, 4.993940673019277, 4.856197454229512, 5.164664722560677, 4.701414974271228, 5.2457551446435655, 4.772259776613648, 4.348310911109407, 4.736207202108801, 4.84948745196657, 4.472116272882264, 4.8842588285738335, 4.234109849602379, 4.302423320418709, 4.556662613880143, 4.489529948996596, 4.415258641735407, 4.079955938004349, 4.402689895224642, 4.441570738586426, 4.6009625709940805, 4.3508420726052055, 4.005961939804266, 4.218796681806372, 4.675924279616594, 4.3300577177136885, 4.196086826666, 4.144786032546841, 3.9088041394858286, 3.9682688438799185, 3.9255714900441054, 3.961484088459653, 4.023381084952715, 4.130193386572705, 3.878979504542321, 4.163383605965714, 4.097416779282044, 3.6633538377287618, 3.8534687844309006, 3.9970185739635276, 3.7658514921456803, 3.894493340668008, 3.657578354316146, 3.852261782448676, 3.77813371512313, 3.5787927463498628, 3.672064742165567, 3.74328949873947, 3.9393730900393877, 3.6719934407021966, 3.6132924715614116, 3.2808540684200125, 3.503525455734441, 3.3857261967529273, 3.67929872890184, 3.648026764567884, 3.5557211095364627, 3.384660979641069, 3.3581930484970712, 3.568752842555717, 3.494497295105488, 3.41708458056787, 3.5601140193706695, 3.2716975681804294, 3.6935381968951635, 3.056674324042616, 3.390742842322652, 3.264025516809666, 3.2112064731888252, 3.0727648412688326, 3.3084536735133154, 3.032015888137004, 3.2958507898337883, 3.232251277024231, 2.986057617135074, 3.1528859573792745, 3.153465094707217, 3.0627514335466532, 3.1587208415510415, 3.1009169816420927, 3.227325618666937, 3.1579588382432986, 3.014408785077033, 3.057157237588579, 2.8511344542482275, 2.94456240873016, 2.738532367511028, 2.940541340956716, 3.036667627336161, 2.84933820507826, 2.6412367493705022, 2.9490295472547925, 2.854043831478824, 2.8256707631802476, 2.733756539450612, 2.961846878332856, 2.6751169206253937, 2.6532443102058294, 2.796266967552968, 2.845611269168009, 2.745646912249081, 2.714918989512767, 2.8327800772847684, 2.8648010753542663, 2.352979525868403, 2.841878870325681, 2.75133902078116, 2.4515225943203034, 2.571182293049187, 2.5570190275047393, 2.60604254861962, 2.5475434720460592, 2.6705208246827836, 2.5202941221936204, 2.529845545555128, 2.559705068890379, 2.4096183088287777, 2.4676328737066786, 2.448355999231575, 2.5957637313281774, 2.4063844872598383, 2.4185853944277804, 2.3937533397450776, 2.447015794418284, 2.372766870915248, 2.4189937473327596, 2.4913243964871468, 2.358449007739728, 2.1322958255257274, 2.3692751889239867, 2.2259187168014662, 2.2426628154598083, 2.2354390112043205, 2.4748248742011603, 2.3820958941899337, 2.091909623233306, 2.23108965840365, 2.4214782687054033, 2.364007318874242, 2.1936703962012065, 2.2751703017934006, 2.2904229265389975, 2.2056927879074353, 2.2193927206880177, 2.184850777360693, 2.2300277320203743, 2.086159758112264, 2.152517690017514, 2.166838274357234, 2.1612217039401123, 2.0979712866126876, 1.9330506534401133, 1.9843234061571349, 2.182639087544291, 2.168742644152198, 1.9522709272174872, 1.8594112719270781, 2.075629514206955, 2.1360730347616794, 2.118656995356318, 1.923947036119319, 2.0646920482747246, 1.9731547503032691, 2.0224963170080192, 1.956102062822736, 1.9255263561512272, 2.0829376424235817, 1.9869986142360179, 2.1266327981089135, 1.981046264112046, 1.7707012108281805, 2.004014987907428, 1.9064671449569408, 1.789306289069134, 2.0031772186530423, 1.951888942712709, 2.0684493997209623, 2.012941248198811, 1.854386096259146, 1.7385167620987187, 1.9910186758852162, 1.7650525806895991, 1.867293700107874, 1.9427923520687735, 1.7775893104405287, 1.9741892614672745, 1.834574810461256, 1.8795376856364558, 1.900696997296787, 1.7803496302566248, 1.6291495485237355, 1.8830818369912594, 1.8578709959515092, 1.8358273599828578, 1.7247666671440312, 1.7041418836724855, 1.79895193542895, 1.7754228888896209, 1.771681067605266, 1.6986649406503738, 1.7459969461629299, 1.690323815384143, 1.6410536286357726, 1.7705128029260881, 1.6942294152217878, 1.7614029671136997, 1.6376874873424192, 1.6085553918343702, 1.6971100279842004, 1.6618929345870022, 1.7952695444684001, 1.5080217173388117, 1.469484510894605, 1.5166092279071115, 1.783380917917183, 1.6280824972031396, 1.6985174083717536, 1.4643009147055115, 1.50600015656098, 1.695860560582113, 1.5123238197475766, 1.6550806742502018, 1.5292410027010048, 1.6364844358359063, 1.4681194160214188, 1.6213519474620284, 1.6118415058388762, 1.4133100276451513, 1.7027471506482692, 1.5659931215680507, 1.4831898183829944, 1.5650486035414444, 1.5333729491659103, 1.5241574626906105, 1.6017630461881187, 1.7199028473278604, 1.574199796073228, 1.615918087946039, 1.4833082074267085, 1.5667851590068294, 1.4063738221798978, 1.4780711181517256, 1.541599996999094, 1.4190952038655031, 1.6193394862936294, 1.5796659499941441, 1.5728251712315864, 1.6478520084041304, 1.4446214007468081, 1.4786329382698, 1.412710195200567, 1.3844887262100107, 1.4844814316763792, 1.4250589022331253, 1.3119076602758977, 1.4430103295086432, 1.4282898210036161, 1.4379731816009016, 1.4543492516434349, 1.5360775623715022, 1.2879218915125994, 1.5032005574302283, 1.4997272474007068, 1.3281820935006918, 1.4412784264800438, 1.3487594176249782, 1.5543778175813372, 1.3698411345306554, 1.4503408068815382, 1.3441772937416312, 1.301327936709397, 1.4689162891476426, 1.3089366230511112, 1.384263983469402, 1.5709261174151203, 1.197717544460511, 1.294859762075167, 1.3645529406725947, 1.1982492652614414, 1.3038468550397913, 1.2563157839909949, 1.3071673632024516, 1.436084754570576, 1.243646611424221, 1.470177827363592, 1.3594249325035923, 1.3532445113526745, 1.405188942268351, 1.1216651345060713, 1.284077824978679, 1.2807911390197417, 1.2505293464773755, 1.4585885260260485, 1.2542956365937588, 1.1811791032049237, 1.4079116109276482, 1.151500363499028, 1.0962549653513243, 1.3247353050796622, 1.383888172664979, 1.251536839995403, 1.274817382720093, 1.318015348503719, 1.207386455313411, 1.2876517534391991, 1.2442387602876215, 1.3178126351476238, 1.2095579805713552, 1.2568178285000133, 1.364929893006162, 1.2383947268752806, 1.2943304732103607, 1.1222237397559642, 1.1598699458077841, 1.2549622409330876, 1.2771874363533433, 1.2151275729727053, 1.1546678624947986, 1.2379065756836474, 1.2299876690179163, 1.21195813910303, 1.2403408701852237, 1.1361282531306822, 1.081551246576589, 1.228005480031456, 1.1541407041513234, 1.0859385779322441, 1.2100157425189213, 1.197252647869721, 1.0635448958036018, 1.149641519529078, 1.0810728008161423, 1.2877285598392354, 1.2033955012174382, 1.1794287097712879, 1.2890214749280462, 1.0155502129073464, 1.1167289480296976, 1.1652382974895499, 0.893355557391434, 1.2203424734258144, 1.393591468806581, 1.0821874774401385, 1.0507171999197693, 1.095056792309148, 1.2030443919383447, 1.1228481947619844, 1.1542435377242168, 1.0573688883524293, 1.1331237226287298, 1.1929461674892934, 1.1750848164517111, 1.1094514189151194, 1.1590125910326348, 1.2557208014926313, 1.1956505266227588, 1.0403075845400958, 1.2194982696083592, 1.1329693765247992, 1.16576602072309, 1.2507022753897916, 1.0399534499575196, 0.9754722345215749, 1.0682157622964656, 1.0555916334814666, 1.125846934131404, 0.9881271315152304, 1.1057710987603755, 1.0975720772715252, 0.9888269366516763, 1.0435716444214027, 1.0180646717289172, 1.0544246043165264, 1.1291272034663316, 1.1035429599709243, 1.0495409143570735, 1.0216790326452214, 0.9678858565158218, 1.2037787973384682, 1.0779978127532321, 1.2225304857909176, 0.9390206288638846, 0.9372292466121945, 1.0491795159529407, 1.0228242817770044, 0.9653905126402552, 1.1040048909616815, 0.9991048184911592, 1.0402262381031315, 1.0262054680401214, 1.075512761972382, 0.9957015101274922, 0.9954622838893004, 1.0230988266864576, 0.9999614589247934, 0.9641281579886817, 0.991703830487516, 0.9756492237080574, 0.8951514348915501, 1.0506253372387488, 0.9095182693126364, 0.9469269287596629, 1.0390051987015365, 1.0565683356056763, 1.0220749093240025, 1.0396961979365815, 0.8949323544520337, 1.0324902278057906, 0.9411695368239589, 0.875455411188632, 0.8894440293316491, 0.872672542767081, 1.0756039593656854, 0.902496140745792, 0.9697196168636095, 1.0769050104530047, 0.9919460747333498, 0.919437268549427, 0.976575572320684, 1.0346087391782013, 0.9518908043851132, 0.8855025123020253, 0.9430250011371712, 0.8627052971507402, 1.0109759280216042, 0.9088170193031642, 0.8415765647231443, 0.9463250722956368, 0.855484740911286], 'mae': [10.482096, 3.6114776, 2.8708832, 2.5916884, 2.4901974, 2.3954685, 2.2678182, 2.1842701, 2.2311149, 2.1465707, 2.1354012, 2.0649688, 2.0866392, 2.0381052, 2.0088122, 1.9928917, 1.9749875, 1.9686183, 1.9332622, 1.9401271, 1.866943, 1.8635213, 1.8968366, 1.8687851, 1.9052924, 1.856486, 1.8901635, 1.8247482, 1.831379, 1.8158753, 1.8503424, 1.7872455, 1.7599026, 1.8308815, 1.7419145, 1.7352698, 1.7175361, 1.6777364, 1.6897978, 1.7412083, 1.7810444, 1.676279, 1.6511055, 1.6713247, 1.6321424, 1.661678, 1.6269443, 1.7066078, 1.6216681, 1.5868319, 1.5906131, 1.5907907, 1.5852118, 1.560473, 1.5173509, 1.5539408, 1.5407107, 1.554221, 1.466442, 1.5442448, 1.5599812, 1.5142719, 1.5541242, 1.4941803, 1.483987, 1.5418036, 1.5024867, 1.5240748, 1.4650421, 1.4407516, 1.5236013, 1.44078, 1.4104234, 1.4641145, 1.5234839, 1.382189, 1.4550805, 1.4101851, 1.3885417, 1.365404, 1.3994638, 1.4213473, 1.4152331, 1.3517108, 1.2917843, 1.365826, 1.3713081, 1.357422, 1.3009424, 1.3889592, 1.3527607, 1.3196917, 1.3409245, 1.2806695, 1.3261639, 1.3266332, 1.365226, 1.3241893, 1.2807844, 1.3203187, 1.2741213, 1.3451463, 1.3269085, 1.2961507, 1.3004655, 1.3084195, 1.2937962, 1.2769984, 1.2491632, 1.2914573, 1.246104, 1.2091552, 1.2520111, 1.2214187, 1.2661319, 1.2246609, 1.2634604, 1.2860571, 1.1809905, 1.2490638, 1.230062, 1.2343036, 1.1875054, 1.2781512, 1.19517, 1.2473437, 1.2314813, 1.2146441, 1.1768861, 1.1902386, 1.1924978, 1.1751206, 1.1690513, 1.2089164, 1.1806616, 1.2227143, 1.147866, 1.1815406, 1.1361841, 1.1381599, 1.1551913, 1.1439433, 1.153801, 1.1732438, 1.172611, 1.1085799, 1.1442795, 1.1540902, 1.1563008, 1.157643, 1.1099519, 1.1710252, 1.1472089, 1.168373, 1.1419457, 1.0753236, 1.1554062, 1.1298808, 1.112866, 1.0983319, 1.0874014, 1.095856, 1.1217494, 1.0890387, 1.0967801, 1.1050973, 1.1133102, 1.1085547, 1.1118286, 1.0408738, 1.0591404, 1.0741059, 1.0931264, 1.0772793, 1.04897, 0.99457896, 1.0847737, 1.089983, 1.0051361, 1.057249, 1.0643559, 1.0417764, 1.0522181, 1.1011921, 1.0497586, 1.0215402, 1.0501093, 1.0206487, 1.045992, 1.0161185, 1.0460565, 1.0262389, 1.0303304, 0.9922427, 1.0654937, 1.0311725, 1.0297695, 1.0287026, 1.0628339, 0.9566405, 1.0272716, 0.974864, 0.9933091, 0.9801704, 1.0025318, 1.0669578, 0.97631824, 0.98447883, 1.0226585, 1.0601394, 0.9732974, 1.0015936, 1.002618, 0.98361295, 1.016186, 0.97066194, 0.9713178, 0.98959714, 0.9740524, 0.98612183, 0.9724407, 0.98856467, 0.93875223, 1.0028433, 1.0103636, 1.021373, 0.94393116, 0.9829925, 0.9951026, 1.0153829, 0.96889657, 0.9292851, 0.97135055, 0.9953045, 0.9926218, 0.96617365, 0.97039634, 0.9662118, 0.9517622, 0.9660281, 1.0101291, 0.9275819, 1.0023869, 0.94923234, 0.9652578, 0.9413573, 0.9973347, 0.98025787, 0.914862, 0.96411043, 0.9318554, 0.9753368, 0.9425804, 0.9732989, 0.9675198, 0.9442445, 0.9511916, 0.98853755, 0.9817077, 0.9347768, 0.96163195, 0.91568273, 0.96610916, 0.9099146, 0.94347966, 0.9001249, 0.96387756, 0.9565243, 0.92974544, 0.9621499, 0.90958154, 0.8959957, 0.9439769, 0.8540991, 0.8876403, 0.91488236, 0.9137954, 0.87225837, 0.9058336, 0.8867532, 0.9124754, 0.96541965, 0.8706223, 0.88247013, 0.9279864, 0.9379519, 0.9055288, 0.88696754, 0.8623025, 0.91712874, 0.90328175, 0.89753485, 0.9235155, 0.87788427, 0.9340895, 0.8953519, 0.91486067, 0.89913094, 0.9180232, 0.91435206, 0.91121125, 0.8748154, 0.89695865, 0.93051326, 0.881844, 0.89455706, 0.92033935, 0.8727554, 0.88233125, 0.84775585, 0.9039527, 0.8166607, 0.86551285, 0.8926078, 0.90627426, 0.91319764, 0.87867886, 0.8825418, 0.90004355, 0.8747528, 0.8801249, 0.84722066, 0.82172024, 0.85836196, 0.80954415, 0.800795, 0.8419467, 0.8420734, 0.86277616, 0.88089657, 0.8886698, 0.8533973, 0.88029593, 0.8938006, 0.8154171, 0.85497916, 0.8725614, 0.88371414, 0.8537776, 0.84278685, 0.8147765, 0.82032347, 0.8563922, 0.83212644, 0.84033203, 0.8865235, 0.799892, 0.8444317, 0.8718266, 0.7955815, 0.8502285, 0.8134441, 0.8316831, 0.8474228, 0.8352442, 0.8661115, 0.84175396, 0.8373292, 0.8834805, 0.78944874, 0.80510354, 0.84464717, 0.8081738, 0.8509568, 0.8086682, 0.8209047, 0.8466426, 0.78600115, 0.7759953, 0.7875033, 0.8207716, 0.8195173, 0.81070703, 0.83479375, 0.78960973, 0.8466411, 0.8010305, 0.8071373, 0.8165642, 0.7973323, 0.8167554, 0.79134566, 0.8467136, 0.7623888, 0.7668035, 0.83622044, 0.8286523, 0.82684845, 0.8019483, 0.7765291, 0.81079084, 0.8255889, 0.8165576, 0.7931514, 0.77243143, 0.81538767, 0.7616216, 0.7820858, 0.80844283, 0.7972698, 0.75449073, 0.7807795, 0.7401558, 0.8045355, 0.76279885, 0.7985192, 0.80781573, 0.7328736, 0.801391, 0.77203614, 0.71612173, 0.7463951, 0.81319255, 0.75455904, 0.73055667, 0.7650609, 0.7665845, 0.7684506, 0.77513266, 0.7785531, 0.7688829, 0.75275296, 0.78673494, 0.78511935, 0.7780806, 0.7886314, 0.77386683, 0.7593762, 0.76084167, 0.78045857, 0.8272445, 0.76364535, 0.76072943, 0.75758064, 0.7637898, 0.77939993, 0.7705034, 0.7561452, 0.74352294, 0.7537622, 0.74384314, 0.75632584, 0.7562607, 0.7608113, 0.75253755, 0.7774121, 0.76098096, 0.71653056, 0.74034274, 0.7929444, 0.76332104, 0.7802674, 0.70992064, 0.70627886, 0.7560445, 0.76626056, 0.75032455, 0.7736787, 0.7411175, 0.76692164, 0.7460987, 0.75177246, 0.708163, 0.70820606, 0.6976229, 0.7155028, 0.72022146, 0.733814, 0.7349209, 0.6694964, 0.7332616, 0.69342124, 0.6893779, 0.73255354, 0.7488694, 0.75076663, 0.74310035, 0.6978421, 0.6830059, 0.70936996, 0.6955235, 0.71154934, 0.6545219, 0.7474175, 0.69270766, 0.7331063, 0.7407678, 0.7177447, 0.6954927, 0.7500192, 0.73790944, 0.714143, 0.68724436, 0.71528405, 0.69827473, 0.70133764, 0.70638144, 0.67935973, 0.7340099, 0.70512515]}\n"
     ]
    }
   ],
   "source": [
    "print( history_dict ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-662-5817204bb8d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss' ] \n",
    "val_loss= history.history['val_loss' ] \n",
    "epochs = range( 1 , len(loss) + 1 ) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot( epochs , loss_values , 'bo' , label = 'Training loss' )\n",
    "plt.plot( epochs , val_loss_values , 'b' , label = 'Validation loss' ) \n",
    "plt.title( 'Training vs validation loss' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'Loss' ) \n",
    "plt.legend() \n",
    "\n",
    "# Here we plot the training and validation accurary side by side.\n",
    "\n",
    "\n",
    "acc = history.history['acc' ] \n",
    "val_acc = history.history['val_acc' ]\n",
    "\n",
    "\n",
    "epochs = range( 1 , len(loss_values) + 1 ) \n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot( epochs , acc_values , 'bo' , label = 'Training acc' )\n",
    "plt.plot( epochs , val_acc_values , 'b' , label = 'Validation acc' ) \n",
    "plt.title( 'Training vs validation acc' ) \n",
    "plt.xlabel( 'Epochs' ) \n",
    "plt.ylabel( 'acc' ) \n",
    "plt.legend() \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 9. Conclusion<a name=\"Ch9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key concepts<a name=\"KeyConcepts\"></a>\n",
    "Don't overlook this, as people may ask some high level questions regarding this. ----Wei Ge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The limitations of deep learning<a name=\"Limitation\"></a>\n",
    "Do not confuse this with the limitation of ML. You should definitly come back to here later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The future of deep learning<a name=\"Future\"></a>\n",
    "Do not confuse this with the limitation of ML. You should definitly come back to here later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models as programs <a name = 'model_as_programs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beyond backpropagation and differentiable layers <a name = 'Beyond'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automated machine learning <a name = 'autoML'></a>\n",
    "1. There are TWO parts in ML\n",
    "    1. data munging\n",
    "    2. Structure munging\n",
    "2. The first one is hard to automated, as it needs higher level knowledges, while the second is easier, simply do the searching\n",
    "3. It is common to use **AutoML** system to do the **hyperparameter** space tuning.\n",
    "4. **Hyperopt** can be used to tune the most basic things, such as the number of layers, nodes etc. See Ch 7.\n",
    "5. Another important AutoML direction involves learning model architecture jointly with model weights.\n",
    "6. a truly powerful AutoML system would evolve architectures at the same time the features of the model were being tuned via backpropagation on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lifelong learning and modular subroutine reuse <a name = 'Lifelong'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The long term vision <a name = 'Long_term'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to stay up to date? <a name = 'StayUpToDate' ></a>\n",
    "Essentially, practice with Kaggle, and read arXiv. And use arXiv Sanity Preserver (http://arxiv-sanity.com) serves as a recommendation engine for new papers and can help you keep track of new develop- ments within a specific narrow vertical of deep learning. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 10. Some practices with Python<a name=\"Ch10\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practices from Ch2: Tensor operation <a name = \"practiceCh2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "(train_images , train_labels) , (test_images , test_labels) = mnist.load_data() \n",
    "\n",
    "\n",
    "batch1 = train_images[:128]\n",
    "batch2 = train_images[128 : 256]\n",
    "# print( batch1 )\n",
    "# print( batch2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# relu(x) = max(x,0) which are element wise\n",
    "def my_relu(x):\n",
    "    assert len(x.shape) == 2\n",
    "    x = x.copy() # to avoid overwringt the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j]=max(x[i,j],0)\n",
    "    return x\n",
    "\n",
    "def my_add(x,y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()\n",
    "    y = y.copy()    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] +=  y[i,j]\n",
    "    return x\n",
    "\n",
    "from numpy.random import rand\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = rand(2,3)\n",
    "y = rand(2,3)\n",
    "\n",
    "print( my_relu(x)  - np.maximum(x,0) ) \n",
    "\n",
    "\n",
    "print()\n",
    "print( my_add(x,y) - (x+y) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70676923 0.88308022 0.78647804]\n",
      " [0.86417064 0.63928508 0.69393197]]\n",
      "[0.00141908 0.10005543 0.56853702]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "### Brocasting: Adding vector to matrix \n",
    "\n",
    "def my_add_matrix_vec(x,y):\n",
    "    assert len(x.shape) == 2 \n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] += y[j]\n",
    "    return x\n",
    "\n",
    "x = rand(2,3)\n",
    "y = rand(3)\n",
    "print( x )\n",
    "print( y )\n",
    "print( x+y - ( my_add_matrix_vec(x,y) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[0.80334389 0.97725329 0.91657235]\n",
      "[0.80334389 0.97725329 0.91657235]\n",
      "[[0.70596093 0.28653512 0.81556951]\n",
      " [0.80334389 0.97725329 0.91657235]]\n"
     ]
    }
   ],
   "source": [
    "### Tensor dot\n",
    "\n",
    "x = rand(2,3) \n",
    "y = rand(3,5)\n",
    "\n",
    "def my_tensor_product( x , y ):\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    z = np.zeros( ( x.shape[0] , y.shape[1] ) ) \n",
    "    for i in range(x.shape[0]):\n",
    "        for k in range(x.shape[1]):\n",
    "            for j in range(y.shape[1]):\n",
    "                z[i,j] += x[i,k] * y[k,j]                \n",
    "    return z\n",
    "\n",
    "print( np.dot(x,y) - my_tensor_product(x,y) )\n",
    "print( x[1])\n",
    "print( x[1,])\n",
    "print( x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[0]\n",
      " [1]\n",
      " [3]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "[[0 1]\n",
      " [3 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "### Tensor reshape\n",
    "\n",
    "x = np.array( [ [0,1,3] , [2,3,4] ] ) \n",
    "print(x.shape)\n",
    "y = x.reshape( (6,1) ) \n",
    "print(y)\n",
    "\n",
    "y = x.reshape( (3,2) ) \n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[9.98797194e-01 8.24733348e-01 1.81927917e-01 1.50327466e-01]\n",
      "  [1.75653877e-01 3.34324942e-01 3.70123830e-01 7.61896598e-01]\n",
      "  [9.04093608e-01 1.69391072e-01 6.59808492e-01 5.97735856e-01]]\n",
      "\n",
      " [[5.73105840e-04 9.67918859e-01 9.80781226e-01 9.76237624e-01]\n",
      "  [9.48356176e-01 5.29111571e-01 6.86531476e-01 1.67380610e-02]\n",
      "  [2.04144808e-01 1.85408216e-01 8.76740103e-01 4.72942284e-01]]]\n",
      "[[0.49968515 0.8963261  0.58135457 0.56328255]\n",
      " [0.56200503 0.43171826 0.52832765 0.38931733]\n",
      " [0.55411921 0.17739964 0.7682743  0.53533907]]\n",
      "[[0.69284823 0.44281645 0.40395341 0.50331997]\n",
      " [0.38435803 0.56081288 0.8480176  0.48863932]]\n",
      "[[0.53894648 0.41049981 0.58275726]\n",
      " [0.7313777  0.54518432 0.43480885]]\n"
     ]
    }
   ],
   "source": [
    "# mean and std of arrays\n",
    "a = rand(2,3,4)\n",
    "print(a)\n",
    "print( a.mean(axis=0) )\n",
    "print( a.mean(axis=1) )\n",
    "print( a.mean(axis=2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practices from Ch3: List and array <a name = \"practiceCh3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practices from Ch4: Random stuff <a name = \"practiceCh4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print( np.random.randint( 0 , 2 , (3,4) )  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practices from Ch5: Python generators <a name = \"practiceCh5\"></a>\n",
    "\n",
    "Python generator essentially acts as an iterator, which you can use it with the for...in operator. It is built with **yield** operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "inside generator\n",
      "1\n",
      "1\n",
      "2\n",
      "inside generator\n",
      "2\n",
      "2\n",
      "3\n",
      "inside generator\n",
      "3\n",
      "3\n",
      "4\n",
      "inside generator\n",
      "4\n",
      "4\n",
      "5\n",
      "inside generator\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def generator():\n",
    "    i = 0 \n",
    "    while True:\n",
    "        print(i) \n",
    "        i += 1\n",
    "        print(i) \n",
    "        print( 'inside generator' ) \n",
    "        yield i\n",
    "        \n",
    "for item in generator():\n",
    "    print(item)\n",
    "    if item>4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont know what the hell the above is doing, let's try this. In particular, the While True thing is really confusing. It is an infinite loop, that was interrupted by the yield thing. See the last example. \n",
    "\n",
    "The yield statement suspends function’s execution and sends a value back to caller, **but retains enough state to enable function to resume where it is left off.** When resumed, the function continues execution immediately after the last yield run. This allows its code to produce a series of values over time, rather them computing them at once and sending them back like a list.\n",
    "\n",
    "Let’s see with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# A Simple Python program to demonstrate working \n",
    "# of yield \n",
    "  \n",
    "# A generator function that yields 1 for first time, \n",
    "# 2 second time and 3 third time \n",
    "def simpleGeneratorFun(): \n",
    "    yield 1\n",
    "    yield 2\n",
    "    yield 3  \n",
    "\n",
    "# Driver code to check above generator function \n",
    "for value in simpleGeneratorFun():  \n",
    "    print(value) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return sends a specified value back to its caller whereas Yield can produce a sequence of values. **We should use yield when we want to iterate over a sequence, but don’t want to store the entire sequence in memory.**\n",
    "\n",
    "Yield are used in Python generators. A generator function is defined like a normal function, but whenever it needs to generate a value, it does so with the yield keyword rather than return. If the body of a def contains yield, the function automatically becomes a generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A Python program to generate squares from 1 \n",
    "# to 100 using yield and therefore generator \n",
    "  \n",
    "# An infinite generator function that prints \n",
    "# next square number. It starts with 1 \n",
    "def nextSquare(): \n",
    "    i = 1; \n",
    "  \n",
    "    # An Infinite loop to generate squares  \n",
    "    while True: \n",
    "        yield i*i                 \n",
    "        i += 1  # Next execution resumes  \n",
    "                # from this point      \n",
    "            \n",
    "# Driver code to test above generator function \n",
    "for num in nextSquare(): \n",
    "    if num > 100: \n",
    "         break    \n",
    "    print(num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Practices from Ch5: Python generators <a name = \"practiceCh5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = [5, 9, 'cat']\n",
    "\n",
    "# converting list to iterator\n",
    "randomIterator = iter(random)\n",
    "print(randomIterator)\n",
    "\n",
    "# Output: 5\n",
    "print(next(randomIterator))\n",
    "\n",
    "# Output: 9\n",
    "print(next(randomIterator))\n",
    "\n",
    "# Output: 'cat'\n",
    "print(next(randomIterator))\n",
    "\n",
    "# This will raise Error\n",
    "# iterator is exhausted\n",
    "print(next(randomIterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ...  6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ...  9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ... -9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ... -6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ... -2.44929360e-16\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ...  6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ...  9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ... -9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ... -6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ... -2.44929360e-16\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ...  6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ...  9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ... -9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ... -6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ... -2.44929360e-16\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ...  6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ...  9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ... -9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ... -6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ... -2.44929360e-16\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ...  6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ...  9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ... -9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ... -6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ... -2.44929360e-16\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ...  6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ...  9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00 -5.02688387e-01 ... -9.84807753e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  8.97078789e-02 ... -6.42787610e-01\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  3.23663436e-01 ... -2.44929360e-16\n",
      "    0.00000000e+00  0.00000000e+00]]] 0\n"
     ]
    }
   ],
   "source": [
    "def quadmodel_generator(N=32):\n",
    "    g1 = -np.kron([[0, -1j], [1j, 0]], [[0, 1], [1, 0]])\n",
    "    g2 = -np.kron([[0, -1j], [1j, 0]], [[0, -1j], [1j, 0]])\n",
    "    g3 = -np.kron([[0, -1j], [1j, 0]], [[1, 0], [0, -1]])\n",
    "    g4 = np.kron([[0, 1], [1, 0]], np.eye(2))\n",
    "    kr = np.linspace(0, 2 * np.pi, N)\n",
    "    kx, ky = np.meshgrid(kr, kr) # row corresponds to y, column corresponds to x\n",
    "    while True:\n",
    "        coef = np.random.uniform(-1.5, 1.5, 2) # coupling lx, ly between [-2, 2]\n",
    "        h = (np.kron((coef[0] + np.cos(kx)), g4.flatten()) + np.kron(np.sin(kx), g3.flatten()) + \n",
    "             np.kron((coef[1] + np.cos(ky)), g2.flatten()) + np.kron(np.sin(ky), g1.flatten())).reshape((N, N, 16))\n",
    "        hr, hi = np.real(h), np.imag(h)\n",
    "        h_tot = np.concatenate((hr, hi), axis = 2) # h_tot has dimension (dim_kx, dim_ky, 16*2)    \n",
    "        if abs(coef[0]) < 1 and abs(coef[1]) < 1:\n",
    "            q = 1\n",
    "        else:\n",
    "            q = 0\n",
    "        yield h_tot, q\n",
    "gen_ex = quadmodel_generator(N=10)\n",
    "h, q = next(gen_ex)\n",
    "# h, q = quadmodel_generator( N = 100 ) # This is WRONG\n",
    "print( h, q ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's give a BAD example that to illustrate that we should NEVER use python generator, in particular if you want to generate sth that is RANDOM iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import rand\n",
    "def a(num):\n",
    "    ranNum = rand(1,num)\n",
    "    mean = randNum.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59407452 0.78586037 0.50499112 0.10694276 0.98260703 0.82862644\n",
      "  0.09221045 0.48281331 0.2994317  0.37811933]]\n",
      "0.505567702655424\n",
      "0.2841390880823435\n"
     ]
    }
   ],
   "source": [
    "aa=rand(1,10)\n",
    "print(aa)\n",
    "print( aa.mean() )\n",
    "print( aa.std() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicse from project: Read data from text file<a name = 'practice_project'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95687    0.96216   -0.005363   0.96002    0.95766    0.98592\n",
      "  0.96729    0.9692     0.9831     0.92667    0.96243    0.97665\n",
      "  0.011367   0.98852    0.95285    0.98762    0.90987   -0.97751\n",
      "  0.95089    0.97094    0.95676    0.96191    0.022448   0.97785\n",
      " -0.004487   0.95255   -0.98735    0.97292    0.9699     0.96305\n",
      " -0.0042447  0.98577    0.9466     0.95806    0.97409    0.96378\n",
      " -0.96489    0.98984    0.96559    0.99179    0.95495    0.01626\n",
      "  0.94534    0.0036222 -0.0079214  0.93339   -0.004023   0.98064\n",
      "  0.003581   0.97424   -0.98605   -0.96047   -0.0045673 -0.95997\n",
      "  0.97761    0.96271   -0.98335    0.94304   -0.96966    0.97065\n",
      "  0.94207    0.96031    0.061725   0.97656    0.96222    0.95971\n",
      "  0.97743    0.95592    0.96622    0.96623    0.95721    0.95809\n",
      "  0.9758     0.95691    0.96286   -0.022124   0.97745    0.94884\n",
      " -0.0029064  0.96718    0.94915    0.97345    0.97412    0.96505\n",
      " -0.9709     0.94477    0.90363    0.97192    0.96708    0.028694\n",
      "  0.95525    0.94927    0.93634    0.96303    0.97194   -0.057264\n",
      "  0.97022    0.93336    0.97185    0.95361  ]\n",
      "(100,)\n",
      "float64\n",
      "(100, 32, 32, 32)\n",
      "[[[-3.6234e-01 -4.2673e-17 -1.9985e+00 ...  3.5827e-16  4.2419e-01\n",
      "    0.0000e+00]\n",
      "  [-3.8924e-01 -1.6392e-02 -2.0292e+00 ...  1.9509e-01  4.1876e-01\n",
      "    0.0000e+00]\n",
      "  [-4.0579e-01 -3.2154e-02 -2.0162e+00 ...  3.8268e-01  4.0267e-01\n",
      "    0.0000e+00]\n",
      "  ...\n",
      "  [-2.3166e-01  4.6680e-02 -1.6640e+00 ... -5.5557e-01  3.7653e-01\n",
      "    0.0000e+00]\n",
      "  [-2.8200e-01  3.2154e-02 -1.8123e+00 ... -3.8268e-01  4.0267e-01\n",
      "    0.0000e+00]\n",
      "  [-3.2613e-01  1.6392e-02 -1.9253e+00 ... -1.9509e-01  4.1876e-01\n",
      "    0.0000e+00]]\n",
      "\n",
      " [[-3.7159e-01 -5.1588e-02 -1.9965e+00 ...  1.7026e-02  4.1876e-01\n",
      "    0.0000e+00]\n",
      "  [-3.9776e-01 -6.7979e-02 -2.0272e+00 ...  2.1223e-01  4.1354e-01\n",
      "    0.0000e+00]\n",
      "  [-4.1361e-01 -8.3741e-02 -2.0141e+00 ...  3.9994e-01  3.9765e-01\n",
      "    0.0000e+00]\n",
      "  ...\n",
      "  [-2.4300e-01 -4.9079e-03 -1.6620e+00 ... -5.3888e-01  3.7049e-01\n",
      "    0.0000e+00]\n",
      "  [-2.9270e-01 -1.9434e-02 -1.8103e+00 ... -3.6589e-01  3.9681e-01\n",
      "    0.0000e+00]\n",
      "  [-3.3612e-01 -3.5196e-02 -1.9232e+00 ... -1.7818e-01  4.1311e-01\n",
      "    0.0000e+00]]\n",
      "\n",
      " [[-3.7119e-01 -1.0119e-01 -1.9904e+00 ...  3.3398e-02  4.0267e-01\n",
      "    0.0000e+00]\n",
      "  [-3.9665e-01 -1.1758e-01 -2.0211e+00 ...  2.2872e-01  3.9765e-01\n",
      "    0.0000e+00]\n",
      "  [-4.1183e-01 -1.3335e-01 -2.0081e+00 ...  4.1653e-01  3.8196e-01\n",
      "    0.0000e+00]\n",
      "  ...\n",
      "  [-2.4460e-01 -5.4513e-02 -1.6559e+00 ... -5.2283e-01  3.5381e-01\n",
      "    0.0000e+00]\n",
      "  [-2.9367e-01 -6.9039e-02 -1.8042e+00 ... -3.4974e-01  3.8032e-01\n",
      "    0.0000e+00]\n",
      "  [-3.3642e-01 -8.4801e-02 -1.9172e+00 ... -1.6192e-01  3.9681e-01\n",
      "    0.0000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.8190e-01  1.4691e-01 -1.9805e+00 ... -4.8486e-02  3.7653e-01\n",
      "    0.0000e+00]\n",
      "  [-3.1088e-01  1.3052e-01 -2.0112e+00 ...  1.4627e-01  3.7049e-01\n",
      "    0.0000e+00]\n",
      "  [-3.2945e-01  1.1476e-01 -1.9982e+00 ...  3.3354e-01  3.5381e-01\n",
      "    0.0000e+00]\n",
      "  ...\n",
      "  [-1.4528e-01  1.9359e-01 -1.6460e+00 ... -6.0310e-01  3.3061e-01\n",
      "    0.0000e+00]\n",
      "  [-1.9747e-01  1.7906e-01 -1.7943e+00 ... -4.3051e-01  3.5620e-01\n",
      "    0.0000e+00]\n",
      "  [-2.4360e-01  1.6330e-01 -1.9073e+00 ... -2.4324e-01  3.7171e-01\n",
      "    0.0000e+00]]\n",
      "\n",
      " [[-3.1661e-01  1.0119e-01 -1.9904e+00 ... -3.3398e-02  4.0267e-01\n",
      "    0.0000e+00]\n",
      "  [-3.4495e-01  8.4801e-02 -2.0211e+00 ...  1.6146e-01  3.9681e-01\n",
      "    0.0000e+00]\n",
      "  [-3.6289e-01  6.9039e-02 -2.0081e+00 ...  3.4883e-01  3.8032e-01\n",
      "    0.0000e+00]\n",
      "  ...\n",
      "  [-1.8184e-01  1.4787e-01 -1.6559e+00 ... -5.8831e-01  3.5620e-01\n",
      "    0.0000e+00]\n",
      "  [-2.3346e-01  1.3335e-01 -1.8042e+00 ... -4.1563e-01  3.8196e-01\n",
      "    0.0000e+00]\n",
      "  [-2.7897e-01  1.1758e-01 -1.9172e+00 ... -2.2826e-01  3.9765e-01\n",
      "    0.0000e+00]]\n",
      "\n",
      " [[-3.4377e-01  5.1588e-02 -1.9965e+00 ... -1.7026e-02  4.1876e-01\n",
      "    0.0000e+00]\n",
      "  [-3.7140e-01  3.5196e-02 -2.0272e+00 ...  1.7795e-01  4.1311e-01\n",
      "    0.0000e+00]\n",
      "  [-3.8866e-01  1.9434e-02 -2.0141e+00 ...  3.6543e-01  3.9681e-01\n",
      "    0.0000e+00]\n",
      "  ...\n",
      "  [-2.1101e-01  9.8267e-02 -1.6620e+00 ... -5.7226e-01  3.7171e-01\n",
      "    0.0000e+00]\n",
      "  [-2.6200e-01  8.3741e-02 -1.8103e+00 ... -3.9948e-01  3.9765e-01\n",
      "    0.0000e+00]\n",
      "  [-3.0683e-01  6.7979e-02 -1.9232e+00 ... -2.1200e-01  4.1354e-01\n",
      "    0.0000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array \n",
    "import re\n",
    "\n",
    "path = '/Users/MaoLin/Documents/LookingForJobs/Machine_Learning/' \n",
    "\n",
    "# Do the target thing first, that is easier\n",
    "f1 = path + 'test_target_1_smaller.txt'\n",
    "f1 = open( f1 , \"r\" ) \n",
    "f1_content = []\n",
    "\n",
    "for line in f1:\n",
    "    f1_content.append(float( line.strip('\\n')) )\n",
    "test_target = array( f1_content )     \n",
    "\n",
    "print(test_target)\n",
    "print(test_target.shape)\n",
    "print(test_target.dtype)\n",
    "\n",
    "\n",
    "# Now do the data\n",
    "\n",
    "f2 = path + 'test_data_1_smaller.txt'\n",
    "f2 = open( f2 , \"r\" ) \n",
    "Nkx = 32 \n",
    "Nky = 32\n",
    "test_data = np.zeros( ( len( test_target ) , Nkx , Nky , 32 ) ) \n",
    "ind = -1 \n",
    "\n",
    "for line in f2:\n",
    "    temp = line.strip( '\\n' )  # read each line as string\n",
    "    temp = re.split( ',' , temp )   # split the string to a list of char\n",
    "    temp = [float(i) for i in temp] # put every char in the list to num\n",
    "    temp = array( temp )  \n",
    "    \n",
    "    # now determine which slot we shall put this line of thing \n",
    "    ind += 1\n",
    "\n",
    "    aa = ind // ( 32*32 ) \n",
    "    ind2 = ind % ( 32*32 ) \n",
    "    bb = ind2 // 32\n",
    "    cc = ind2 % 32 \n",
    "#     print( ind , aa  , bb , cc ) \n",
    "    \n",
    "    test_data[ aa , bb , cc , : ] = temp\n",
    "    \n",
    "print( test_data.shape ) \n",
    "print( test_data[0] )\n",
    "\n",
    "# # To get ourselves some sense how the indexing work\n",
    "# a = 4\n",
    "# b = 2\n",
    "# c = 4\n",
    "# ind = 32 * 32 * a + 32 * b + c\n",
    "# aa = ind // (32*32 ) \n",
    "# print( aa ) \n",
    "# ind2 = ind % (32*32) \n",
    "# bb = ind2 // 32\n",
    "# print( bb ) \n",
    "# cc = ind2 % 32 \n",
    "# print( cc ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print( test_data[0,0,0,] )\n",
    "print( len( test_target ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a = 4\n",
    "b = 2\n",
    "c = 4\n",
    "ind = 32 * 32 * a + 32 * b + c\n",
    "aa = ind // (32*32 ) \n",
    "print( aa ) \n",
    "ind2 = ind % (32*32) \n",
    "bb = ind2 // 32\n",
    "print( bb ) \n",
    "cc = ind2 % 32 \n",
    "print( cc ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "[0.52961625 0.91558402 0.67111505 0.31227922]\n",
      "[[[0.52961625 0.91558402 0.67111505 0.31227922]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros( ( 2 , 3, 4 ) ) \n",
    "# a = rand( 2 , 3 , 4 ) \n",
    "b = rand( 4,1 ) \n",
    "b = np.transpose(b) \n",
    "print( a ) \n",
    "print( b[0] ) \n",
    "a[0,0,:] = b.flatten()\n",
    "print( a ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicse from project: Plot something<a name = 'practice_project_2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hamiltonian2(h, q):\n",
    "    dim_k = h.shape[0]\n",
    "    assert dim_k == h.shape[1]\n",
    "    assert h.shape[2] == 32\n",
    "    ham_32 = np.zeros((dim_k * 4, dim_k * 8))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            ham_32[ i * dim_k : (i + 1) * dim_k , j * dim_k : (j + 1) * dim_k ] = h[ :, :, i * 4 + j ]\n",
    "    for i in range(4):\n",
    "        for j in range(4,8,1):\n",
    "            ham_32[ i * dim_k : (i + 1) * dim_k , j * dim_k : (j + 1) * dim_k ] = h[ :, :, (4-1)*4 + i * 4 + j ]        \n",
    "    plt.figure(figsize=(16, 8))\n",
    "#     plt.imshow(ham_32, cmap='viridis')\n",
    "    plt.imshow(ham_32, cmap='viridis', vmin=-2.5, vmax=2.5)\n",
    "    plt.title('q={}'.format(q))\n",
    "    plt.colorbar()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1UAAAHBCAYAAACBl5G6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdfbQkZ13o++9T1XuGmITIAtYFFfImea4guBQQ44LFEfEAeni5eOUeYIkHEeL1QMZkBgkoGN7fMonRCIl4D8jCeyUgIhdRlKicCEHh8BJAeYjJJKi8nHs8MYSQzOyueu4fVT27d2fPzN67e3dX1/5+1qpV3VXVNc/u2fvX/avf8zwVcs5IkiRJkranWHQDJEmSJGmZmVRJkiRJ0hRMqiRJkiRpCiZVkiRJkjQFkypJkiRJmoJJlSRJkiRNYbDoBkiSJEnaPeqvnzPzezoV9/tymPU5t/TvL/IflyRJkqRlZ6VKkiRJ0tzU1DM/56IrRYv+9yVJkiRpqVmpkiRJkjQ3VZ59pWrRSY2VKkmSJEmawqKTOkmSJEm7SM3MJ/9bOJMqSZIkSXOzExNVLJrd/yRJkiRpClaqJEmSJM1NlfvX/c9KlSRJkiRNwUqVJEmSpLlxogpJkiRJmkLVw6TK7n+SJEmSNAUrVZIkSZLmpo/d/6xUSZIkSdIUrFRJkiRJmps+TqluUiVJkiRpbupFN2AHmFRJkmYqxvgdwEuBZwLfDRwCfht4S0qpf5cnJUm7nkmVJGlmYowl8B7gJ4EPAe8FngRcAZwJHFhc6yRJXeCU6pIkHd//QZNQXZJS+qmU0kXAI4C/BC6MMT50oa2TJGkHmFRJkmbpPwND4HWjDSmlVeDXgAA8b0HtkiR1RJVnvyya3f8kaZeKMT4QeA3wROAk4BrgfOC/AjenlP5djPGvgcee4FQfbY/dC/ww8NmU0q0Tx/wd8O1NnEuSpKVjUiVJu1CbUP0tcD/g/wUS8FTgr4CTxw59B/DXJzjdze36dJrPlRsnD0gpVTHGfwLOmaLZkqQecPY/SVJfvJEmofqFlNL/BRBjfAXN2KezRgellN6xhXPeu13/2zH239b8M3GQUhpuucWSpF6oCItuwsw5pkqSdpkY40k0Val/GCVUACmlO4H9U5x6pV0fPsb+0fZ7TPFvSJLUOVaqJGn3OZtmDNUnN9j3CeDO0ZMY438CzjjB+W5uK1qj1+05xnF7gUwztkqStEvVHZhYYtZMqiRp9zmtXX9rckdKqY4xjk8y8Z/YxEQVNGOvRq877RjHnQZ8K6XUx+70kqRdzKRKknafUfJz/8kdMcZAMzbqBoCU0r/bwnlvBo7Q3OR38rwl8ADg77fWVElS3zimSpLUB18GbgceEWNcmdj3/TTd9LasnXzib4EfjDGeOrH7h4HvAK7bzrklSf1REWa+LJpJlSTtMm3y83aaytGvjLbHGPfQzAo4jXfSJGWvHDvvCvDq9unbpjy/JEmdY/c/SdqdXg48DnhNjPHHgM8DPw58z5TnfTvwXOCCGONDgf9Gc3PhHwAuSSl9fsrzS5KWXJ0XX1maNStVkrQLpZS+CTwGuIKmy995wL/QJEDTnLdqz3EZ8H3APpoLeC8EXjLNuSVJ6qqQcw/nNJQkbUuM8TtpJrL46BYnqZAkaVM+/ZUHzjwB+aEHfmWh5a+5d/+LMT6fpg//9wCfBS5MKTlwWZIkSdoFqh52lpvrTxRjfA5wJfAu4KeBfwM+HGO82/S7kiRJkrQM5lapau998irgd1JKr2y3/QWQgAuA8+fVFkmSJEmL0ceJKubZ/e97gdOBD4w2pJRWY4x/wpQDoyVJs5FS+jfowA0/JElaIvNMqs5p1/84sf0m4OwYY9nOGiVJkiSppxZ1s94YY0kzK+3zgQcCtwBvAX47pTTV5BnzTKru2a5vn9h+O83YrpOBb270wsPDYd478JZakiRJ0rgzrriEm194YKl6GFR5YRNVvBy4iOaG9J+gubXIbwDfAbxpmhPPM1MZ/WdPZoGj7fWxXrh3MODMyw/uSKO05tC+/b7PO+zQvv0847rzqHM42p94WJfUNM+HdUGVCw4PB1Q5sFqVVHVgWJVUdUFVFVRVoK4L6iMluQowDFAHQhWgChRDCFWgWIVQNY9D3T6um4Wa5rjR89wsIed2ffe25wAEyCE0f7UB6gHkolko1h7ncrTO1HvGHg+AMpPLDEWGQabYUxHKTFHUlGWmLGvKomZQVpRFZqWs2FNWDIqaMtQMipoiZAoyg6Ipbg+KtfBx9blXcfabL93Z/8hd7sYXX2ismANj8s47tG8/Z1xxCZSsxaUyEwY1xUp9NC4165oytI+LzJ6yamJRyIR2XbTBs2i/6tTtV5xRzM9tnM9AVRccGTaxvc6Bqiqo60BdFU2MrwL5SAFVG+OHTZw/GterJq4XR8LR+F5UY7F+tB7mdbE+1G2Mz2vro9pvZGvxvl2XY/G9CBNxvv0sKCEPoC7zun037b+QM956yXz+Q3erctENWA4xxgK4EHhzSum17eZrYoz3BQ6wREnVbe36VOAbY9tPoUmo7phjWyT1mbffk6TOyKVBWevVi5lS/TTgncD7JrYn4L4xxpNTStvOR+aZVN3Qrs9i/biqs4A0bT9GSRrZqNImSVqQlWN2RpLmJqV0K/DCDXY9GfjnaRIqmH9S9U/A04A/B4gxrgA/BfzJHNshqe9MqiSpM8LApErrLWqiikkxxl8AHs8Mbu00t6QqpZRjjG8Arogx3gp8jCZbvA9w2bzaIan/rFRJUncEu/+pg2KMzwauBN4LXDHt+eY6pV5K6S0xxpNopjK8APgs8ISU0k3zbIeknvPzW5I6IxQGZa23wNn/AIgxXgAcpLl/7rNnMQxp7vOUp5QO0vwQkrQjgj1NJKkzisKgrPXqBXb/izG+DngpzaQVz0spDWdxXm/+JKl3Qt2NvtqSJCjt/qeOiDHuo0moLgcumOVEeSZVkvrHi6KS1BlWqjSpWsCU6jHG+wNvBD4P/AHwqBjj+CGfmqZqZVIlqXfs/idJ3VGWBmV1whOAvcBDges22H9f4H9s9+QmVZJ6x6RKkrqjdEpWTVjERBUppXcA79ip85tUSeqdYnXRLZAkjawMqkU3QR1TL6D7304zqZLUO1aqJKk7SsdUaRcwqZLUO8GLopLUGXb/06Qq92+WXpMqSb1jUiVJ3VGYVGkXMKmS1DsmVZLUHSZVmrSIKdV3mkmVpN4pKj/AJakrgkmVJtQLmP1vp5lUSeodK1WS1B1WqrQbmFRJ6h2TKknqDpMqTbL7nyQtgcFhP8AlqSsKjMnqP5MqSb1jpUqSpO5ySnVJWgJOVCFJkubJpEpS7xRDkypJ6oqa/lUlNJ3aMVWS1H1huOgWSJKkY6mcUl2Sui/Y/U+SOqPu4fgZaZJJlaTecUyVJHWHSZUm9bFLqEmVpN4JjqmSpM7IJlXaBUyqJPVOUdWLboIkqTWs+zd+RtNxTJUkLYHisDeqkqSuMKnSpMrZ/ySp+5yoQpK6ozKp0i5gUiWpf4Z2/5Okrqhqx1RpvT5OXmJSJal3Qm1SJUldYaVKu4FJlaT+sfufJHVGH6sSmo5jqiRpCVipkqTuqKr+fYHWdGpn/5OkJeCU6pLUGbVjqrQLmFRJ6h8rVZLUGbWVKk2o6F+ibVIlqXfC0PtUSVJXDFfLRTdB2nEmVZL6JztRhSR1Ra76V5XQdBxTdRwxxnsD/2ODXX+YUvrfY4wBeBlwHnAf4GPAi1JKX5pVGyQJMKmSpC4xqdIuMMtK1Q+06ycA3xzb/q/t+hXARcBLgJuBXwOuiTE+OKV02wzbIWm3q02qJKkzTKo0wTFVx/cw4BsppT+f3BFjPBU4AFycUvrNdtu1wC3A84BLZ9gOSbudlSpJ6g5n/9MEu/8d38OA64+x70eAU4APjDaklG6NMX4UeCImVZIkSb0UhiZV6r9ZJ1V3xRg/DvwQzfiq3wTeDJzTHnPjxGtuAp46wzZIkiSpQ4Ld/zShslK1sRhjATwYuIOmm99XgJ8EXg/cA1gFDqeUjky89HbgnrNogyRJkroneJcL7QIhz2DsQYyxBB4LfCWl9I9j298KPAd4LfDylNJJE697LXBeSuk+J/gnHCAhSZIkTTj7kku58cCFS1UOvPgLT535d/uLv/+PF/oezKRSlVKqgL/cYNefAb9IU8HaG2NcSSmtju0/BdjUzH9nXn5w6nbq+A7t2+/7vMMO7dvPM647jzoH6tz87Q/rkprm+bAuqHLB4eGAKgdWq5KqDgyrkqouqKqCqgrUdUF9pGzu/TEMUIeme0UVKIZNV4titbk6GKpAqNvHdbNQ0xw3ep6bJeTcru/e9hyAADkEaB/XA8hFs1CsPc7laJ2p94w9HgBlJpcZigyDTLGnIpSZoqgpy0xZ1pRFzaCsKIvMSlmxp6wYFDVlqBkUNUXIFGQGRXP5c1DUR9t59blX8aTTL9jZ/8hd7k9vucxYMQfG5J13aN9+zrjiEihZi0tlJgxqipX6aFxq1jVlaB8XmT1l1cSikAntumiDZ9FeC67bGc5GMT+3cT4DVV1wZNjE9joHqqqgrgN1VTQxvgrkI0Uzc14dCMMmzh+N61UT14sj4Wh8L6qxWD9aD/O6WB/qNsbntfVR7VfStXjfrsux+F6EiTjffhaUkAdQl3ndvpv2X8iDXufQ+Z20jB3p7P53DDHG7wL+A/BHKaX/b2zXqDJ1K82f6pnAl8f2nwWkWbRBko4qluqCnST1mmOqtBvMaqKKvcBVwMnAZWPbf5omiXpfu/9pwJsAYoz3ouky+MoZtUGSGsEPcEnqCsdUadKot06fzKr736EY4/8DvDrGWAP/APwMTVL1tJTSt2KMvwW8pt3/ZeBXaW4S/LuzaIMkHWVSJUmdYVKl3WCWU6o/D3g58MvA/WkSq59OKY3uTfUyoKaZHfAU4OPAz6WUNjWmSpI2zaRKkjoj1Cc+RrtLtZQjwY5vZklVSulOmsTpZcfYPwQuahdJ2jG57F+wlqRlVVip0gS7/0nSMihMqiSpK+z+p93ApEpS/1ipkqTOsPufJtV2/5Ok7ssr5aKbIElqFasnPkZadiZVknon2/1PkrrDSpUmVI6pkqQlMDCpkqSuCHVedBOkHWdSJal3ctm/K2CStKwcU6VJzv4nSUvAKdUlqTtMqjSpzv37nDapktQ7edC/K2CStLTs/addwKRKUu/UVqokqTOsVGlSRf8ufppUSeodK1WS1B3BSpV2AZMqSb1TO1GFJHWHSZUmOFGFJC2Bem//grUkLSsrVZrkRBWStARqu/9JUneYVGkXMKmS1Dt2/5MkqbtqJ6qQpO7L5aJbIEmSdhOTKkm9Y6VKkjrEkKwJlRNVSFL31UY2SZI6y4kqJGkJ2P1Pkrqjh0UJ6W5MqiT1jkmVJHWISZUmeJ8qSVoC1Z7+BWtJWlY97Okl3Y1JlaTesVIlSd3Rw6KEpuSU6pK0BEyqJKlDrFRpFzCpktQ7djWRpO4wJmuSY6okaQlYqZKk7jCp0iSnVJekJdDDWC1JSysX/atKSJNMqiT1jkmVJHWHMVmT7P4nSUvAD3BJ6g67ZGs3MKmS1D8mVZLUGV7o0iSnVJekJVAP8qKbIElq1XsW3QJ1jd3/JGkZ9C9WS9LS6uH3Z+luTKok9Y4f4JLUHY6p0iQrVZK0DPoXqyVpaWW/bWoX2PKveYzxKcDvp5ROHdsWgJcB5wH3AT4GvCil9KWxY/YCbwCeCZwMfBg4P6X01al+AkmaZFIlSZ1Rl45z1Xq7vlIVY/xR4F3c/SvLK4CLgJcANwO/BlwTY3xwSum29pgrgacA+4FvAa8HPhRjfHhKqdr2TyBJE/z4lqTusPufJu3apKqtMu0DXg3cAewZ23cqcAC4OKX0m+22a4FbgOcBl8YYzwaeAzwrpfTu9pjPAQl4KvC+Wf1AkmSlSpK6wynV1UUb9b6bxmZ/zZ8EvBR4MfBbE/t+BDgF+MBoQ0rpVuCjwBPbTY9r1x8cO+YG4Itjx0iSJKlncumy08uyqQkzX7biOL3vtm2z3f8+CZyZUvq3GOPFE/vOadc3Tmy/iaYKNTrm6ymlOzY45hykXeTvPvOgmZ3rbpGgzNQlQKbLfWpDFaAKsAr5zpIM1MBwFic/Fx7+6AQ03QvqHBjWZRN0c2BYF1S5aNZ1QZUDq1XJ4dVB87wqqKpAXRfkKpCrZs2R4mi7i2HzM4QaQjVaAsUqzba6+YHCxEINIWfIEDboo5gDECCH0PznhuYK77pl0K7L0TqvfbAWUO3NUGZymaHIMMiEMhPKmlBmiqJmz56KsqgZlBVlkVkpK8qQm21Fzd5ySBEyBZlB0fwmFSFTbNRoack1f9cQJiJqBqp2WYQ2BNxNEwsyrLQbTp733+XW/708yBSH12JmUQXCsI2dud222j6vIdR5Im62j8dj53gz2jcqF00czQUT8TNAAfXKWvysy7FYWq7F0npPXns+aONo2SyhzAxWKoqypigyZVlTtLGzLEbrmpWiJrQxcxQ3i7EGjz6P6hzIee1xlQNVXbA6LJvHVUFdj30m1aH5XBoWsFqs/e5Wds/YrOP1vpvWppKqlNK/HGf3PYHDKaUjE9tvb/eNjrl9g9feDjxgM22QJEmatewkCtLcLXBM1Xjvu3vTzPUwE7OY5DKw8WWLQHPxebPHHNehfTP7mXUcvs877+ZfOrDoJvTe1edetegm9J6xYj58n3fezS80Ju80f4931pmXH1x0E5bJ8XrfTWUWSdVtwN4Y40pKaXVs+yntvtExGw0CGz/muPyF2XmH9u33fd5hh/bt54y3XrLoZvTazf/nAZ5x3XmA3f92qvvf1edeZayYA2Pyzju0bz+n/86bF92MXrvlBS/mzMsP2v1vJ7v/LeHsTIuqVJ2g991UZpFU3UDz63wm8OWx7WfRzO43OuZ+McaTUkp3Thxz7QzaIC2N7LgUSeqOwpgszduunVL9BD4O3AU8DXgTQIzxXsBjgVe2x1wDlMCTgavbYx4EPAS4eAZtkJZH/+KIJC0vx1RJmoGpk6qU0rdijL8FvCbGWNNUq34V+Cbwu+0xN8YY3wO8LcZ4GnArzc1/rwfeP20bpKViUiVJnREGmxraLWmGrFQd28toJpw4QDNO6uPAz6WUxsdLPRe4DHgjzf2xPgKcn1Lq8szPkiSpx4KVKkkzsOWkKqV0MRNd9lJKQ+CidjnW6+4AXtAu0u7lmCpJ6ozgmCpp7rKVKklTG/gBLkldsbJihxlp3uoejoUwqZLmzauiktQZReGYKmk32qj33TRMqqQ5s6uJJHVHWZpUSfPmRBWSpmZSJUndUTrOVdIMmFRJc+ZMU5LUHXb/k+bPiSokTc0PcEnqjtLeA9Lc2f1P0tQKP8AlqTNKL3RJmgGTKmnOHBQtSd3hmCpp/uz+J2lqJlWS1B2FSZWkGTCpkubsHivDRTdBktSy+580f46pkjS1lbJadBMkSS0rVZJmwaRKmrMVr4pKUmeYVEnzl3v4Z2dSJc3Z3tLuf5IkafeqsfufpCnZ/U+SuqOgh5fMJc2dSZU0Z3sKK1WSJGn3ckp1SVPbY6VKkiSpV0yqpDk7qVxddBMkSa0+ju2Qus4p1SVN7dTBXYtugiSp1ccvd1LXOfufpKntdUyVJHWGSZWkWTCpkubMpEqSuqOPA+alruvj351JlTRn9ygcUyVJXTGsi0U3QVIPmFRJc7bXpEqSOqOHQzukzrNSJWlq9wh2/5OkrqisVElz18exjCZV0pytmFRJUmeYVEmaBZMqac4cUyVJ3VHV/btiLnWdU6pLmtpKqBbdBElSy0qVpFkwqZLm7OTi8KKbIElqrQ7LRTdB2nWcqELS1BxTJUndUdv9T5o7kypJU1vB7n+S1BV1Zfc/SdMzqZLmbI9jqiSpM2rHVElz18N5KkyqpHkrQr3oJkiSWnXVv25IkuZvy0lVjPEpwO+nlE4d2/YI4JMbHH4wpXSgPWYv8AbgmcDJwIeB81NKX91Ow6VlVWJSJUldkU2qpLnb9WOqYow/CrwLmHwnHgbcATx+Yvt4wnQl8BRgP/At4PXAh2KMD08p2R9Ku0bZy6K3JC0pkypJM7CppKqtMu0DXk2TPO2ZOORhwBdSSp84xuvPBp4DPCul9O522+eABDwVeN+2Wi8tIbv/SVKHmFRJ89fD68ubrVQ9CXgp8GLg3jTVpnEPA64/zusf164/ONqQUrohxvhF4ImYVGkXsVIlSd0RjjhRhTRvu7n73yeBM1NK/xZjvHiD/Q8FDscYPws8GPgK8OqU0u+1+88Bvp5SumPidTe1+yRJkuYuWKmSNAObSqpSSv9yrH0xxu8C7gM8iKaadSvNZBTviDHmlNI7gXsCt2/w8tuBB2y10ZIkSbPgXS6k+cs97LQziynV/42mC9/1KaWvtds+0iZbvw68k2Zii43evgCbmwrt0L7JHofaCb7PO++Rp9+y6Cb03tXnXrXoJvSesWI+fJ933k0X+B7vNH+Pd9aZlx9cdBPEDJKqlNK3aaZHn/RnwBNjjKcAtwGnbnDMaN8J+Quz8w7t2+/7vMMO7dvPJ285fdHN6LVHnn4Lz7juPADqHKhzYFiX1IweF1S5aNZ1QZUDq1XJ4dVB87wqqKpAXRfkKpCrZs2RoukmVAWKYdNlKNTNVe5mCRSrNNtqoF57vG5bzpAhbHCZKQcgQA6hueQUIBcTy6Bdl6N1bh63z6u9GcpMLjMUGQaZUGZCWRPKTFHU7NlTURY1g7KiLDIrZUUZcrOtqNlbDilCpiAzKJrL+EXIFG2jrz73KmPFHBiTd96hffv53jdeuuhm9No/vuRCzrz8IMXhtZhZVIEwbGNnbretts9rCHWeiJvt4/HYOR5D2x6cuWjiaC6YiJ8BCqhX1uJnXY7F0nItltZ78trzQRtHy2YJZWawUlGUNUWRKcuaoo2dZTFa16wUNaGNmaO4WYw1ePR5VOdAzmuPqxyo6oLVYdk8rgrqeuwzqQ7N59KwgNXRZxKEu03K3X27eUzVMcUYz6GZiOLtKaXDY7tOAu6kmS3wBuB+McaTUkp3jh1zFnDttG2Qlkm1hMFPkvrK7n/SAphUbei7gbcC3wD+CCDGGICnA9emlHKM8RqgBJ4MXN0e8yDgIcDFM2iDtDTq7ExTktQVTlQhaRZmkVT9V+BvgCtjjPcCvgacRzPN+qMBUko3xhjfA7wtxngazWQWr6eZhv39M2iDtDSsVElSd1ipkubPiSo2kFKqYoxPBV4HvIrmPlafBn4ipfSpsUOfC1wGvBEogI8A56eUDGfaVSqsVElSV5hUSZqFLSdVKaWLmeiyl1L6n8AvnuB1dwAvaBdp17qrXll0EyRJrXJ10S2QdiErVZKmtUq56CZIklpWqqT5c/Y/SVNbzf7ZSVJXhE3dLVOSjs9vd9KcHclWqiSpK6xUSQtg9z9J07JSJUndYaVK0iz47U6as1UrVZLUHSZV0tw5pkrS1Jz9T5K6I9Q97Ickae5MqqQ5s/ufJHWH3f+kBejhtQy/3Ulzdlt10qKbIElqOVGFtAh2/5M0Jbv/SVJ3WKmSNAsmVdKcHa79s5Okrgg97IYkdV4P/+78difNmUmVJHVID7/cSZo/v91Jc2ZSJUndYaVKWoAe/t357U6asyMmVZIkaTfzPlWSpnVn5UQVktQZPbxiLmn+TKqkOTtSlYtugiRJ0sLkHl7MMKmS5szuf5IkSf3itztpzr51ZO+imyBJGunf0A6p+6xUSZrWal0sugmSpFYPx8tL3dfDPzyTKmnOVh1TJUnd0b/vdpIWwKRKmrNhZaVKkrqihxfMpc7r4/3hTKqkORva/U+SusOkStIMmFRJc1ZZqZKkzrBSJS2AlSpJ0zKpkqQOMSRLmgGTKmnO6trLopLUFdmkSpq/HpaITaqkOatWnf1PkrqiHvTvy53UeXb/kzStbKVKkjrDSpWkWTCpkubMpEqSuiPbeUCaPytVkqZmUiVJnWGlStIsmFRJ82ZSJUmdYaVKWgArVZKmVi+6AZKkEZMqaQGc/U/S1HoYSCRpWRmSJc2CSZUkSdq1rFRJ8xd2a/e/GGMJ7AOeDzwQuAV4C/DbKaUcYwzAy4DzgPsAHwNelFL60tg59gJvAJ4JnAx8GDg/pfTV2f040hLoYSCRpGWVvbws7SoxxucDvwJ8D/BZ4MKU0nXTnnezoeTlwEXAq4FPAI8BfgP4DuBNwCva/S8BbgZ+DbgmxvjglNJt7TmuBJ4C7Ae+Bbwe+FCM8eEppWraH0RaFsGJKiSpM6o9XumS5m5Bf3YxxufQ5CSvAj4JvAj4cIzxB1JKh6Y59wmTqhhjAVwIvDml9Np28zUxxvsCB2KMbwUOABenlH6zfc21NNWs5wGXxhjPBp4DPCul9O72mM8BCXgq8L5pfghpqfj5LUmd4ZTq0u7Q9qx7FfA7KaVXttv+giYfuQA4f5rzbyaUnAa8k7snPgm4L/A44BTgA0d3pHQr8FHgie2mx7XrD44dcwPwxbFjJEmS5iqXLju5SB3yvcDprM9ZVoE/YQb5yAkrVW2C9MINdj0Z+Gea/ogAN07sv4mmCgVwDvD1lNIdGxxzzqZbK0mSNEO5tPuANG8LmqhilHP848T2m4CzY4zlNEOStjU8M8b4C8Djacpk9wQOp5SOTBx2e7uPdn37Bqe6HXjAZv7NR/7wlwGoc6DOgWFdUjN6XFDlolnXBVUO3LU6YFiVzfOqoKoCdV2Qq0CuCvLhAupAqAJUgWIIoQqEGkIFxZG1x6FuFuq1x6N9ZAg5t+u7tzsHIEAOAUaPi7FlMPa4HK3z0Ss89UqmHgBlbgJ/kWGQCWUmlDXlSkVZZsqypixqBmVFWWRWyooyZMqiZm85ZFDUFCFTkBkUze9LETJF2+i/+8yDmvau+OGy08KqY6p22if/bnbXato/23Wav9nJv5VZ/u1s/1xNrAob/p5loALu3PbZW+dCGIa1+FhBUQXCsI2ZGYojTMTQfPdYmlkfO8d/7DAeP9suWuviZ2jiZ7kWP+uxx82SqfeMx9RMHrRxtGyWck9NUdQUZU1RNLG0aGNnWWT2DIYEYFDUhDZmjuJm0Ta4bn9DRp9PuamgxgcAACAASURBVF3XOVDlwJFhSd1+NlVV0TwefSbVoflcOlyOfSY1n0eaE9/quaj3jv+Bz/u7xhb/vdzGuOHdfzmqdlmUjT6TtGmjvGQyJ7mdpvfeycA3t3vyLSdVMcZn0wzwei9wBfBSNv5tDazd5jRs4pjjuvrcq7baVG3Fuc3q5l86sNh27AKH9u1fdBN6z/d45920/8JFN2FX8Hd55/ke7zzf45115uUHF92ErVvMDeJG/+hkTjLavqmc5Fi2lFTFGC8ADtL0RXx2O536bcDeGONK2y9x5BRgNPPfbcCpG5xy/JjjesZ15wFWqnayUnXzLx3gjLdcspn/Dm3Tzb90YDmD3xI5tG+/7/EOO7RvP2cdvNRK1Q5Xqg7t28/Zb7p0Pv+pu9SNv3Kh8WKHGZPVIaOc41TgG2PbT6FJqCaHKW3JppOqGOPraKpS7wSel1IatrtuoEkXzgS+PPaSs2gmsxgdc78Y40kppTsnjrl2m23XLIWJtSRp4cJU100lqaMWM9rkhnZ9FuvHVZ0FpJTSVK3a7M1/99EkVJcDF0z8ox8H7gKeRnPPKmKM9wIeC7yyPeYaoKSZ3OLq9pgHAQ8BLp7mB9BsjMaH3H2ciCRpUcojXumS1EOLS6r+iSZn+XOAGOMK8FM0MwBOZTP3qbo/8Ebg88AfAI+KMY4f8ingt4DXxBhrmmrVr9IM9PpdgJTSjTHG9wBvizGeBtxKc/Pf64H3T/tDaAasVElS54ThiY+RJJ1YO2zpDcAVMcZbgY/RzHB+H+Cyac+/mUrVE4C9wEOB6zbYf1/gZTR9EQ/Q9Ev8OPBzKaXx8VLPpWnwG2lm2PgIcP40UxdKktRnwU9IST20oCnVSSm9JcZ4ErCP5oa/nwWekFK6adpzb+Y+Ve8A3rGJc13ULsc6zx3AC9pFXTP67V7Ub7kk6W4MyZI0WymlgzQT783Utu5TpR4qJtaSpIWzUiWpl3p4wcikSo3RBBVOVCFJnWFSJamXevh106RKDZMqSeocp1SXpOVgUiUAQptMBZMqSeoMK1WS+qiP40VNqgSYVElSF1mpkqTlYFIlAMqVat1akrR4xdALXZJ6KPfvxqgmVQKgaCtUhZUqSeoMK1WSeqmHXzdNqgRAWdbr1pKkDjAkS9JSMKkSYFIlSV3Ux8HcktTH2GZSJQAGRb1uLUnqgB5+8ZCkPjKpEgCDtkI1sFIlSZ3Rx6u5ktTHC0YmVQJgpazWrSVJHdDDLx6S1EcmVQJgpe32t2L3P0nqDCtVkvqoj7HNpEoAnLLn8Lq1JKkDevjFQ5L6GNtMqgTAnmK4bi1JkiRpc0yqBMCedizVHsdUSZIkaSdZqVJfnVSurltLkjogLLoBkqTNMKkSYPc/SZIkzYcTVai39rbJ1F6TKknqjGylSpKWgkmVAJMqSeokkypJWgomVQJMqiSpi6xUSeolu/+pr+5RrK5bS5IWLxeLboEkaTNMqgTAaeWd69aSpMXL5aJbIEmz50QV6q2VMFy3liQtnpUqSb1kUqW+svufJHVPLhxUJUnLwKRKAKyEat1aktQBVqok9ZGVKvWV3f8kqXvs/idJy8GkSgDsaStUe6xUSVJnOFGFpD5yogr1lpUqSeoeK1WStBxMqgTACtW6tSRp8axUSeolK1XqK2f/k6TuqVYW3QJJmj27/6m3Sup1a0nS4lmpkqTlsKmkKsZYAvuA5wMPBG4B3gL8dkopxxgfAXxyg5ceTCkdaM+xF3gD8EzgZODDwPkppa9O/VNoamVbhy37WI+VpCVlUiWpl3r4dXOzlaqXAxcBrwY+ATwG+A3gO4A3AQ8D7gAeP/G68YTpSuApwH7gW8DrgQ/FGB+eUnIgz4IVoV63liQtXi57+M1DknrohElVjLEALgTenFJ6bbv5mhjjfYEDrCVVX0gpfeIY5zgbeA7wrJTSu9ttnwMS8FTgfdP+IJqOlSpJ6h4rVZJ6qYdfNzdTqToNeCd3T3wScN8Y48k0SdX1xznH49r1B4++OKUbYoxfBJ64wbklSdr1TKok9dGunKgipXQr8MINdj0Z+OeU0h0xxocCh2OMnwUeDHwFeHVK6ffaY88Bvp5SumPiHDe1+yRJ0gS7/0nScgg5bz1gxxh/AXgbcD7wh8C/ADcCLwVupZmM4ueBn0spvTPGeBXw2JTS/zpxnncBD04p/dAJ/kk/VSRJkqQJZ15+kEP79odFt2MrHnLRZTP/bv/FN1yw0Pdgy1OqxxifTTPpxHuBK4CTaLrwXZ9S+lp72EdijN8F/DpN18HAxolRgM3N4f2M684DoM6BOgeGdUnN6HFBlYtmXRdUOXDX6oBhVTbPq4KqCtR1Qa4CuSrIhwuoA6EKUAWKIYQqEGoIFRRH1h6Hulmo1x6P9pEh5Nyu797uHJqfMofQ/LQBcjG2DMYel6N1bh6XUK9k6gFQ5uaKZZFhkAllJpQ15UpFWWbKsqYsagZlRVlkVsqKMmTKomZvOWRQ1BQhU5AZFM28IEXIFG2jX/xdf8YjT7+FT95y+mb+O7RNjzz9Fs68/OCim9Frh/bt9z3eYYf27eesg5euxccKiioQhm3MzFAcYSKG5rvH0sz62DkeQ8N4/Gxi4/r4GZr4Wa7Fz3rscbNk6j3jMTWTB20cLZul3FNTFDVFWVMUTSwt2thZFpk9gyEBGBQ1oY2Zo7hZtA2uaT7HR59PuV3XOVDlwJFhSd1+NlVV0TwefSbVoflcOlyOfSY1n0eH9u3njN++ZD7/qbvUzf/5gPFihxmTtVtsKamKMV4AHAQ+ADw7pZSBb9NMjz7pz4AnxhhPAW4DTt3gmNE+LVjVfikYrSVJi5f3OCOrpB7qYR+0TSdVMcbX0XTveyfwvJTSsN1+Ds1EFG9PKR0ee8lJwJ00U63fANwvxnhSSunOsWPOAq6d7kfQLNS5WLeWJHWAY6ok9dCunKgCIMa4jyahuhy4oK1QjXw38FbgG8AftccH4OnAte3Nga8BSprJLa5uj3kQ8BDg4pn8JJqKlSpJ6iCTKklaCpu5T9X9gTcCnwf+AHhUjHH8kI8DfwNcGWO8F/A14DyaadYfDZBSujHG+B7gbTHG02gms3g9zTTs75/ZT6NtqyjWrSVJixdMqiT1UQ9D22YqVU8A9gIPBa7bYP99aW7g+zrgVcC9gU8DP5FS+tTYcc8FLqNJ0ArgI8D5KaVq263XzNj9T5K6pzCpkqSlsJn7VL0DeMcmzvWLJzjPHcAL2kUdc6S9w+QR7zQpSZ1RFE5UIal/du2YKvXfKuW6tSRp8YrSpEqSloFJlQBYzYN1a0nS4hVFDy/nSlIPQ5vfoAXAHfXedWtJ0uKtDBx2LKmHTKrUV6vtWKpVx1RJUmeUjqmSpKVgUiUA7qpX1q0lSYtX2v1PUg/18a6oJlUCHFMlSV1kpUqSloPfoAXAXW0ydZdJlSR1hkmVpF7qYRHeb9AC4HDb7e+w3f8kqTP62EVGkrxPlXrLMVWS1D0DK1WStBRMqgTA4Xqwbi1JWrzQx8u5ktTD0OY3aAEmVZLURYVJlSQtBb9BC4Dbh/dYt5YkLZ5JlaRe6mFoM6kSAHdWK+vWkqTFK/r4zUPSrtfH60UmVQLgSFWuW0uSJEnaHJMqAXCkHUt1xDFVkiRJ2klWqtRXq22FatVKlSR1Ru2dqiRpKZhUCYDD1WDdWpIkSdoJjqlSb63Wxbq1JGnx6mylSpKWgUmVALv/SVIXmVRJ6iUrVeqru1YH69aSpMWr7D0gqYfs/qfeqqpi3VqStHhWqiRpOZhUCTCpkqQuqkyqJPWRlSr1VV2HdWtJ0uLZ/U+SloNJlQCo2w/u2g9wSeqMygtdkvrISpX6Kldh3VqStHhe6JLUR05Uod7K7dXQ7FVRSeoMx1RJ0nIwqRJgUiVJXeTkQZJ6yUqVemuUTJlUSVJn2P1PkpaDSZUaw7B+LUlauNXVctFNkKSZC7l/pSqTKjVG/fbtvy9JnWGXbEm9tAQ5VYzxVOALwP6U0ntPdLxJlSRJHeWMrJI0f21C9cfAAzf7mk0lVTHGPcArgJ8F7gP8LXAgpfTpdn8AXgac1+7/GPCilNKXxs6xF3gD8EzgZODDwPkppa9utrHaQXliLUlauDx0TJWk/unylOoxxscCVwL/y1Zet9lofRlwPk1S9L8B3wb+KsZ4erv/FcCvAZcA/xE4Dbgmxnja2DmuBJ4DXAQ8F/gB4EMxRjuMd8F4UuWyc4skbUUVXHZykaS7ez/weeCJW3nRCStVbWL0fOCilNJb223XAv8K/GyM8XLgAHBxSuk3x/bfAjwPuDTGeDZNQvWslNK722M+ByTgqcD7ttJozV5ox1IFx1RJUnc4pkpSH3X7QvNjUkpfiDGesZUXbab73x3Ao4Cbx7at0rwde4EfAU4BPjDamVK6Ncb4UZoM71Lgce2uD44dc0OM8YvtMSZVi2b3P0nqnGA1RVIPLaL7X4xxBTj7OId8I6V0a0rpC9s5/wmTqpTSEPhM25gCOB14Jc3X73cBj28PvXHipTfRVKEAzgG+nlK6Y4NjztlOwyVJ6r1q0Q2QpN74buAfjrP/AuA3tnvyrc7+93Lg4vbxK1JKKcb4dOBwSunIxLG3A/dsH9+zfT7pduABm/mH/9vfxC029e5CuxxPDpAHUA9mnUJv73xhGCiHcKyWZ2DYLtP44UenZv2DN1DnQN12AxzWJTWBI1VJlQuGdUFVF1Q5sFqVVHVgWJUcWR1QVYG6LshVIFfNmmGAOhCqQHG4WYcaQjVa1p4XQ6CmeT62jMYjhTof88pGbv9zcxHa9fqFdl2vtNvKTC5plvZ5PQDKTF6pYZAJZSaUNaHMFEVNWWbKsmbvyiplkVkpK8qQKYuaQVFThmY9CDWDovkmVIRM0eXRmNI25UEmr/sEmffv+Rb/vdzE0zBxL75Mk7csMnc54WdTSROXykwYNDEpFGtxqShq9gyq5nnRxKSyjT1FyISxx8XY+1a3/+oo5q9WZfN+jOJ8HajqgjoHqqqgrgPD1QF1FZr4Plrq5n0NVaA4EtbH99HjGooKitW15028z+ti/dHYv1HvidGdP4qxmB84Gt+bJazF+lGMD+16AHUb+5v9xmZpYRbw55dSupkTpwLbttWk6o+AvwZ+DHhFOyvgnWz81gSgHnt8omOO68YXX7jFpmo7rj73qkU3ofcO7du/6Cb0nu/xzvM9no+bX3hg0U3oPX+Xd57v8c468/KDi26C2GJSlVK6vn340Xb+9hcDLwH2xhhXUkqrY4efAtzWPr4NOHWDU44fc1xnv/nSrTRVW/TwRyeuPvcqnnHdeVaqdrBSdfW5Vxn8dtihfft9j3fYoX37OftNxuSdduOvXMgZV1xipWpkBypVh37ZeLHTjMnaSB878Wxm9r/7AU8C3ptSGu/C9xmaiSpupQl1ZwJfHtt/Fs3sfgA3APeLMZ6UUrpz4phrN9XSHr75krSswqb6GEiStDtsplL1ncB/aR+/fWz7vwf+O81c7ncBTwPeBBBjvBfwWJoJLQCuAUrgycDV7TEPAh7C2hit43L+I0nqjuAECpKk7ephsWQzs/99Kcb4h8DBdgzVTcDTgZ8Ffj6l9M0Y428Br4kx1jTVql8Fvgn8bnuOG2OM7wHe1t736lbg9cD1NEnZifXwzZekZVU41bckaZuWofvfVie22OyYqucAvw68FLg/8PfAz6SU3tvufxnNhBMHaMZJfRz4uZTS+Hip5wKXAW8ECuAjwPkppc1d71yCN1+Sdosw7ZSjkiT1yKaSqpTSt2kmpHjJMfYPgYva5VjnuAN4Qbts2TJktJK0W9j9T5K0bbl/X+y3OqX64vTvvZekpeWFLkmS1ixNUlUM7b8vSV1RTN7uXZKkTerjhbmlSao2d4tgSdI8OKW6JGnbTKoWxw9wSeoOx1RJkrTGpEqStGXGZEnSdvXxM8SkSpK0ZaHuYd8NSZK2aXmSKruaSFJneKFLkrRtPbwutzxJlR/gktQZxmRJ0nY5+98CWamSpA4xqZIk6SiTKknSlvXxKqMkaU5y/z5EliapKo/0782XpGVl9z9JktYsTVJlpUqSOsTrXJKkbepjbweTKknSlvXxA1GSpO1amqSqGC66BZIkSZKm1sMLc8uTVFU9fPclaVkZkiVJ29TH3g5Lk1TZ/U+SJElSFy1NUmWlSpIkSeoBp1RfnGLYvzdfkpZWWHQDJEnqjuVJqg6bVElSV2STKknSNjmmaoHs/idJHWJSJUnarh5+rV+apCrY/U+SOsNKlSRJa5YmqSqqetFNkCS1crHoFkiSlpXd/xbISpUkdYiVKkmSjlqepMpKlSR1hpUqSdK21f0rlixRUtW/N1+SlpVJlSRp23r4tX5pkiqGVqokqStyYf8/SZJGliapCrVJlSR1hpUqSdI2OVHFAoXVatFNkCS16pVFt0CSpO5YmqQKJ6qQpM5wTJUkadty/0pVy5NU2f1Pkjojl4tugSRJ3bE0SZVTqktSd9QmVZKkbXJM1SL1sEwoScvK7n+SpG3r4df6TSVVMcY9wCuAnwXuA/wtcCCl9Ol2/yOAT27w0oMppQPtMXuBNwDPBE4GPgycn1L66qZaalIlSZ1h9z9JktZstlJ1GU1C9RLgRuB84K9ijA9LKd0CPAy4A3j8xOvGE6YrgacA+4FvAa8HPhRjfHhK6cRT+5lUSVJnmFRJkrYr9PB7/QmTqhjjacDzgYtSSm9tt10L/CtNovUamqTqCymlTxzjHGcDzwGelVJ6d7vtc0ACngq874Qtrfv35kvSssqlMVmSpJHNVKruAB4F3Dy2bZWmN+Te9vnDgOuPc47HtesPjjaklG6IMX4ReCKbSap6mNFK0rKq9yy6BZKkpdXD+edOmFSllIbAZwBijAVwOvBKmqTqXe1hDwUOxxg/CzwY+Arw6pTS77X7zwG+nlK6Y+L0N7X7JElLxEqVJGm7dmX3vwkvBy5uH78ipZRijN9FM3nFg4CXArfSTEbxjhhjTim9E7gncPsG57sdeMB2Gi5JWhzHVEmStCbkLWSKMcaHAfcCfgz4VZrZ/F4PPAa4PqX0tbFj/xQ4J6V0dozxd4DHpJS+b+J8vw/ElNIjTvBP9y+dlSRJkqZ05uUHObRvf1h0O7bix3/s9TP/bn/NX710oe/BlipVKaXRuKmPxhhPBV4MvCql9OENDv8z4IkxxlOA24BTNzhmtO+EnvTAX95KU7VFp777Tq4+9yqecd151DlQ5+b3cliX1ASOVCVVLhjWBVVdUOXAalVS1YFhVXJkdUBVBeq6IFeBXDVrhgHqQKgCxeFmHWoI1WhZe14MgZrm+dhCbpZQ52PeLC4HIEAuQrtev9Cu65V2W5nJZXO1ffS8HgBlJq/UMMiEMhPKmlBmiqKmLDNlWbN3ZZWyyKyUFWXIlEXNoKgpQ7MehJpB0UxoWYRMMdboq8+9ijMvP7iD/5M6tG+/7/EOO7RvP6df9eZFN6P3bjnvxZxxxSVQ0sSlMhMGTUwKxVpcKoqaPYOqeV40MalsY08RMmHscTF2jbKmifOjmL9alWRoYnxdUNWBqi6oc6CqCuo6MFwdUFehie+jpQ6EYRvnj4T18X30uIaigmJ17XkT7/O6WH809o+aOR7z269LuRiL+YGj8b1ZwlqsH8X40K4HULexv9mfOfTLxoudZkzWbrGZ2f/uBzwJeG9KabwL32doJqo4N8b4YODtKaXDY/tPAu6kmejiBuB+McaTUkp3jh1zFnDtploalioBl6R+K+xAIEnapl06puo7gf/SPn772PZ/D/x3YAV4K/AN4I8AYowBeDpwbUopxxivAUrgycDV7TEPAh7C2hit4ytMqiSpM5yoQpK0TcfqebTMNjP735dijH8IHIwx7qGZse/pNPeo+nngr4G/Aa6MMd4L+BpwHs00649uz3FjjPE9wNva+17dSjMW63rg/ZtqqZUqSeoOkypJko7a7Jiq5wC/TjO73/2Bvwd+JqX0XoAY41OB1wGvAu4NfBr4iZTSp8bO8VzgMuCNQAF8BDg/pVRtqgUmVZLUGcGkSpK0Xbu0+x8ppW8DL2mXjfb/T+AXT3COO4AXtMuW5YHz90pSVwxWNnc9TJKk3WCr96lanKJYdAskSa2irBfdBEnSkgo9/AhZnqSqNKmSpK4onP1PkqSjliapylaqJKkzSitVkqTt2q1jqjqhdKIKSeqKoo/z4UqS5qOHHyFLk1RZqZKk7igLK1WSJI0sTVLFwKRKkrqidEyVJGmbgt3/Fifb/U+SOsNKlSRJa5Ymqar3ep8qSeqKgUmVJGm7rFQtTu2U6pLUGSZVkqRt6+FHyNIkVXlg9z9J6org7H+SJB21NElV7ZgqSeoMp1SXJG2XE1UskBNVSFJ3mFRJkrRmeZKqpWmpJEmSpGOyUrU4tWOqJKkzCvr3gShJmhOTqsVxTJUkSZKkLlqapCp7mypJkiRp+Tml+uIM91qpkqSuqDEmS5I0sjRJlZUqSeqOOptUSZK2xynVF8ikSpK6w6RKkqQ1S5NUOVGFJHVHNqmSJG2XlarFsVIlSd1hpUqStG0mVYtjUiVJ3WFSJUnSGpMqSdKWVSZVkqTtslK1OLlYdAskSSNVbVCWJGlkaZKqemXRLZAkjawO7T4gSdomb/67OFaqJKk77P4nSdou71O1QCZVktQdVWVQliRpZGmSKvz8lqTOqB1TJUnarg5XqmKMPwq8FvhB4NvAR4AXp5S+cbzXLU1SlYvuvvmStNtUld3/JEn9EmP8PuAa4C+AZwL3Al4NfDjG+MiU0uqxXrtESdWiWyBJGrFSJUnatrqzxZIXAl8DfnqUQMUYbwD+DvgJ4EPHeuHSJFV4UVSSOiPXBmVJ0jZ1t/vfF4G/n6hIpXZ95vFeuDRJlRNNSVJ3ZLv/SZJ6JqX0lg02P7ldf+l4r12apMpKlSR1Rx7a/U+StE0LqFTFGFeAs49zyDdSSrdOvOYBwCXAp4C/PN75lyapslIlSR2yalIlSVoq3w38w3H2XwD8xuhJm1BdQzMH+X9MKR03E1yapMpKlSR1R7D7nyRpuxZQqUop3cwmM4oY4/cDfwqsAD+RUrrxRK9ZnqRKktQd1aIbIEnS7MUYH0WTUH0TeFxK6YbNvM6kSpK0ZVaqJEnb1tEp1WOMZ9AkVN8Afjyl9NXNvjbk7k5pKEmSJKlnnnTmhTNPQP700KVTX+2LMf4x8FPAzwKHJnbfklL62rFea6VKkiRJ0q7Wzg74k0AJ/N8bHPJimpkAN2RSJUmSJGl+OthTrr3h78p2X++cuJIkSZI0BStVkiRJkuanoxNVTMOkSpIkSdL8dLD737Ts/idJkiRJU+h0pSrG+HzgV4DvAT4LXJhSum6xrVpOMcanAL+fUjp1bFsAXgacB9wH+BjwopTSl8aO2Qu8AXgmcDLwYeD8rczb32cxxhLYBzwfeCBwC/CW/7+9O4+ZqyrjOP6tLHUp4EINSBQB5UGIxSWoiIkgICiKrIkRrVRsRUUW8xJAoJQaFiVhc2ELokCIEkSCS1gslZAQGw1UEiUPKCYgxgWBgpWl0tc/zrl2mL604r2ddzrv95NMbnvu/WP6m9s7c+5zzrnAtzJz3Izbi4iNgfmU5U03B5YAY5l5V91vxh2qWS0FlmTm4bXNjDsQEa8BHplg1w8z8xBz7kZE7AmcCcwC/gZ8F1iYmc+ZcXsRsTuweA2HvBF4EHPWmlipGpyImA1cDFwNHAw8DtwcEdtM6htbD0XEeyk59q/fPx84hbI85MeBzYBFEbFZzzEXA7OBE4E5wM7Az2pnQnAq5cv7amB/4FrgfMqym2DGXTgPOJry5Xsg8C9gcURsXfebcbdOA3boazPjbuxct/sAu/a8Tqrt5txSROxGeXDnvZRnzXwTOIGSK5hxF+7i+efvrsAewD+AW4GHMGdNQUNZqap3khYCl2bm6bXtViCB4yg/sLQW9S7QMcBXgeXAxj37NgHGgAWZeWFtu4NSaTkCODcitqNc8D6RmT+ox/yG8jl8DLh+cP+a4RMRLwG+DJyTmWfU5kURMRMYi4iLMONW6hfwXODEzLyott1B+fL+VERcgBl3JiLeTrm+PtLT5rWiO7OAv2bmLf07zLkzZwO3NFVW4LZaIdwjIs7FjFvLzCeAX/a2RcT5wDhwGKXqZM5aMytVA/MmYGvgxqahrh3/U2DfyXpT66EPUe6AHg98o2/fe4AZPD/jx4DbWZXxB+r2Jz3H3A/8Fj8HKHfermT1i38CMyn5mXE7y4F3A1f0tK2gfHlPx/O4MxGxIfAd4Bzg4Z5dZtydWcA9L7DPnFuqN7R2Ay7tbc/MEzNzd8x4nYiIHYGjgFMy8++Ys/4XK1d2/5pkQ1mpArav29/3tT8AbBcRG2TmcwN+T+ujXwHbZObjEbGgb1+T8R/62h+g3CVqjvlLZi6f4JjtmeLql8RRE+z6KPAnylxAMOP/W2b+G7gb/lsZ3Bo4ndKpuhrYqx5qxu2dQKlmn0UZZtnwWtGdWcDTEXEn8A5KRfBCSkfWnNt7K2WY+/KI+DGwN/AEZZ7rQsx4XTkDuA+4rP7dnDUlDWunatO6fbKv/UlKde0VlAul1iAzH17D7k2BZzLz2b72J1mV/6as/hk0x7y+/TscPRHxWcoP/aMx466dCiyof56fmRkRB2HGrUXEDsDJwJ6Z+WxE9O72PO5AvSmwI6X6OkaZyP9hSif2pZQKrDm3M7NurwSuAc4F3k+Z2/MU5feDGXeoznPfH5iXmU2pwGuG1m4Eh/8Na6eqWVChP/GmffJrfOu/aayeb9O+8kUcoyoiDqNMvL2OMjn6JMy4Sz8CfkGZED2/rgr4FGbcSv2xfzlw+Qusruq1ohvTgI8AD2ZmMwpjcUTMoFQJz8Cc29qobm/OzGaxoMURsTmlY3U2Zty1ucBjlJEDDa8ZmpKGdU7VsrrdpK99BuU/W3+5WC/eMmB6RGzUTpyfdgAAA91JREFU1z6DVfkvY/XPoP8YARFxHHAVZXz4YZk5jhl3KjPvyczbM3MBZcjU8ZRrgRm38yXKsMr5EbFhnVsFMK3+2fO4A5n5XGbe1tOhatwEvBzP5S78s25v6mu/lZLR45hx1w4AbsjMZ3ravGZo7cbHu39NsmHtVN1ft9v2tW8LZP3Bqnbup9wR6l+iflvKQgvNMVtExMvWcMyUFxFnUoaZXAUc0jPkwYxbiogtImJOXRmt192UhSoew4zbOhDYCniUMgRtBWVp49k9fzfjliLidRExry6m0KvJzHO5vabDunFfe/Pj3nO5QxHxBuAtrL5Yk999mpKGuVP1EOUOCAD1jsd+wKLJelMj5k7gaZ6f8aso48+bjBcBG1AWXmiOeTOwE34OAETEMZRhfhcAh9eFFRpm3N4rKSvSHdLX/kHKQz1vwIzb+hywS9/rPkrVdRfg+5hxF6YDlwCf7Gs/mJL39ZhzW7+jrFx5aF/7fsCf8Vzu2rvqdklfu999WruV492/Jtm08SEol00kIr5AmZdyFuVJ3EcB7wPelpkPTOZ7Wx/V1f/GMnNGT9vXgWMpTz2/jzJRfStgp8xcVo+5lvKgyjHKndSzKMNU3jnVV2CMiC2BP1KymzfBIb+mPBjYjFuIiOsoy++eRFkZ6iDgSOAzmXmF53H3ImIpsLR51o8ZdyMirqFM6j+Z8nDaQynP7TkgM2805/YiYjbwPVbNb92LMmft85l5iRl3p/6u+GJm9ldfvWZorfbdfF7nHZCbHrl02tqPWneGdaEKMvPbtSx8DOWBv0uBfexQdeorlDlqY5RxzHcCn24ueNUc4Dzga5TK5s+Bo73gAeXLYDplGd+JJvjPxIy7MBs4jdKp2pJyN/rQzLyu7jfjdc+Mu3EEZRXLYynn8r3AwZnZPM/HnFvKzCsjYgUlyzmUUS9HZmbz7Coz7s5rKfPUJmLOmnKGtlIlSZIkafTs++q53VeqHr1sUitVwzqnSpIkSZLWC0M7/E+SJEnSCBrBkXJ2qiRJkiQNzsrRe8azw/8kSZIkqQUrVZIkSZIGZwSH/1mpkiRJkqQWrFRJkiRJGpjxEZxTZadKkiRJ0uA4/E+SJEmS1MtKlSRJkqTBWWmlSpIkSZLUw0qVJEmSpMEZH72FKqxUSZIkSVILVqokSZIkDcz4CM6pslMlSZIkaXAc/idJkiRJ6mWlSpIkSdLAjOLwPytVkiRJktSClSpJkiRJgzOCc6qmjY+PXvlNkiRJkgbF4X+SJEmS1IKdKkmSJElqwU6VJEmSJLVgp0qSJEmSWrBTJUmSJEkt2KmSJEmSpBb+A5+OQ8hCGMIzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.titlesize'] = 24\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 16\n",
    "\n",
    "gen_ex = quadmodel_generator(N=100)\n",
    "\n",
    "while True:\n",
    "    h, q = next(gen_ex)\n",
    "    if q == 0:\n",
    "        plot_hamiltonian2(h, q)\n",
    "        while q != 1:\n",
    "            h, q = next(gen_ex)\n",
    "        plot_hamiltonian2(h, q)\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ham_mirror( kx , ky ):\n",
    "    s1 = [[0, 1], [1, 0]]\n",
    "    s2 = [[0, -1j], [1j, 0]]\n",
    "    s3 = [[1, 0], [0, -1]]    \n",
    "    s = np.zeros( ( 2 , 2 , 3 ) ) \n",
    "    s[ : , : , 1 ] = s1 \n",
    "    s[ : , : , 2 ] = s2 \n",
    "    s[ : , : , 3 ] = s3     \n",
    "def quadmodel_generator(N=32):\n",
    "    g1 = -np.kron([[0, -1j], [1j, 0]], [[0, 1], [1, 0]])\n",
    "    g2 = -np.kron([[0, -1j], [1j, 0]], [[0, -1j], [1j, 0]])\n",
    "    g3 = -np.kron([[0, -1j], [1j, 0]], [[1, 0], [0, -1]])\n",
    "    g4 = np.kron([[0, 1], [1, 0]], np.eye(2))\n",
    "    kr = np.linspace(0, 2 * np.pi, N)\n",
    "    kx, ky = np.meshgrid(kr, kr) # row corresponds to y, column corresponds to x\n",
    "    while True:\n",
    "        coef = np.random.uniform(-1.5, 1.5, 2) # coupling lx, ly between [-2, 2]\n",
    "        h = (np.kron((coef[0] + np.cos(kx)), g4.flatten()) + np.kron(np.sin(kx), g3.flatten()) + \n",
    "             np.kron((coef[1] + np.cos(ky)), g2.flatten()) + np.kron(np.sin(ky), g1.flatten())).reshape((N, N, 16))\n",
    "        hr, hi = np.real(h), np.imag(h)\n",
    "        h_tot = np.concatenate((hr, hi), axis = 2) # h_tot has dimension (dim_kx, dim_ky, 16*2)    \n",
    "        if abs(coef[0]) < 1 and abs(coef[1]) < 1:\n",
    "            q = 1\n",
    "        else:\n",
    "            q = 0\n",
    "        yield h_tot, q\n",
    "gen_ex = quadmodel_generator(N=10)\n",
    "h, q = next(gen_ex)\n",
    "# h, q = quadmodel_generator( N = 100 ) # This is WRONG\n",
    "print( h, q ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert complex to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-607-6a9b035a07eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert complex to float"
     ]
    }
   ],
   "source": [
    "    s1 = [[0, 1], [1, 0]]\n",
    "    s2 = [[0, -1j], [1j, 0]]\n",
    "    s3 = [[1, 0], [0, -1]]    \n",
    "    s = np.zeros( ( 2 , 2 , 3 ) ) \n",
    "    s[ : , : , 1 ] = s1 \n",
    "    s[ : , : , 2 ] = s2 \n",
    "    s[ : , : , 3 ] = s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicse from project: String manipulation<a name = 'string'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 10.0\n",
      "<class 'str'>\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "a = 10.0\n",
    "\n",
    "# Output \"a = 2\" for a given a\n",
    "a_str = 'a = ' + str(a)\n",
    "print( a_str ) \n",
    "print( type( a_str) )\n",
    "print( type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
